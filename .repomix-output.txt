This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-10T10:17:55.583Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
cmd/
  api/
    main.go
  metadatatool/
    main.go
deployments/
  alertmanager/
    config.yml
  grafana/
    dashboards/
      auth.json
      queue-metrics.json
    provisioning/
      dashboards/
        auth.yml
      datasources/
        prometheus.yml
  otel/
    config.yaml
  prometheus/
    rules/
      queue_alerts.yml
    prometheus.yml
  docker-compose.monitoring.yml
docs/
  architecture/
    components/
      ai_service.md
      cache.md
      ddex.md
      storage.md
    ai_processing.md
    implementation.md
    overview.md
  development/
    guidelines.md
  integrations/
    qwen2-audio/
      README.md
  monitoring/
    alerts.md
    dashboard.md
    setup.md
  authentication.md
  queue_system.md
  queue.md
  README.md
  TODO.md
internal/
  config/
    sentry.go
  domain/
  graphql/
    generated/
      resolver.go
    resolvers/
      mutation.go
      query.go
      resolver.go
      subscription.go
      track.go
      types.go
    schema/
      schema.graphql
  handler/
    middleware/
      auth_test.go
      auth.go
      metrics.go
      session_test.go
      session.go
      tracing.go
    audio_handler.go
    auth_handler_test.go
    auth_handler.go
    ddex_handler.go
    health_handler.go
    metrics_handler.go
    track_handler.go
    user_handler.go
  pkg/
    analytics/
      bigquery.go
    audio/
      analysis.go
      batch.go
      effects.go
      ffmpeg.go
      processor.go
      validator.go
    config/
      app_config.go
    database/
      postgres.go
    ddex/
      schema_validator.go
    domain/
      ai.go
      audio.go
      auth.go
      context.go
      ddex.go
      errors.go
      experiment.go
      file_types.go
      graphql_input.go
      graphql.go
      job.go
      metadata.go
      models.go
      queue.go
      session.go
      storage_types.go
      track.go
      user.go
      validation.go
    errors/
      errors.go
    errortracking/
      sentry.go
    logger/
      logger.go
    metrics/
      auth.go
      jobs.go
      metrics.go
      operations.go
      queue.go
      storage_metrics.go
      timer.go
    middleware/
      auth.go
      logger.go
      metrics.go
      ratelimit.go
      sentry.go
      tracing.go
    monitoring/
      tracing/
        config.go
        provider.go
        queue_middleware.go
    queue/
      queue.go
    retry/
      retry.go
    session/
      redis.go
    storage/
      storage.go
    tracing/
      provider.go
    utils/
      audio.go
    validator/
      validator.go
  repository/
    ai/
      composite_service.go
      openai_client.go
      openai_service.go
      qwen2_client.go
      qwen2_service.go
    analytics/
      bigquery.go
    audio/
      audio_processor.go
    auth/
      jwt_service_test.go
      jwt_service.go
    base/
      memory_auth_service.go
      memory_repository.go
      memory_session_repository.go
      track_repository.go
      user_repository.go
    cached/
      track_repository.go
      user_repository.go
    jobs/
      audio_handler.go
      processor.go
      redis_queue.go
    postgres/
      track_repository.go
      user_repository.go
    queue/
      mock_service_test.go
      mock_service.go
      pubsub_service_test.go
      pubsub_service.go
      redis_queue_test.go
      redis_queue.go
    redis/
      session_store_test.go
      session_store.go
    storage/
      cleanup_worker.go
      cleanup.go
      s3.go
    cache_service.go
  test/
    integration/
      auth_test.go
      helper.go
    testutil/
      server.go
    auth_test.go
    helper.go
  usecase/
    audio_processor_service.go
    audio_service.go
    audio_storage_service.go
    auth_service_test.go
    auth_service.go
    auth_usecase.go
    cache_service.go
    ddex_service.go
    track_usecase.go
    user_usecase.go
metadata/
  infra/
  metadata/
    domain/
      ddex.go
      track.go
      user.go
    pkg/
      audio/
    repository/
      cached/
  migrations/
    000001_create_tracks_table.down.sql
    000001_create_tracks_table.up.sql
    000002_create_users_table.down.sql
    000002_create_users_table.up.sql
  monitoring/
    alertmanager/
      templates/
        slack.tmpl
      alertmanager.yml
    grafana/
      dashboards/
        metadata_service.json
      provisioning/
        dashboards/
          dashboards.yml
        datasources/
          prometheus.yml
    prometheus/
      prometheus.yml
      rules.yml
  tools/
    llm_api.py
    screenshot_utils.py
    search_engine.py
    web_scraper.py
  .cursorrules
  docker-compose.monitoring.yml
  go.mod
  README.md
  requirements.txt
monitoring/
  alertmanager/
    alertmanager.yml
  grafana/
    dashboards/
      metadatatool.json
    provisioning/
      dashboards/
        provider.yml
      datasources/
        prometheus.yml
  prometheus/
    rules/
      alerts.yml
    prometheus.yml
scripts/
  setup_dev.sh
  start_pubsub_emulator.sh
.gitignore
.golangci.yml
cursorrules
DEBUGGING.md
docker-compose.monitoring.yml
Dockerfile
go.mod
LICENSE
PRD.md
README.md
requirements.txt
TODO_DETAILED.md
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="cmd/api/main.go">
package main
import (
	"context"
	"database/sql"
	"fmt"
	"log"
	"metadatatool/internal/handler"
	"metadatatool/internal/handler/middleware"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/errortracking"
	"metadatatool/internal/pkg/logger"
	"metadatatool/internal/pkg/metrics"
	"metadatatool/internal/pkg/session"
	"metadatatool/internal/pkg/tracing"
	"metadatatool/internal/pkg/validator"
	"metadatatool/internal/repository/ai"
	"metadatatool/internal/repository/base"
	"metadatatool/internal/repository/cached"
	queuepkg "metadatatool/internal/repository/queue"
	storagepkg "metadatatool/internal/repository/storage"
	"metadatatool/internal/usecase"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"
	"github.com/gin-gonic/gin"
	_ "github.com/lib/pq"
	goredis "github.com/redis/go-redis/v9"
)
func main() {
	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}
	// Initialize logger
	log := logger.NewLogger()
	// Initialize error tracking
	errorTracker := errortracking.NewErrorTracker()
	// Initialize Redis client
	redisClient := goredis.NewClient(&goredis.Options{
		Addr:     cfg.Redis.GetAddress(),
		Password: cfg.Redis.Password,
		DB:       cfg.Redis.DB,
	})
	// Test Redis connection
	if err := redisClient.Ping(context.Background()).Err(); err != nil {
		log.Fatalf("Failed to connect to Redis: %v", err)
	}
	// Initialize queue service
	queueConfig := &queuepkg.PubSubConfig{
		ProjectID:          cfg.Queue.ProjectID,
		HighPriorityTopic:  cfg.Queue.HighPriorityTopic,
		LowPriorityTopic:   cfg.Queue.LowPriorityTopic,
		DeadLetterTopic:    cfg.Queue.DeadLetterTopic,
		SubscriptionPrefix: cfg.Queue.SubscriptionPrefix,
		MaxRetries:         cfg.Queue.MaxRetries,
		AckDeadline:        cfg.Queue.AckDeadline,
		RetentionDuration:  cfg.Queue.RetentionDuration,
	}
	queueMetrics := metrics.NewQueueMetrics()
	queueService, err := queuepkg.NewPubSubService(context.Background(), queueConfig, queueMetrics)
	if err != nil {
		log.Fatalf("Failed to initialize queue service: %v", err)
	}
	defer queueService.Close()
	// Initialize PostgreSQL connection
	dbDSN := fmt.Sprintf(
		"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
		cfg.Database.Host, cfg.Database.Port, cfg.Database.User,
		cfg.Database.Password, cfg.Database.DBName,
	)
	db, err := sql.Open("postgres", dbDSN)
	if err != nil {
		log.Fatalf("Failed to connect to database: %v", err)
	}
	defer db.Close()
	// Test database connection
	if err := db.Ping(); err != nil {
		log.Fatalf("Failed to ping database: %v", err)
	}
	// Initialize repositories
	baseTrackRepo := base.NewTrackRepository(db)
	trackRepo := cached.NewTrackRepository(redisClient, baseTrackRepo)
	baseUserRepo := base.NewUserRepository(db)
	userRepo := cached.NewUserRepository(redisClient, baseUserRepo)
	sessionStore := session.NewRedisSessionStore(redisClient, configToDomainSession(cfg.Session))
	// Initialize services
	authService := usecase.NewAuthService(&cfg.Auth, userRepo)
	storageService, err := storagepkg.NewS3Storage(&cfg.Storage)
	if err != nil {
		log.Fatalf("Failed to initialize storage service: %v", err)
	}
	// Initialize tracing
	tp, err := tracing.NewProvider(context.Background(), &tracing.Config{
		Enabled:     cfg.Tracing.Enabled,
		ServiceName: cfg.Tracing.ServiceName,
		Endpoint:    cfg.Tracing.Endpoint,
		SampleRate:  cfg.Tracing.SampleRate,
		BatchSize:   512,
		Timeout:     30 * time.Second,
	})
	if err != nil {
		log.Fatalf("Failed to initialize tracing: %v", err)
	}
	defer tp.Shutdown(context.Background())
	// Initialize AI service
	aiConfig := &domain.OpenAIConfig{
		APIKey:                cfg.AI.APIKey,
		Endpoint:              cfg.AI.BaseURL,
		TimeoutSeconds:        int(cfg.AI.Timeout.Seconds()),
		MinConfidence:         cfg.AI.MinConfidence,
		MaxConcurrentRequests: cfg.AI.BatchSize,
		RetryAttempts:         3,
		RetryBackoffSeconds:   1,
	}
	aiService, err := ai.NewOpenAIService(aiConfig)
	if err != nil {
		log.Fatalf("Failed to initialize AI service: %v", err)
	}
	// Initialize use cases
	authUseCase := usecase.NewAuthUseCase(userRepo, sessionStore, authService)
	userUseCase := usecase.NewUserUseCase(userRepo)
	// Initialize validator
	validator := validator.NewValidator()
	// Initialize handlers
	healthHandler := handler.NewHealthHandler(redisClient)
	authHandler := handler.NewAuthHandler(
		authUseCase,
		userUseCase,
		sessionStore,
	)
	trackHandler := handler.NewTrackHandler(trackRepo, aiService, storageService, validator, errorTracker)
	metricsHandler := handler.NewMetricsHandler()
	// Initialize router
	router := gin.New()
	router.Use(gin.Recovery())
	router.Use(middleware.Tracing())
	router.Use(middleware.Metrics())
	// Initialize routes
	initRoutes(
		router,
		healthHandler,
		authHandler,
		trackHandler,
		metricsHandler,
		authService,
		userRepo,
		sessionStore,
		configToDomainSession(cfg.Session),
	)
	// Start server
	srv := &http.Server{
		Addr:    cfg.Server.Address,
		Handler: router,
	}
	go func() {
		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			log.Fatalf("Failed to start server: %v", err)
		}
	}()
	// Wait for interrupt signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	// Shutdown server
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	if err := srv.Shutdown(ctx); err != nil {
		log.Fatalf("Server forced to shutdown: %v", err)
	}
}
func initRoutes(
	router *gin.Engine,
	healthHandler *handler.HealthHandler,
	authHandler *handler.AuthHandler,
	trackHandler *handler.TrackHandler,
	metricsHandler *handler.MetricsHandler,
	authService domain.AuthService,
	userRepo domain.UserRepository,
	sessionStore domain.SessionStore,
	sessionConfig domain.SessionConfig,
) {
	// Add basic middleware
	router.Use(gin.Logger())
	router.Use(gin.Recovery())
	// Metrics endpoints
	router.GET("/metrics", metricsHandler.PrometheusHandler())
	router.GET("/health", metricsHandler.HealthCheck)
	// Health check
	router.GET("/health", healthHandler.Check)
	// Public routes
	auth := router.Group("/api/v1/auth")
	{
		auth.POST("/register", authHandler.Register)
		auth.POST("/login", middleware.CreateSession(sessionStore, sessionConfig), authHandler.Login)
		auth.POST("/refresh", authHandler.RefreshToken)
		auth.POST("/logout", middleware.ClearSession(sessionStore, sessionConfig), authHandler.Logout)
		// Session management routes (protected)
		sessions := auth.Group("")
		sessions.Use(middleware.Auth(authService))
		sessions.Use(middleware.Session(sessionStore, sessionConfig))
		{
			sessions.GET("/sessions", authHandler.GetActiveSessions)
			sessions.DELETE("/sessions/:id", authHandler.RevokeSession)
			sessions.DELETE("/sessions", authHandler.RevokeAllSessions)
		}
	}
	// Protected routes
	api := router.Group("/api/v1")
	api.Use(middleware.APIKeyAuth(userRepo))
	// Track routes
	tracks := api.Group("/tracks")
	{
		// Public endpoints
		tracks.GET("", trackHandler.ListTracks)
		tracks.GET("/:id", trackHandler.GetTrack)
		// Protected endpoints (require authentication)
		authenticated := tracks.Group("")
		authenticated.Use(middleware.Auth(authService))
		{
			authenticated.POST("", trackHandler.CreateTrack)
			authenticated.PUT("/:id", trackHandler.UpdateTrack)
			authenticated.DELETE("/:id", trackHandler.DeleteTrack)
		}
		// Admin-only endpoints
		admin := authenticated.Group("")
		admin.Use(middleware.RequireRole(domain.RoleAdmin))
		{
			admin.POST("/batch", trackHandler.BatchProcess)
			admin.POST("/export", trackHandler.ExportTracks)
		}
	}
	// Audio routes
	audio := api.Group("/audio")
	audio.Use(middleware.Auth(authService))
	{
		audio.POST("/upload", trackHandler.UploadAudio)
		audio.GET("/:id", trackHandler.GetAudioURL)
	}
	// DDEX routes (admin only)
	ddex := api.Group("/ddex")
	ddex.Use(middleware.Auth(authService))
	ddex.Use(middleware.RequireRole(domain.RoleAdmin))
	{
		ddex.POST("/validate", trackHandler.ValidateERN)
		ddex.POST("/import", trackHandler.ImportERN)
		ddex.POST("/export", trackHandler.ExportERN)
	}
}
func configToDomainSession(cfg config.SessionConfig) domain.SessionConfig {
	return domain.SessionConfig{
		CookieName:         cfg.CookieName,
		CookieDomain:       cfg.CookieDomain,
		CookiePath:         cfg.CookiePath,
		CookieSecure:       cfg.CookieSecure,
		CookieHTTPOnly:     cfg.CookieHTTPOnly,
		CookieSameSite:     cfg.CookieSameSite,
		SessionDuration:    cfg.SessionDuration,
		CleanupInterval:    cfg.CleanupInterval,
		MaxSessionsPerUser: cfg.MaxSessionsPerUser,
	}
}
</file>

<file path="cmd/metadatatool/main.go">
// Package main implements a command-line interface for the metadata tool.
//
// The metadatatool CLI provides functionality to:
//   - Enrich track metadata using AI services
//   - Validate track metadata against quality standards
//   - Export tracks in various formats (JSON, DDEX)
//
// Usage:
//
//	metadatatool -action=enrich -track=<track_id>
//	metadatatool -action=validate -track=<track_id>
//	metadatatool -action=export -track=<track_id> -format=[json|ddex]
//	metadatatool -action=export -batch=<file> -format=[json|ddex]
//
// Environment Variables:
//   - DB_HOST: PostgreSQL host
//   - DB_PORT: PostgreSQL port
//   - DB_USER: PostgreSQL user
//   - DB_PASSWORD: PostgreSQL password
//   - DB_NAME: PostgreSQL database name
//   - AI_API_KEY: API key for AI services
//   - AI_BASE_URL: Base URL for AI services
//   - BIGQUERY_PROJECT: Google Cloud project ID
//   - BIGQUERY_DATASET: BigQuery dataset name
package main
import (
	"context"
	"database/sql"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"metadatatool/internal/pkg/analytics"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/ddex"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/repository/ai"
	"metadatatool/internal/repository/base"
	"metadatatool/internal/usecase"
	"os"
	"strings"
	_ "github.com/lib/pq"
)
// Command line flags
type flags struct {
	trackID   *string // ID of the track to process
	action    *string // Action to perform (enrich, validate, export)
	format    *string // Export format (json, ddex)
	batchFile *string // File containing list of track IDs to process
}
// parseFlags parses and validates command line flags
func parseFlags() *flags {
	f := &flags{
		trackID:   flag.String("track", "", "Track ID to process"),
		action:    flag.String("action", "", "Action to perform (enrich, validate, export)"),
		format:    flag.String("format", "json", "Export format (json, ddex)"),
		batchFile: flag.String("batch", "", "File containing list of track IDs to process"),
	}
	flag.Parse()
	return f
}
func main() {
	flags := parseFlags()
	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}
	// Initialize services
	services, err := initializeServices(cfg)
	if err != nil {
		log.Fatalf("Failed to initialize services: %v", err)
	}
	defer services.cleanup()
	// Process command
	ctx := context.Background()
	if err := processCommand(ctx, flags, services); err != nil {
		log.Fatalf("Command failed: %v", err)
	}
}
// services holds all the initialized services needed by the CLI
type services struct {
	analytics *analytics.BigQueryService
	ai        domain.AIService
	tracks    domain.TrackRepository
	ddex      domain.DDEXService
	db        *sql.DB
}
// cleanup performs cleanup of all services
func (s *services) cleanup() {
	if s.analytics != nil {
		s.analytics.Close()
	}
	if s.db != nil {
		s.db.Close()
	}
}
// initializeServices initializes all required services
func initializeServices(cfg *config.AppConfig) (*services, error) {
	// Initialize BigQuery analytics
	analyticsService, err := analytics.NewBigQueryService(
		cfg.Queue.ProjectID,
		cfg.Queue.HighPriorityTopic,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize BigQuery: %w", err)
	}
	// Initialize AI service
	aiConfig := &ai.Config{
		EnableFallback:        cfg.AI.Experiment.EnableFallback,
		TimeoutSeconds:        int(cfg.AI.Timeout.Seconds()),
		MinConfidence:         cfg.AI.MinConfidence,
		MaxConcurrentRequests: cfg.AI.BatchSize,
		RetryAttempts:         3,
		RetryBackoffSeconds:   5,
		OpenAIConfig: &ai.OpenAIConfig{
			APIKey:    cfg.AI.APIKey,
			Endpoint:  cfg.AI.BaseURL,
			Model:     cfg.AI.ModelName,
			MaxTokens: cfg.AI.MaxTokens,
		},
		Qwen2Config: &ai.Qwen2Config{
			APIKey:    cfg.AI.APIKey,
			Endpoint:  cfg.AI.BaseURL,
			Model:     cfg.AI.ModelName,
			MaxTokens: cfg.AI.MaxTokens,
		},
	}
	aiService, err := ai.NewCompositeAIService(aiConfig, analyticsService)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize AI service: %w", err)
	}
	// Initialize database connection
	db, err := sql.Open("postgres", fmt.Sprintf(
		"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
		cfg.Database.Host,
		cfg.Database.Port,
		cfg.Database.User,
		cfg.Database.Password,
		cfg.Database.DBName,
	))
	if err != nil {
		return nil, fmt.Errorf("failed to connect to database: %w", err)
	}
	// Initialize repositories and services
	trackRepo := base.NewTrackRepository(db)
	// Initialize DDEX service
	schemaValidator := ddex.NewXMLSchemaValidator()
	ddexConfig := &usecase.DDEXConfig{
		MessageSender:    "MetadataTool",
		MessageRecipient: "DSP",
		SchemaPath:       "schemas/ddex/ern/4.3/release-notification.xsd",
		ValidateSchema:   true,
	}
	ddexService := usecase.NewDDEXService(schemaValidator, ddexConfig)
	return &services{
		analytics: analyticsService,
		ai:        aiService,
		tracks:    trackRepo,
		ddex:      ddexService,
		db:        db,
	}, nil
}
// processCommand processes the CLI command based on the provided flags
func processCommand(ctx context.Context, f *flags, s *services) error {
	switch *f.action {
	case "enrich":
		if *f.trackID == "" {
			return fmt.Errorf("track ID is required for enrich action")
		}
		return enrichTrack(ctx, *f.trackID, s.tracks, s.ai)
	case "validate":
		if *f.trackID == "" {
			return fmt.Errorf("track ID is required for validate action")
		}
		return validateTrack(ctx, *f.trackID, s.tracks, s.ai)
	case "export":
		if *f.trackID == "" && *f.batchFile == "" {
			return fmt.Errorf("either track ID or batch file is required for export action")
		}
		return exportTracks(ctx, *f.trackID, *f.batchFile, *f.format, s.tracks, s.ddex)
	default:
		printUsage()
		return fmt.Errorf("invalid action: %s", *f.action)
	}
}
// printUsage prints the CLI usage information
func printUsage() {
	fmt.Println("Available commands:")
	fmt.Println("  metadatatool -action=enrich -track=<track_id>")
	fmt.Println("  metadatatool -action=validate -track=<track_id>")
	fmt.Println("  metadatatool -action=export -track=<track_id> -format=[json|ddex]")
	fmt.Println("  metadatatool -action=export -batch=<file> -format=[json|ddex]")
}
// enrichTrack enriches a track's metadata using AI services
func enrichTrack(ctx context.Context, trackID string, repo domain.TrackRepository, ai domain.AIService) error {
	track, err := repo.GetByID(ctx, trackID)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	return ai.EnrichMetadata(ctx, track)
}
// validateTrack validates a track's metadata using AI services
func validateTrack(ctx context.Context, trackID string, repo domain.TrackRepository, ai domain.AIService) error {
	track, err := repo.GetByID(ctx, trackID)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	confidence, err := ai.ValidateMetadata(ctx, track)
	if err != nil {
		return err
	}
	fmt.Printf("Track validation confidence: %.2f\n", confidence)
	return nil
}
// exportTracks exports tracks in the specified format
func exportTracks(ctx context.Context, trackID, batchFile, format string, repo domain.TrackRepository, ddex domain.DDEXService) error {
	var tracks []*domain.Track
	if trackID != "" {
		track, err := repo.GetByID(ctx, trackID)
		if err != nil {
			return fmt.Errorf("failed to get track: %w", err)
		}
		tracks = append(tracks, track)
	} else if batchFile != "" {
		// Read track IDs from batch file
		content, err := os.ReadFile(batchFile)
		if err != nil {
			return fmt.Errorf("failed to read batch file: %w", err)
		}
		// Parse track IDs (assuming one ID per line)
		trackIDs := strings.Split(strings.TrimSpace(string(content)), "\n")
		for _, id := range trackIDs {
			id = strings.TrimSpace(id)
			if id == "" {
				continue
			}
			track, err := repo.GetByID(ctx, id)
			if err != nil {
				return fmt.Errorf("failed to get track %s: %w", id, err)
			}
			if track != nil {
				tracks = append(tracks, track)
			}
		}
	} else {
		return fmt.Errorf("either track ID or batch file is required")
	}
	if len(tracks) == 0 {
		return fmt.Errorf("no tracks found to export")
	}
	if format == "ddex" {
		output, err := ddex.ExportTracks(ctx, tracks)
		if err != nil {
			return fmt.Errorf("failed to export tracks to DDEX: %w", err)
		}
		fmt.Println(output)
	} else if format == "json" {
		// Implement JSON export
		jsonData, err := json.MarshalIndent(tracks, "", "  ")
		if err != nil {
			return fmt.Errorf("failed to marshal tracks to JSON: %w", err)
		}
		fmt.Println(string(jsonData))
	} else {
		return fmt.Errorf("unsupported format: %s", format)
	}
	return nil
}
</file>

<file path="deployments/alertmanager/config.yml">
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK_URL'
route:
  group_by: ['alertname', 'job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'slack-notifications'
  routes:
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 10s
      repeat_interval: 1h
receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#queue-alerts'
        send_resolved: true
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        footer: '{{ template "slack.default.footer" . }}'
  - name: 'slack-critical'
    slack_configs:
      - channel: '#queue-alerts-critical'
        send_resolved: true
        title: '[CRITICAL] {{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        footer: '{{ template "slack.default.footer" . }}'
templates:
  - '/etc/alertmanager/template/*.tmpl'
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'job']
</file>

<file path="deployments/grafana/dashboards/auth.json">
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "sum"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(auth_attempts_total{status=\"success\"}[5m])",
          "legendFormat": "{{method}}",
          "refId": "A"
        }
      ],
      "title": "Successful Authentication Rate (5m)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "red",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "sum"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(auth_attempts_total{status=\"failure\"}[5m])",
          "legendFormat": "{{method}}",
          "refId": "A"
        }
      ],
      "title": "Failed Authentication Rate (5m)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "yellow",
                "value": 1000
              },
              {
                "color": "red",
                "value": 5000
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "10.0.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "active_sessions_total",
          "refId": "A"
        }
      ],
      "title": "Active Sessions",
      "type": "gauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": ["mean", "p95", "max"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(auth_operation_duration_seconds_sum[5m]) / rate(auth_operation_duration_seconds_count[5m])",
          "legendFormat": "{{operation}}",
          "refId": "A"
        }
      ],
      "title": "Authentication Operation Latency (5m avg)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "bars",
            "fillOpacity": 100,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 0,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 16
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": ["sum"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(increase(permission_checks_total[5m])) by (permission, status)",
          "legendFormat": "{{permission}} - {{status}}",
          "refId": "A"
        }
      ],
      "title": "Permission Checks (5m)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "bars",
            "fillOpacity": 100,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 0,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": ["sum"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(increase(role_checks_total[5m])) by (role, status)",
          "legendFormat": "{{role}} - {{status}}",
          "refId": "A"
        }
      ],
      "title": "Role Checks (5m)",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["auth", "security"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Authentication & Authorization",
  "uid": "auth",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="deployments/grafana/dashboards/queue-metrics.json">
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(queue_messages_published_total[5m])",
          "legendFormat": "{{topic}}",
          "refId": "A"
        }
      ],
      "title": "Message Publish Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "p95"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(queue_processing_latency_seconds_sum[5m]) / rate(queue_processing_latency_seconds_count[5m])",
          "legendFormat": "{{subscription}}",
          "refId": "A"
        }
      ],
      "title": "Message Processing Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(queue_dead_letters_total[5m])",
          "legendFormat": "{{type}}",
          "refId": "A"
        }
      ],
      "title": "Dead Letter Rate",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["queue", "monitoring"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Queue Metrics",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="deployments/grafana/provisioning/dashboards/auth.yml">
apiVersion: 1
providers:
  - name: 'Authentication & Authorization'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
</file>

<file path="deployments/grafana/provisioning/datasources/prometheus.yml">
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    version: 1
</file>

<file path="deployments/otel/config.yaml">
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
  memory_limiter:
    check_interval: 1s
    limit_mib: 1000
  resourcedetection:
    detectors: [env, system]
    timeout: 2s
exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: "otel"
    send_timestamps: true
    metric_expiration: 180m
    resource_to_telemetry_conversion:
      enabled: true
  logging:
    loglevel: debug
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [logging]
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [prometheus, logging]
</file>

<file path="deployments/prometheus/rules/queue_alerts.yml">
groups:
  - name: queue_alerts
    rules:
      - alert: HighMessageProcessingLatency
        expr: |
          rate(queue_processing_latency_seconds_sum[5m]) / 
          rate(queue_processing_latency_seconds_count[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High message processing latency
          description: Message processing latency is above 5 seconds for subscription {{ $labels.subscription }}
      - alert: HighDeadLetterRate
        expr: rate(queue_dead_letters_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High dead letter rate
          description: Dead letter rate is above 10 messages/minute for message type {{ $labels.type }}
      - alert: HighErrorRate
        expr: |
          (
            rate(queue_publish_errors_total[5m]) +
            rate(queue_processing_errors_total[5m])
          ) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High error rate in queue operations
          description: Error rate is above 5 errors/minute for queue operations
      - alert: QueueProcessingStalled
        expr: |
          rate(queue_messages_processed_total[15m]) == 0 and
          rate(queue_messages_published_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Queue processing has stalled
          description: Messages are being published but not processed for subscription {{ $labels.subscription }}
      - alert: SubscriptionErrors
        expr: rate(queue_subscription_errors_total[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Subscription errors detected
          description: Subscription errors detected for {{ $labels.subscription }}
</file>

<file path="deployments/prometheus/prometheus.yml">
global:
  scrape_interval: 15s
  evaluation_interval: 15s
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093
rule_files:
  - "rules/*.yml"
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8889']
  - job_name: 'metadata-tool'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/metrics'
    scheme: 'http'
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'metadata-tool'
</file>

<file path="deployments/docker-compose.monitoring.yml">
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "9090:9090"
    restart: unless-stopped
    networks:
      - monitoring
  grafana:
    image: grafana/grafana:10.0.0
    container_name: grafana
    volumes:
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/auth.json
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - monitoring
volumes:
  prometheus_data:
  grafana_data:
networks:
  monitoring:
    driver: bridge
  otel-collector:
    image: otel/opentelemetry-collector:0.96.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel/config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
    depends_on:
      - prometheus
    restart: unless-stopped
  alertmanager:
    image: prom/alertmanager:v0.27.0
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/config.yml:/etc/alertmanager/config.yml
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped
</file>

<file path="docs/architecture/components/ai_service.md">
# AI Service Architecture

## Overview
The AI service is responsible for metadata enrichment, validation, and batch processing of audio tracks. It implements the `domain.AIService` interface and supports multiple AI providers (OpenAI and Qwen2) with fallback capabilities.

## Components

### 1. Service Interface
```go
type AIService interface {
    EnrichMetadata(ctx context.Context, track *Track) error
    ValidateMetadata(ctx context.Context, track *Track) (float64, error)
    BatchProcess(ctx context.Context, tracks []*Track) error
}
```

### 2. Implementation Structure
- **OpenAI Service**: Primary implementation using OpenAI's API
- **Qwen2 Service**: Alternative implementation using Qwen2
- **Composite Service**: Manages multiple providers with fallback support

### 3. Key Features

#### Metadata Enrichment
- Genre classification
- Mood detection
- BPM analysis
- Key detection
- Confidence scoring

#### Validation
- Metadata completeness check
- Format validation
- Rights management validation
- DDEX compliance check

#### Batch Processing
- Parallel processing with goroutines
- Error aggregation
- Progress tracking
- Automatic retries

### 4. Configuration

```go
type AIConfig struct {
    ModelName     string
    ModelVersion  string
    Temperature   float64
    MaxTokens     int
    BatchSize     int
    MinConfidence float64
    APIKey        string
    BaseURL       string
    Timeout       time.Duration
}
```

### 5. Metrics

The service tracks the following metrics:
- Request duration
- Success/failure rates
- Model version usage
- Confidence score distribution
- Batch processing performance

### 6. Error Handling

#### Error Types
- `ValidationError`: Metadata validation failures
- `EnrichmentError`: AI enrichment failures
- `BatchProcessError`: Batch processing failures
- `ConfigurationError`: Service configuration issues

#### Retry Strategy
- Exponential backoff
- Maximum retry attempts
- Failure thresholds

### 7. Usage Examples

#### Enriching Track Metadata
```go
track := &domain.Track{
    Title:  "Example Track",
    Artist: "Example Artist",
}
err := aiService.EnrichMetadata(ctx, track)
```

#### Validating Metadata
```go
confidence, err := aiService.ValidateMetadata(ctx, track)
if confidence < minConfidence {
    // Handle low confidence case
}
```

#### Batch Processing
```go
tracks := []*domain.Track{...}
err := aiService.BatchProcess(ctx, tracks)
```

## Future Improvements

1. **Model Versioning**
   - Version tracking in metadata
   - Migration strategy
   - Confidence score normalization

2. **Batch Processing**
   - Enhanced retry logic
   - Better progress tracking
   - Improved error reporting

3. **Performance Optimization**
   - Request batching
   - Caching of common results
   - Parallel processing improvements

4. **Monitoring**
   - Enhanced metrics
   - Alert configuration
   - Performance dashboards
```
</file>

<file path="docs/architecture/components/cache.md">
# Cache Service Architecture

## Overview
The Cache Service provides a high-performance caching layer using Redis, implementing probabilistic cache refresh and supporting various caching strategies for metadata, file information, and frequently accessed data.

## Components

### 1. Service Interface
```go
type CacheService interface {
    Get(key string) ([]byte, error)
    Set(key string, value []byte, expiration time.Duration) error
    Delete(key string) error
    PreWarm(keys []string) error
    RefreshProbabilistic(key string, threshold time.Duration, probability float64) error
}
```

### 2. Implementation Structure
- **Redis Cache**: Primary implementation using Redis
- **Local Cache**: Development/testing implementation
- **Composite Cache**: Multi-level caching support

### 3. Key Features

#### Cache Management
- Key-value storage
- TTL-based expiration
- Probabilistic refresh
- Cache warming
- Batch operations

#### Performance Optimization
- Connection pooling
- Pipelining support
- Compression options
- Memory optimization

#### Monitoring
- Hit/miss ratio tracking
- Memory usage monitoring
- Operation latency tracking
- Error rate monitoring

### 4. Configuration

```go
type CacheConfig struct {
    // Redis settings
    Host     string
    Port     int
    Password string
    DB       int

    // Cache settings
    DefaultTTL      time.Duration
    MaxMemory       string
    MaxMemoryPolicy string
    
    // Performance settings
    PoolSize        int
    MinIdleConns    int
    ConnectTimeout  time.Duration
    ReadTimeout     time.Duration
    WriteTimeout    time.Duration

    // Refresh settings
    RefreshInterval time.Duration
    RefreshProb     float64
    TTLThreshold    time.Duration
}
```

### 5. Metrics

The service tracks the following metrics:
- Cache hit/miss rates
- Operation latency
- Memory usage
- Eviction counts
- Error rates

### 6. Error Handling

#### Error Types
- `CacheError`: Base error type for cache operations
- `ConnectionError`: Redis connection issues
- `SerializationError`: Data serialization failures
- `MemoryError`: Out of memory conditions

#### Recovery Strategy
- Automatic reconnection
- Circuit breaking
- Fallback to source
- Error rate monitoring

### 7. Usage Examples

#### Basic Cache Operations
```go
// Setting a value
err := cacheService.Set("user:123", userData, 1*time.Hour)

// Getting a value
data, err := cacheService.Get("user:123")

// Deleting a value
err := cacheService.Delete("user:123")
```

#### Pre-warming Cache
```go
keys := []string{
    "popular:tracks",
    "top:artists",
    "recent:uploads",
}
err := cacheService.PreWarm(keys)
```

#### Probabilistic Refresh
```go
err := cacheService.RefreshProbabilistic(
    "stats:daily",
    30*time.Minute,
    0.1, // 10% chance of refresh when TTL < threshold
)
```

## Caching Strategies

### 1. Metadata Caching
- Track metadata
- User information
- File metadata
- API responses

### 2. Query Results Caching
- Search results
- Filtered lists
- Aggregated data
- Report data

### 3. Session Data
- User sessions
- API tokens
- Rate limit data
- Temporary states

## Future Improvements

1. **Advanced Caching**
   - Implement cache-aside pattern
   - Add write-through caching
   - Support cache stampede prevention
   - Implement cache coherence protocols

2. **Performance**
   - Add support for Redis Cluster
   - Implement cache sharding
   - Add compression options
   - Optimize memory usage

3. **Monitoring**
   - Enhanced cache analytics
   - Real-time monitoring
   - Cache efficiency metrics
   - Cost analysis tools

4. **Reliability**
   - Implement cache replication
   - Add failover support
   - Enhance error handling
   - Add circuit breakers

5. **Integration**
   - GraphQL caching
   - REST response caching
   - File metadata caching
   - Session management
</file>

<file path="docs/architecture/components/ddex.md">
# DDEX Service Architecture

## Overview
The DDEX Service handles DDEX ERN (Electronic Release Notification) message creation, validation, and processing. It implements the DDEX ERN 4.3 standard for digital music delivery and supports both import and export operations.

## Components

### 1. Service Interface
```go
type DDEXService interface {
    ValidateTrack(ctx context.Context, track *Track) (bool, []string)
    ExportTrack(ctx context.Context, track *Track) (string, error)
    ExportTracks(ctx context.Context, tracks []*Track) (string, error)
}
```

### 2. Implementation Structure
- **DDEX Service**: Core implementation of DDEX functionality
- **ERN Message**: DDEX ERN message structure
- **Validation**: DDEX schema validation
- **Export**: ERN XML generation

### 3. Key Features

#### Message Handling
- ERN 4.3 message creation
- XML validation
- Schema compliance
- Batch processing

#### Validation
- Metadata completeness
- Format compliance
- Rights validation
- Territory checks

#### Export
- Single track export
- Batch export
- Custom formatting
- Error reporting

### 4. Message Structure

```go
type ERNMessage struct {
    MessageHeader MessageHeader
    ResourceList  ResourceList
    ReleaseList   ReleaseList
    DealList      DealList
}

type MessageHeader struct {
    MessageID              string
    MessageSender          string
    MessageRecipient       string
    MessageCreatedDateTime string
}

type ResourceList struct {
    SoundRecordings []SoundRecording
}

type ReleaseList struct {
    Releases []Release
}

type DealList struct {
    ReleaseDeals []ReleaseDeal
}
```

### 5. Metrics

The service tracks the following metrics:
- Validation success/failure rates
- Export operation duration
- Message size statistics
- Error frequency by type
- Batch processing performance

### 6. Error Handling

#### Error Types
- `ValidationError`: DDEX validation failures
- `ExportError`: Export operation failures
- `SchemaError`: Schema compliance issues
- `FormatError`: Format conversion errors

#### Validation Rules
- Required field presence
- Format compliance
- Rights management
- Territory validation

### 7. Usage Examples

#### Validating a Track
```go
valid, errors := ddexService.ValidateTrack(ctx, track)
if !valid {
    for _, err := range errors {
        log.Printf("Validation error: %s", err)
    }
}
```

#### Exporting a Single Track
```go
ernXML, err := ddexService.ExportTrack(ctx, track)
if err != nil {
    log.Printf("Export failed: %v", err)
}
```

#### Batch Export
```go
ernXML, err := ddexService.ExportTracks(ctx, tracks)
if err != nil {
    log.Printf("Batch export failed: %v", err)
}
```

## DDEX Compliance

### 1. ERN 4.3 Standard
- Message format compliance
- Required fields
- Data type validation
- Relationship validation

### 2. Rights Management
- Territory rights
- Usage rights
- Commercial model types
- Deal terms

### 3. Resource Types
- Sound recordings
- Musical works
- Release bundles
- Deal information

## Future Improvements

1. **Enhanced Validation**
   - Advanced schema validation
   - Custom validation rules
   - Real-time validation
   - Validation caching

2. **Import Support**
   - ERN message import
   - Batch import
   - Format conversion
   - Error recovery

3. **Performance**
   - Parallel processing
   - Batch optimization
   - Memory efficiency
   - Response caching

4. **Integration**
   - DSP integration
   - Rights database integration
   - Metadata enrichment
   - Automated delivery

5. **Monitoring**
   - Enhanced error tracking
   - Performance monitoring
   - Usage analytics
   - Compliance reporting

## Best Practices

### 1. Message Creation
- Use consistent identifiers
- Include all required fields
- Validate before sending
- Handle special characters

### 2. Validation
- Validate early and often
- Provide clear error messages
- Cache validation results
- Log validation failures

### 3. Error Handling
- Provide detailed error information
- Support error recovery
- Maintain audit trails
- Enable debugging

### 4. Performance
- Use batch processing
- Implement caching
- Optimize large messages
- Monitor resource usage
</file>

<file path="docs/architecture/components/storage.md">
# Storage Service Architecture

## Overview
The Storage Service handles file storage, retrieval, and management for audio files and related assets. It implements the `domain.StorageService` interface and primarily uses S3-compatible storage with support for quotas, cleanup, and monitoring.

## Components

### 1. Service Interface
```go
type StorageService interface {
    Upload(ctx context.Context, key string, data io.Reader) error
    Download(ctx context.Context, key string) (io.ReadCloser, error)
    Delete(ctx context.Context, key string) error
    GetSignedURL(ctx context.Context, key string, operation SignedURLOperation, expiry time.Duration) (string, error)
    GetMetadata(ctx context.Context, key string) (*FileMetadata, error)
    ListFiles(ctx context.Context, prefix string) ([]*FileMetadata, error)
    GetQuotaUsage(ctx context.Context, userID string) (int64, error)
    ValidateUpload(ctx context.Context, filename string, size int64, userID string) error
    CleanupTempFiles(ctx context.Context) error
}
```

### 2. Implementation Structure
- **S3 Storage**: Primary implementation using AWS S3 or compatible services
- **Local Storage**: Development/testing implementation using local filesystem
- **Mock Storage**: Testing implementation for unit tests

### 3. Key Features

#### File Management
- Secure file upload/download
- Metadata extraction and storage
- File format validation
- Temporary file management
- Automatic cleanup

#### Quota Management
- Per-user storage quotas
- Total storage quota
- Usage tracking and alerts
- Quota enforcement

#### Security
- Pre-signed URLs
- Access control
- Encryption at rest
- Secure file transfer

### 4. Configuration

```go
type StorageConfig struct {
    // Provider settings
    Provider         string
    Region           string
    Bucket           string
    AccessKey        string
    SecretKey        string
    Endpoint         string
    UseSSL           bool
    UploadPartSize   int64
    MaxUploadRetries int

    // File restrictions
    MaxFileSize      int64
    AllowedFileTypes []string

    // Quota settings
    UserQuota       int64
    TotalQuota      int64
    QuotaWarningPct int

    // Cleanup settings
    TempFileExpiry  time.Duration
    CleanupInterval time.Duration

    // Performance settings
    UploadBufferSize int64
    DownloadTimeout  time.Duration
    UploadTimeout    time.Duration
}
```

### 5. Metrics

The service tracks the following metrics:
- Upload/download latency
- Storage operation success/failure rates
- Quota usage per user
- Temporary file count
- Storage operation errors

### 6. Error Handling

#### Error Types
- `StorageError`: Base error type for storage operations
- `QuotaExceededError`: User or total quota exceeded
- `ValidationError`: File validation failures
- `TimeoutError`: Operation timeout errors

#### Retry Strategy
- Configurable retry attempts
- Exponential backoff
- Operation-specific timeouts

### 7. Usage Examples

#### Uploading a File
```go
file := openFile("audio.mp3")
key := "uploads/user123/audio.mp3"
err := storageService.Upload(ctx, key, file)
```

#### Generating a Download URL
```go
url, err := storageService.GetSignedURL(ctx, key, domain.DownloadOperation, 1*time.Hour)
```

#### Checking Quota Usage
```go
usage, err := storageService.GetQuotaUsage(ctx, userID)
if usage > warningThreshold {
    // Send notification
}
```

## Future Improvements

1. **Enhanced Caching**
   - Implement caching layer for frequently accessed files
   - Cache metadata to reduce storage operations
   - Implement cache invalidation strategy

2. **Advanced File Processing**
   - Automatic format conversion
   - Audio quality validation
   - Metadata extraction pipeline

3. **Scalability**
   - Multi-region support
   - Cross-region replication
   - Load balancing

4. **Monitoring**
   - Enhanced usage analytics
   - Cost optimization metrics
   - Performance tracking

5. **Security**
   - Enhanced encryption options
   - Fine-grained access control
   - Audit logging
</file>

<file path="docs/architecture/ai_processing.md">
# AI Processing Architecture

## Overview

The metadata tool implements a robust AI processing system using a combination of Qwen2-Audio (primary) and OpenAI (fallback) models, orchestrated through Google Cloud Platform services.

## Core Components

### 1. Queue System (Google Pub/Sub)

#### Priority Queues
- **High Priority Queue**
  - Real-time API requests
  - Target latency: <1s
  - Used by Enterprise tier customers
  
- **Low Priority Queue**
  - Batch processing requests
  - Target processing time: <10 minutes
  - Used for bulk uploads and free tier processing

#### Dead Letter Queue (DLQ)
- Handles failed processing attempts
- Maximum retries: 3
- Retention period: 7 days
- Manual intervention triggers
- Failure analytics and monitoring

### 2. AI Model Integration

#### Primary: Qwen2-Audio
- Self-hosted solution
- Real-time processing capability
- Confidence threshold: 80%
- Metrics tracking:
  - Inference speed
  - Confidence scores
  - Error rates

#### Fallback: OpenAI
- Triggered when:
  - Qwen2 confidence < 80%
  - Qwen2 processing failure
  - System degradation
- Cost optimization through selective usage

### 3. A/B Testing Infrastructure

#### Test Configuration
- 90% traffic to primary model
- 10% traffic to experimental model
- Metrics stored in BigQuery
- Real-time performance comparison

#### Metrics Tracked
- Inference speed
- Confidence scores
- Error rates
- Cost per request
- Processing success rate

### 4. File Retention Policies

#### Tier-based Retention
- Free Tier: 30 days
- Pro Tier: 6 months
- Enterprise: Configurable per customer

#### Cleanup Process
- Automated cleanup system
- 7-day advance notification
- Extension options for Enterprise users
- Temporary file management (24-hour retention)

## Processing Flow

```mermaid
graph TD
    A[Audio Upload] --> B[Priority Router]
    B -->|High Priority| C[Real-time Queue]
    B -->|Low Priority| D[Batch Queue]
    C --> E[Qwen2-Audio]
    D --> E
    E -->|Confidence < 80%| F[OpenAI Fallback]
    E -->|Success| G[Metadata Storage]
    F --> G
    E -->|Failure x3| H[Dead Letter Queue]
```

## Monitoring & Observability

### Prometheus Metrics
- AI response time
- Queue size
- Processing success rate
- Confidence score distribution
- Error rates by type

### Alert Conditions
- Confidence score < 70% for >20% of requests
- Queue size > 500 pending jobs
- Processing time > SLA
- Error rate > 5%

### Dashboards
- Real-time processing metrics
- A/B test comparisons
- Queue health monitoring
- Cost analysis

## Scaling Strategy

### Queue Processing
- Auto-scaling based on queue size
- Separate scaling policies for high/low priority
- Regional deployment for latency optimization

### AI Processing
- Horizontal scaling of Qwen2 instances
- Load balancing across processing nodes
- Automatic failover to OpenAI

## Cost Optimization

### Processing Strategies
- Batch processing for non-urgent requests
- Selective use of OpenAI fallback
- Caching of common results
- Resource auto-scaling

### Storage Optimization
- Tiered storage classes
- Automatic cleanup of temporary files
- Compression for long-term storage

## Security & Compliance

### Data Protection
- Encryption at rest and in transit
- Access control based on subscription tier
- Audit logging of all operations

### Compliance
- GDPR-compliant data handling
- Configurable retention policies
- Data residency options for Enterprise

## Future Enhancements

### Planned Features
- Multi-region processing
- Enhanced A/B testing framework
- Custom model training pipeline
- Advanced batch processing optimizations

### Scalability Improvements
- Global load balancing
- Cross-region replication
- Enhanced caching strategies
- Predictive auto-scaling
</file>

<file path="docs/architecture/implementation.md">
# Technical Implementation Details

## System Components

### 1. Domain Layer (`internal/pkg/domain/`)

#### AI Service Interfaces
```go
// ai.go
type AIService interface {
    EnrichMetadata(ctx context.Context, track *Track) error
    ValidateMetadata(ctx context.Context, track *Track) (float64, error)
    BatchProcess(ctx context.Context, tracks []*Track) error
}

type AIProcessingResult struct {
    ModelVersion    string
    Confidence     float64
    ProcessingTime time.Duration
    IsExperimental bool
    RetryCount     int
    Error          error
}
```

#### Queue Service Interfaces
```go
// queue.go
type QueueService interface {
    Publish(ctx context.Context, topic string, message *QueueMessage, priority QueuePriority) error
    Subscribe(ctx context.Context, subscription string, handler MessageHandler) error
    HandleDeadLetter(ctx context.Context, message *QueueMessage) error
}
```

#### Storage Service Interfaces
```go
// storage.go
type StorageService interface {
    Upload(ctx context.Context, key string, data io.Reader) error
    Download(ctx context.Context, key string) (io.ReadCloser, error)
    Delete(ctx context.Context, key string) error
    GetSignedURL(ctx context.Context, key string, operation SignedURLOperation, expiry time.Duration) (string, error)
}
```

### 2. Infrastructure Layer

#### Google Cloud Integration
```go
// repository/queue/pubsub.go
type PubSubService struct {
    client    *pubsub.Client
    projectID string
    metrics   *metrics.QueueMetrics
}

// repository/analytics/bigquery.go
type BigQueryService struct {
    client     *bigquery.Client
    projectID  string
    dataset    string
    experiment *ExperimentTable
}
```

#### AI Services Implementation
```go
// repository/ai/qwen2_service.go
type Qwen2Service struct {
    config  *domain.Qwen2Config
    client  *Qwen2Client
    metrics *domain.AIMetrics
}

// repository/ai/openai_service.go
type OpenAIService struct {
    config  *domain.OpenAIConfig
    client  *OpenAIClient
    metrics *domain.AIMetrics
}
```

### 3. Monitoring Infrastructure

#### Prometheus Metrics
```go
// pkg/metrics/metrics.go
var (
    AIProcessingLatency = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "ai_processing_latency_seconds",
            Help: "Time taken for AI processing",
            Buckets: []float64{0.1, 0.5, 1, 2, 5, 10},
        },
        []string{"model", "operation"},
    )

    QueueSize = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "queue_size",
            Help: "Number of messages in queue",
        },
        []string{"priority", "status"},
    )
)
```

#### Alert Rules
```yaml
# monitoring/prometheus/rules.yml
groups:
  - name: ai_processing_alerts
    rules:
      - alert: LowConfidenceScore
        expr: avg_over_time(ai_confidence_score[5m]) < 0.7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Low AI confidence scores detected
```

### 4. Processing Pipeline Implementation

#### Queue Processor
```go
// usecase/queue_processor.go
type QueueProcessor struct {
    aiService     domain.AIService
    queueService  domain.QueueService
    storageService domain.StorageService
    metrics       *metrics.QueueMetrics
}

func (p *QueueProcessor) ProcessMessage(ctx context.Context, msg *QueueMessage) error {
    // Implementation details for message processing
}
```

#### Batch Processing
```go
// usecase/batch_processor.go
type BatchProcessor struct {
    processor    *QueueProcessor
    batchSize    int
    maxWorkers   int
    metrics      *metrics.BatchMetrics
}

func (p *BatchProcessor) ProcessBatch(ctx context.Context, tracks []*domain.Track) error {
    // Implementation details for batch processing
}
```

## Deployment Configuration

### Cloud Run Service
```yaml
# deployment/cloudrun/queue-processor.yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: queue-processor
spec:
  template:
    spec:
      containers:
      - image: gcr.io/project/queue-processor
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"
        env:
        - name: PUBSUB_TOPIC
          value: "ai-processing"
```

### Cleanup Job
```yaml
# deployment/cronjobs/cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: file-cleanup
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: gcr.io/project/cleanup
```

## Testing Strategy

### Unit Tests
```go
// repository/ai/qwen2_service_test.go
func TestQwen2Service_EnrichMetadata(t *testing.T) {
    // Test cases for AI processing
}

// repository/queue/pubsub_test.go
func TestPubSubService_PublishMessage(t *testing.T) {
    // Test cases for queue operations
}
```

### Integration Tests
```go
// tests/integration/ai_processing_test.go
func TestAIProcessingPipeline(t *testing.T) {
    // End-to-end test cases
}
```

### Performance Tests
```go
// tests/performance/ai_benchmark_test.go
func BenchmarkAIProcessing(b *testing.B) {
    // Performance benchmarks
}
```

## Configuration Management

### Environment Variables
```bash
# .env.example
PUBSUB_TOPIC=ai-processing
PUBSUB_SUBSCRIPTION=ai-processor
BIGQUERY_DATASET=ai_metrics
QWEN2_ENDPOINT=http://qwen2-service:8080
OPENAI_API_KEY=sk-...
```

### Feature Flags
```go
// pkg/config/features.go
type FeatureFlags struct {
    EnableExperimentalModel bool
    UseOpenAIFallback      bool
    EnableBatchProcessing  bool
}
```
</file>

<file path="docs/architecture/overview.md">
# Architecture Overview

## System Architecture

The Metadata Tool follows Clean Architecture principles, organizing code into concentric layers with clear separation of concerns.

```mermaid
graph TD
    A[External Systems] --> B[Handlers Layer]
    B --> C[Use Cases Layer]
    C --> D[Domain Layer]
    C --> E[Repository Layer]
    E --> F[External Services/DB]
```

## Core Layers

### 1. Domain Layer
- Contains business entities and interfaces
- Framework-independent
- Pure business logic

```plaintext
domain/
 ai/
    service.go     # AI service interfaces
    types.go       # Shared AI types
    models.go      # AI domain models
 track/
    entity.go      # Track entity
    repository.go  # Track repository interface
 audio/
     service.go     # Audio processing interface
```

### 2. Use Cases Layer
- Application-specific business rules
- Orchestrates domain entities
- Independent of external concerns

```plaintext
usecase/
 ai/
    enrichment.go
    validation.go
    batch_processing.go
 track/
    management.go
 audio/
     processing.go
```

### 3. Repository Layer
- Data access implementation
- External service integration
- Caching logic

```plaintext
repository/
 ai/
    openai/
    qwen/
    composite/
 storage/
 cached/
```

### 4. Handlers Layer
- HTTP/API endpoints
- Request/Response handling
- Input validation

## Key Components

### AI Services Integration
```mermaid
graph LR
    A[Request] --> B[AI Router]
    B --> C[OpenAI Service]
    B --> D[Qwen2-Audio Service]
    C --> E[Cache]
    D --> E
    E --> F[Response]
```

### Data Flow
```mermaid
sequenceDiagram
    participant C as Client
    participant H as Handler
    participant U as UseCase
    participant AI as AI Service
    participant R as Repository
    
    C->>H: Request
    H->>U: Process
    U->>AI: Enrich
    AI->>R: Store
    R->>H: Response
    H->>C: Result
```

## Infrastructure Components

### 1. Monitoring Stack
- Prometheus metrics
- Grafana dashboards
- Custom AI performance metrics

### 2. Caching Strategy
- Redis for hot data
- Tiered caching approach
- Cache invalidation rules

### 3. Storage
- S3-compatible object storage
- Regional routing
- Access patterns optimization

## Security Architecture

### 1. Authentication
- JWT-based auth
- API key management
- Role-based access control

### 2. Data Protection
- Encryption at rest
- Secure communication
- Audit logging

## Scalability Considerations

### 1. Horizontal Scaling
- Stateless services
- Distributed caching
- Load balancing

### 2. Performance Optimization
- Batch processing
- Async operations
- Resource pooling

## Error Handling

### 1. Error Categories
```plaintext
- ValidationError
- BusinessError
- TechnicalError
- IntegrationError
```

### 2. Recovery Strategies
- Retry policies
- Circuit breakers
- Fallback mechanisms

## Configuration Management

### 1. Environment Configuration
```plaintext
- Development
- Staging
- Production
```

### 2. Feature Flags
- AI model selection
- Caching strategies
- Performance tuning

## Monitoring and Observability

### 1. Key Metrics
- Response times
- Error rates
- Resource utilization

### 2. Logging Strategy
- Structured logging
- Log levels
- Correlation IDs

## Integration Points

### 1. External Services
- OpenAI API
- Qwen2-Audio
- Storage services

### 2. Internal Services
- Authentication
- Caching
- Monitoring

## Deployment Architecture

### 1. Container Strategy
```plaintext
- Application containers
- Sidecar patterns
- Resource limits
```

### 2. Network Architecture
- Service mesh
- Load balancing
- Security groups

## Future Considerations

### 1. Planned Improvements
- Enhanced AI capabilities
- Advanced caching
- Performance optimization

### 2. Scalability Roadmap
- Regional deployment
- Multi-cloud support
- Enhanced monitoring
</file>

<file path="docs/development/guidelines.md">
# Development Guidelines

## Code Organization

### Project Structure
```plaintext
metadatatool/
 cmd/                  # Application entry points
 internal/            # Private application code
    domain/         # Business domain models
    usecase/        # Application business rules
    repository/     # Data access layer
    handler/        # HTTP handlers
    config/         # Configuration
    pkg/            # Shared packages
 pkg/                # Public packages
 docs/               # Documentation
 scripts/            # Build and maintenance scripts
```

## Coding Standards

### Go Code Style
1. Follow standard Go formatting:
   ```bash
   # Format code
   go fmt ./...
   
   # Run linter
   golangci-lint run
   ```

2. Package Organization:
   ```go
   package example

   import (
       // Standard library
       "context"
       "fmt"
       
       // Third party
       "github.com/example/pkg"
       
       // Internal packages
       "metadatatool/internal/domain"
   )
   ```

3. Error Handling:
   ```go
   // Good
   if err != nil {
       return fmt.Errorf("failed to process request: %w", err)
   }

   // Bad
   if err != nil {
       return err
   }
   ```

### Clean Architecture Guidelines

1. **Domain Layer**
   ```go
   // Good
   type Track struct {
       ID       string
       Title    string
       Metadata Metadata
   }

   // Bad - mixing concerns
   type Track struct {
       ID       string
       Title    string
       DBFields map[string]interface{}
   }
   ```

2. **Use Case Layer**
   ```go
   // Good
   type TrackUseCase struct {
       repo domain.TrackRepository
       ai   domain.AIService
   }

   // Bad - direct database access
   type TrackUseCase struct {
       db *sql.DB
   }
   ```

3. **Repository Layer**
   ```go
   // Good
   func (r *TrackRepository) Create(ctx context.Context, track *domain.Track) error {
       // Implementation
   }

   // Bad - exposing implementation details
   func (r *TrackRepository) CreateInPostgres(track *domain.Track) error {
       // Implementation
   }
   ```

## Testing Guidelines

### Unit Tests
```go
func TestTrackUseCase_Create(t *testing.T) {
    // Arrange
    mockRepo := mocks.NewMockTrackRepository()
    useCase := NewTrackUseCase(mockRepo)
    
    // Act
    result, err := useCase.Create(context.Background(), track)
    
    // Assert
    assert.NoError(t, err)
    assert.NotNil(t, result)
}
```

### Integration Tests
```go
func TestTrackAPI_Integration(t *testing.T) {
    if testing.Short() {
        t.Skip("skipping integration test")
    }
    // Test implementation
}
```

### Performance Tests
```go
func BenchmarkTrackProcessing(b *testing.B) {
    for i := 0; i < b.N; i++ {
        // Benchmark implementation
    }
}
```

## Documentation Guidelines

### Code Documentation
```go
// TrackService handles track-related operations
type TrackService interface {
    // Create creates a new track with the given details
    // Returns ErrInvalidInput if the track data is invalid
    Create(ctx context.Context, track *Track) error
}
```

### API Documentation
```go
// @Summary Create track
// @Description Create a new track with metadata
// @Tags tracks
// @Accept json
// @Produce json
// @Param track body Track true "Track object"
// @Success 200 {object} Response
// @Router /tracks [post]
func (h *Handler) CreateTrack(c *gin.Context) {
    // Implementation
}
```

## Error Handling

### Error Types
```go
type ErrorCode string

const (
    ErrNotFound     ErrorCode = "NOT_FOUND"
    ErrInvalidInput ErrorCode = "INVALID_INPUT"
    ErrInternal     ErrorCode = "INTERNAL_ERROR"
)

type AppError struct {
    Code    ErrorCode
    Message string
    Err     error
}
```

### Error Handling Pattern
```go
func (s *service) Process(ctx context.Context) error {
    result, err := s.repository.Get(ctx)
    if err != nil {
        return &AppError{
            Code:    ErrInternal,
            Message: "Failed to get data",
            Err:     err,
        }
    }
    return nil
}
```

## Performance Guidelines

### Database Access
```go
// Good - Using indexes
db.Where("created_at > ?", time.Now().Add(-24*time.Hour))

// Bad - Full table scan
db.Where("EXTRACT(day FROM created_at) = ?", time.Now().Day())
```

### Caching Strategy
```go
func (s *service) GetTrack(ctx context.Context, id string) (*Track, error) {
    // Try cache first
    if cached, err := s.cache.Get(ctx, id); err == nil {
        return cached, nil
    }
    
    // Get from database
    track, err := s.repo.Get(ctx, id)
    if err != nil {
        return nil, err
    }
    
    // Update cache
    s.cache.Set(ctx, id, track)
    return track, nil
}
```

## Security Guidelines

### Input Validation
```go
func validateTrack(track *Track) error {
    if track.Title == "" {
        return ErrInvalidInput("title is required")
    }
    if len(track.Title) > 255 {
        return ErrInvalidInput("title too long")
    }
    return nil
}
```

### Authentication
```go
func AuthMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        token := c.GetHeader("Authorization")
        if !validateToken(token) {
            c.AbortWithStatus(http.StatusUnauthorized)
            return
        }
        c.Next()
    }
}
```

## Monitoring Guidelines

### Metrics
```go
func (s *service) Process(ctx context.Context) error {
    start := time.Now()
    defer func() {
        metrics.ProcessingDuration.Observe(time.Since(start).Seconds())
    }()
    
    // Implementation
}
```

### Logging
```go
func (s *service) Process(ctx context.Context) error {
    logger := log.WithContext(ctx).With(
        "operation", "process",
        "user_id", ctx.Value("user_id"),
    )
    
    logger.Info("Starting processing")
    // Implementation
    logger.Info("Processing completed")
}
```

## Version Control Guidelines

### Commit Messages
```plaintext
feat: add track metadata enrichment
^--^  ^------------------------^
|     |
|     +-> Summary in present tense
|
+-------> Type: feat, fix, docs, style, refactor, test, chore
```

### Branch Naming
```plaintext
feature/track-enrichment
bugfix/metadata-validation
docs/api-documentation
```

## CI/CD Guidelines

### Pipeline Stages
```yaml
stages:
  - lint
  - test
  - build
  - deploy

lint:
  script:
    - golangci-lint run

test:
  script:
    - go test ./...

build:
  script:
    - go build ./...
```

## Dependencies Management

### Adding Dependencies
```bash
# Add direct dependency
go get -u github.com/example/pkg

# Add development dependency
go get -u -d github.com/example/pkg
```

### Updating Dependencies
```bash
# Update all dependencies
go get -u ./...

# Update specific dependency
go get -u github.com/example/pkg
```

## Review Guidelines

### Code Review Checklist
```plaintext
1. Architecture
   - Follows clean architecture
   - Proper separation of concerns
   - Clear dependencies

2. Code Quality
   - Follows coding standards
   - Proper error handling
   - Adequate testing

3. Performance
   - Efficient algorithms
   - Proper resource usage
   - Caching strategy

4. Security
   - Input validation
   - Authentication/Authorization
   - Secure communication
```
</file>

<file path="docs/integrations/qwen2-audio/README.md">
# Qwen2-Audio Integration

## Overview

This document details the integration of Qwen2-Audio with our metadata tool, including architecture, implementation details, and best practices.

## Table of Contents

1. [Architecture](#architecture)
2. [Configuration](#configuration)
3. [Implementation](#implementation)
4. [Deployment](#deployment)
5. [Monitoring](#monitoring)
6. [Troubleshooting](#troubleshooting)

## Architecture

### Component Integration
```mermaid
graph TD
    A[Audio Upload] --> B[Audio Processor]
    B --> C[Qwen2-Audio Service]
    C --> D[Metadata Enrichment]
    D --> E[Storage]
    D --> F[Cache]
```

### Service Structure
```plaintext
qwen2-audio/
 service/
    client.go       # Qwen2-Audio client
    processor.go    # Audio processing
    mapper.go       # Data mapping
 config/
    settings.go     # Configuration
 models/
     types.go        # Domain models
```

## Configuration

### Environment Variables
```yaml
QWEN_API_KEY: "your-api-key"
QWEN_MODEL_VERSION: "latest"
QWEN_ENDPOINT: "https://api.qwen.ai/v1"
QWEN_BATCH_SIZE: 10
QWEN_TIMEOUT: "30s"
QWEN_MAX_RETRIES: 3
```

### Model Settings
```json
{
  "audio_analysis": {
    "confidence_threshold": 0.85,
    "min_duration": "5s",
    "max_duration": "300s"
  },
  "speech_recognition": {
    "language": ["en", "es", "fr"],
    "model": "qwen2-audio-large"
  }
}
```

## Implementation

### Service Interface
```go
type Qwen2AudioService interface {
    AnalyzeAudio(ctx context.Context, data []byte) (*AudioAnalysis, error)
    ExtractSpeech(ctx context.Context, data []byte) (*Transcript, error)
    ClassifySound(ctx context.Context, data []byte) (*Classification, error)
    BatchProcess(ctx context.Context, files [][]byte) ([]*AudioAnalysis, error)
}
```

### Error Handling
```go
type QwenError struct {
    Code        string
    Message     string
    RetryCount  int
    Recoverable bool
}
```

### Retry Strategy
```plaintext
1. Exponential backoff
2. Maximum 3 retries
3. Circuit breaker pattern
4. Fallback to OpenAI
```

## Deployment

### Prerequisites
- Go 1.21+
- Redis 6.0+
- PostgreSQL 13+
- S3-compatible storage

### Resource Requirements
```yaml
compute:
  cpu: "2 cores"
  memory: "4 GB"
  storage: "20 GB"
network:
  bandwidth: "100 Mbps"
  latency: "<100ms"
```

### Security Requirements
- TLS 1.3
- API key rotation
- Rate limiting
- IP whitelisting

## Monitoring

### Metrics
```plaintext
1. Response Times
   - p95 latency
   - Average processing time
   - Queue length

2. Quality Metrics
   - Confidence scores
   - Error rates
   - Fallback rates

3. Resource Usage
   - API calls
   - Memory usage
   - Network bandwidth
```

### Alerting Rules
```yaml
alerts:
  - name: HighErrorRate
    condition: error_rate > 0.05
    duration: 5m
    severity: critical

  - name: LowConfidence
    condition: confidence_score < 0.8
    duration: 10m
    severity: warning

  - name: HighLatency
    condition: p95_latency > 2s
    duration: 5m
    severity: warning
```

## Troubleshooting

### Common Issues

1. **Connection Errors**
   ```plaintext
   - Check network connectivity
   - Verify API credentials
   - Confirm endpoint availability
   ```

2. **Processing Errors**
   ```plaintext
   - Validate input format
   - Check file size limits
   - Verify model availability
   ```

3. **Performance Issues**
   ```plaintext
   - Monitor resource usage
   - Check batch sizes
   - Verify cache hit rates
   ```

### Debug Tools
```plaintext
1. Health Checks
   - /health/qwen
   - /metrics/qwen
   - /status/qwen

2. Logging
   - Structured JSON logs
   - Correlation IDs
   - Error tracking
```

## Best Practices

### Audio Processing
```plaintext
1. Pre-processing
   - Format validation
   - Size optimization
   - Quality checks

2. Batch Processing
   - Optimal batch size
   - Queue management
   - Error handling
```

### Performance Optimization
```plaintext
1. Caching Strategy
   - Results caching
   - Model caching
   - Configuration caching

2. Resource Management
   - Connection pooling
   - Memory management
   - Garbage collection
```

### Security
```plaintext
1. API Security
   - Key management
   - Request signing
   - Rate limiting

2. Data Protection
   - Encryption
   - Access control
   - Audit logging
```

## Version History

### Current Version: 1.0.0
```plaintext
- Initial integration
- Basic audio analysis
- Speech recognition
- Sound classification
```

### Planned Updates
```plaintext
1.1.0:
- Enhanced batch processing
- Advanced caching
- Performance improvements

1.2.0:
- Multi-language support
- Custom model training
- Advanced analytics
```

## Support

### Contact Information
```plaintext
- Technical Support: support@qwen.ai
- Documentation: docs.qwen.ai
- Community: community.qwen.ai
```

### Additional Resources
- [API Documentation](https://api.qwen.ai/docs)
- [Model Documentation](https://docs.qwen.ai/models)
- [Best Practices Guide](https://docs.qwen.ai/best-practices)
</file>

<file path="docs/monitoring/alerts.md">
# Alert Rules Documentation

## Overview
This document details the alert rules configured in our monitoring stack. These rules are defined in Prometheus and handled by AlertManager.

## Alert Severity Levels

### Critical
- Immediate action required
- Service is down or severely impacted
- Business impact is significant
- Page on-call engineer 24/7

### Warning
- Action required within business hours
- Service degradation
- Some business impact
- Notify during business hours

### Info
- No immediate action required
- Potential issues to watch
- No direct business impact
- Collect for reporting

## Alert Rules Configuration

### Application Health

1. **High Error Rate**
```yaml
alert: HighErrorRate
expr: |
  sum(rate(http_requests_total{status=~"5.."}[5m])) 
  / 
  sum(rate(http_requests_total[5m])) > 0.05
for: 5m
labels:
  severity: critical
annotations:
  summary: High HTTP error rate (> 5%)
  description: "Error rate is {{ $value | humanizePercentage }} over last 5m"
```

2. **Slow Response Time**
```yaml
alert: SlowResponseTime
expr: |
  histogram_quantile(0.95, 
    rate(http_request_duration_seconds_bucket[5m])
  ) > 1.0
for: 5m
labels:
  severity: warning
annotations:
  summary: Slow response time (95th percentile > 1s)
  description: "95th percentile latency is {{ $value }}s"
```

### Database Performance

1. **High Connection Usage**
```yaml
alert: HighDBConnections
expr: database_connections{state="active"} > 80
for: 5m
labels:
  severity: warning
annotations:
  summary: High database connection count
  description: "{{ $value }} active connections"
```

2. **Slow Queries**
```yaml
alert: SlowDatabaseQueries
expr: |
  rate(database_query_duration_seconds_sum[5m]) 
  / 
  rate(database_query_duration_seconds_count[5m]) > 1
for: 5m
labels:
  severity: warning
annotations:
  summary: Slow database queries
  description: "Average query time is {{ $value }}s"
```

### Cache Performance

1. **Low Cache Hit Rate**
```yaml
alert: LowCacheHitRate
expr: |
  sum(rate(cache_hits_total[5m])) 
  / 
  (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m]))) 
  < 0.7
for: 10m
labels:
  severity: warning
annotations:
  summary: Low cache hit rate
  description: "Cache hit rate is {{ $value | humanizePercentage }}"
```

### AI Service Performance

1. **High AI Latency**
```yaml
alert: HighAILatency
expr: |
  rate(ai_request_duration_seconds_sum[5m]) 
  / 
  rate(ai_request_duration_seconds_count[5m]) > 5
for: 5m
labels:
  severity: warning
annotations:
  summary: High AI service latency
  description: "Average AI request time is {{ $value }}s"
```

2. **Low AI Confidence**
```yaml
alert: LowAIConfidence
expr: avg(ai_confidence) < 0.6
for: 15m
labels:
  severity: warning
annotations:
  summary: Low AI confidence scores
  description: "Average confidence is {{ $value | humanizePercentage }}"
```

### Track Processing

1. **High Processing Failure Rate**
```yaml
alert: HighProcessingFailureRate
expr: |
  sum(rate(tracks_processed_total{status="error"}[5m])) 
  / 
  sum(rate(tracks_processed_total[5m])) > 0.1
for: 5m
labels:
  severity: critical
annotations:
  summary: High track processing failure rate
  description: "Failure rate is {{ $value | humanizePercentage }}"
```

2. **Low Processing Rate**
```yaml
alert: LowProcessingRate
expr: rate(tracks_processed_total[5m]) < 1
for: 15m
labels:
  severity: warning
annotations:
  summary: Low track processing rate
  description: "Processing {{ $value }} tracks per second"
```

### System Resources

1. **High Memory Usage**
```yaml
alert: HighMemoryUsage
expr: |
  (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)
  /
  node_memory_MemTotal_bytes > 0.85
for: 5m
labels:
  severity: warning
annotations:
  summary: High memory usage
  description: "Memory usage is {{ $value | humanizePercentage }}"
```

2. **High CPU Usage**
```yaml
alert: HighCPUUsage
expr: |
  100 - (avg by(instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
for: 5m
labels:
  severity: warning
annotations:
  summary: High CPU usage
  description: "CPU usage is {{ $value }}%"
```

## Alert Routing

### Slack Integration
```yaml
receivers:
- name: 'slack-critical'
  slack_configs:
  - channel: '#alerts-critical'
    username: 'Prometheus'
    icon_emoji: ':fire:'
    title: '{{ .GroupLabels.alertname }}'
    text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"

- name: 'slack-warnings'
  slack_configs:
  - channel: '#alerts-warnings'
    username: 'Prometheus'
    icon_emoji: ':warning:'
    title: '{{ .GroupLabels.alertname }}'
    text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"
```

## Alert Grouping
```yaml
route:
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'slack-warnings'
  routes:
  - match:
      severity: critical
    receiver: 'slack-critical'
    repeat_interval: 1h
```

## Maintenance Windows
```yaml
time_intervals:
- name: business-hours
  time_intervals:
  - weekdays: ['monday:friday']
    times:
    - start_time: 09:00
      end_time: 17:00
```

## Best Practices

1. **Alert Design**
   - Clear, actionable alerts
   - Appropriate thresholds
   - Meaningful descriptions
   - Proper severity levels

2. **Alert Management**
   - Regular review of alert effectiveness
   - Tuning of thresholds based on patterns
   - Documentation of resolution steps
   - Runbooks for common issues

3. **Notification Channels**
   - Multiple notification methods
   - Escalation paths
   - On-call rotation integration
   - Alert acknowledgment tracking
</file>

<file path="docs/monitoring/dashboard.md">
# Metadata Tool Monitoring Dashboard

## Overview
The Metadata Tool monitoring dashboard provides real-time visibility into the system's performance, health, and operational metrics. The dashboard is accessible at `http://localhost:3000` with default credentials (admin/admin).

## Dashboard Organization
The dashboard is organized into several logical sections, each focusing on specific aspects of the system:

### 1. Application Performance (Row 1)
- **HTTP Request Rate**
  - Metric: `rate(http_requests_total[5m])`
  - Description: Shows the rate of incoming HTTP requests over 5-minute windows
  - Purpose: Monitor traffic patterns and identify unusual spikes or drops

- **Active Users**
  - Metric: `active_users`
  - Description: Gauge showing current number of active users
  - Thresholds: Warning at 80 users (red)
  - Purpose: Real-time user activity monitoring

### 2. Response Times & Caching (Row 2)
- **Average Response Time**
  - Metric: `rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])`
  - Unit: Seconds
  - Description: Average response time for HTTP requests
  - Purpose: Track application responsiveness

- **Cache Hit Rate**
  - Metric: `sum(rate(cache_hits_total[5m])) / (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))`
  - Unit: Percentage (0-1)
  - Description: Ratio of cache hits to total cache operations
  - Purpose: Monitor caching efficiency

### 3. Database Performance (Row 3)
- **Database Connections**
  - Metric: `database_connections{state='active'}`
  - Description: Number of active database connections
  - Purpose: Monitor connection pool utilization

- **Database Query Latency**
  - Metric: `rate(database_query_duration_seconds_sum[5m]) / rate(database_query_duration_seconds_count[5m])`
  - Unit: Seconds
  - Description: Average database query execution time
  - Purpose: Identify slow queries and performance issues

### 4. AI Service Performance (Row 4)
- **AI Service Latency**
  - Metric: `rate(ai_request_duration_seconds_sum[5m]) / rate(ai_request_duration_seconds_count[5m])`
  - Unit: Seconds
  - Description: Average response time for AI service requests
  - Purpose: Monitor AI service responsiveness

- **AI Confidence Score**
  - Metric: `avg(ai_confidence)`
  - Unit: Percentage (0-1)
  - Description: Average confidence score of AI predictions
  - Purpose: Monitor AI prediction quality

### 5. System Resources (Row 5)
- **System Resource Usage**
  - Metrics:
    - Memory: `100 * (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes`
    - CPU: `100 * (1 - avg by (instance)(irate(node_cpu_seconds_total{mode='idle'}[5m])))`
  - Unit: Percentage
  - Description: System-level resource utilization
  - Purpose: Monitor infrastructure health

- **HTTP Error Rate**
  - Metric: `100 * sum(rate(http_requests_total{status=~'5..'}[5m])) / sum(rate(http_requests_total[5m]))`
  - Unit: Percentage
  - Description: Percentage of HTTP requests resulting in 5xx errors
  - Purpose: Track error rates and service stability

### 6. Track Processing (Row 6)
- **Track Processing Rate**
  - Metric: `rate(tracks_processed_total[5m])`
  - Description: Number of tracks processed per minute
  - Purpose: Monitor processing throughput

- **Track Processing Success Rate**
  - Metric: `sum(rate(tracks_processed_total{status='success'}[5m])) / sum(rate(tracks_processed_total[5m]))`
  - Unit: Percentage (0-1)
  - Description: Ratio of successfully processed tracks
  - Purpose: Monitor processing reliability

## Dashboard Settings
- **Refresh Rate**: 10 seconds
- **Time Range**: Last 1 hour by default
- **Style**: Dark theme
- **Tags**: metadatatool

## Panel Features
Each panel includes:
- Table-style legends with statistical calculations (mean, max, min where applicable)
- Tooltips for detailed point-in-time information
- Consistent color schemes
- Appropriate units and formatting
- 5-minute rate calculations for smoother graphs

## Using the Dashboard
1. **Time Range**: Adjust the time range using the picker in the top right
2. **Legends**: Click legend items to toggle visibility
3. **Tooltips**: Hover over graphs for detailed metrics
4. **Tables**: Sort legend tables by clicking column headers

## Alert Integration
The dashboard integrates with the following alert rules:
- High error rate alerts (>5% for 5 minutes)
- Slow response time alerts (>1s 95th percentile)
- Database connection alerts (>80 active connections)
- Cache performance alerts (<70% hit rate)
- AI service latency alerts (>5s 95th percentile)
- Track processing failure alerts (>10% failure rate)
- Low active user alerts (<10 during business hours)

## Maintenance
- Dashboard JSON is version controlled
- Panels can be exported/imported individually
- Dashboard is provisioned automatically via configuration
- Changes should be made through version control, not UI
</file>

<file path="docs/monitoring/setup.md">
# Monitoring Stack Setup Guide

## Overview
The monitoring stack consists of:
- **Prometheus**: Time-series database and monitoring system
- **Grafana**: Visualization and dashboarding
- **AlertManager**: Alert handling and routing
- **Node Exporter**: System metrics collection
- **Redis Exporter**: Redis metrics collection
- **Postgres Exporter**: PostgreSQL metrics collection

## Prerequisites
- Docker and Docker Compose installed
- Access to the application network
- Sufficient disk space for persistent storage

## Directory Structure
```
monitoring/
 prometheus/
    prometheus.yml
    rules/
        alerts.yml
 alertmanager/
    alertmanager.yml
    templates/
        slack.tmpl
 grafana/
    provisioning/
       datasources/
          prometheus.yml
       dashboards/
           provider.yml
    dashboards/
        metadatatool.json
```

## Installation Steps

1. **Create Required Directories**
```bash
mkdir -p monitoring/{prometheus,alertmanager,grafana/provisioning/{datasources,dashboards}}
```

2. **Configure Docker Compose**
File: `docker-compose.monitoring.yml`
```yaml
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:v2.45.0
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  alertmanager:
    image: prom/alertmanager:v0.26.0
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    ports:
      - "9093:9093"

  grafana:
    image: grafana/grafana:10.1.0
    volumes:
      - ./monitoring/grafana:/etc/grafana
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false

  node-exporter:
    image: prom/node-exporter:v1.6.1
    ports:
      - "9100:9100"

  redis-exporter:
    image: oliver006/redis_exporter:v1.54.0
    ports:
      - "9121:9121"

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.13.2
    ports:
      - "9187:9187"
```

3. **Start the Stack**
```bash
docker-compose -f docker-compose.monitoring.yml up -d
```

## Component Configuration

### Prometheus
File: `prometheus.yml`
- Scrape interval: 15s
- Evaluation interval: 15s
- Targets:
  - Application metrics (`:8080/metrics`)
  - Node Exporter (`:9100`)
  - Redis Exporter (`:9121`)
  - Postgres Exporter (`:9187`)

### AlertManager
File: `alertmanager.yml`
- Route configuration for different severity levels
- Slack integration for notifications
- Grouping and inhibition rules
- Templates for alert formatting

### Grafana
- Default admin credentials: admin/admin
- Auto-provisioned datasources and dashboards
- Disabled sign-up
- Minimum refresh interval: 5s

## Security Considerations
1. **Network Security**
   - Services exposed only on necessary ports
   - External access restricted through network policies
   - TLS configuration for production

2. **Authentication**
   - Change default passwords
   - Configure SSO for production
   - API key rotation policy

3. **Authorization**
   - Role-based access control
   - Viewer/Editor/Admin roles
   - Dashboard permissions

## Maintenance

### Backup
1. **Volume Backup**
```bash
docker run --rm --volumes-from prometheus -v $(pwd):/backup alpine tar cvf /backup/prometheus-backup.tar /prometheus
docker run --rm --volumes-from grafana -v $(pwd):/backup alpine tar cvf /backup/grafana-backup.tar /var/lib/grafana
```

2. **Configuration Backup**
- Keep all configuration in version control
- Document changes in git commits
- Include dashboard JSON exports

### Scaling
1. **Storage**
   - Monitor disk usage
   - Configure retention policies
   - Consider remote storage for long-term metrics

2. **Performance**
   - Adjust scrape intervals
   - Configure recording rules
   - Optimize queries

### Troubleshooting
1. **Common Issues**
   - Check container logs: `docker-compose logs <service>`
   - Verify network connectivity
   - Validate configuration files

2. **Monitoring the Monitors**
   - Set up alerts for monitoring stack
   - Monitor resource usage
   - Check scrape targets in Prometheus

## Access URLs
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000
- AlertManager: http://localhost:9093

## Best Practices
1. **Configuration**
   - Use environment variables
   - Version control all configs
   - Document all changes

2. **Alerting**
   - Define clear severity levels
   - Set appropriate thresholds
   - Avoid alert fatigue

3. **Dashboard Design**
   - Consistent naming
   - Clear visualization
   - Useful legends and documentation
</file>

<file path="docs/authentication.md">
# Authentication System Documentation

## Overview
The authentication system provides a secure, JWT-based authentication mechanism with role-based access control (RBAC) and session management. It implements industry best practices for token management, password security, and API key handling.

## Components

### JWT Service (`internal/repository/auth/jwt_service.go`)

The JWT service handles token generation, validation, and management. It implements the `domain.AuthService` interface.

#### Key Features
- Token pair generation (access + refresh tokens)
- Secure token validation
- Password hashing using bcrypt
- API key generation
- Role-based permission management

#### Token Configuration
```go
const (
    accessTokenDuration  = 15 * time.Minute
    refreshTokenDuration = 7 * 24 * time.Hour
)
```

### Token Types

#### Access Token
- Short-lived (15 minutes)
- Used for API authentication
- Contains user claims:
  - User ID
  - Email
  - Role
  - Permissions
  - Standard JWT claims (exp, iat, nbf, jti)

#### Refresh Token
- Long-lived (7 days)
- Used to obtain new access tokens
- Contains minimal claims for security

### Claims Structure
```go
type customClaims struct {
    jwt.RegisteredClaims
    UserID      string
    Email       string
    Role        domain.Role
    Permissions []domain.Permission
}
```

### Roles and Permissions

The system implements a hierarchical role-based access control:

#### Roles
- `RoleAdmin`: Full system access
- `RoleUser`: Standard user access
- `RoleGuest`: Limited read-only access
- `RoleSystem`: System-level operations

#### Permissions
```go
const (
    PermissionCreateTrack    = "track:create"
    PermissionReadTrack      = "track:read"
    PermissionUpdateTrack    = "track:update"
    PermissionDeleteTrack    = "track:delete"
    PermissionEnrichMetadata = "metadata:enrich"
    PermissionExportDDEX     = "metadata:export_ddex"
    PermissionManageUsers    = "users:manage"
    PermissionManageRoles    = "roles:manage"
    PermissionManageAPIKeys  = "apikeys:manage"
)
```

## Usage Examples

### Token Generation
```go
authService, err := auth.NewJWTService(secretKey)
if err != nil {
    return err
}

tokens, err := authService.GenerateTokens(user)
if err != nil {
    return err
}

// Use tokens.AccessToken and tokens.RefreshToken
```

### Token Validation
```go
claims, err := authService.ValidateToken(tokenString)
if err != nil {
    return err
}

// Access claims.UserID, claims.Role, etc.
```

### Password Management
```go
// Hash password
hashedPassword, err := authService.HashPassword(password)
if err != nil {
    return err
}

// Verify password
err = authService.VerifyPassword(hashedPassword, password)
if err != nil {
    // Invalid password
}
```

### Permission Checking
```go
if authService.HasPermission(user.Role, domain.PermissionCreateTrack) {
    // User has permission to create tracks
}
```

## Security Considerations

1. **Token Security**
   - Tokens include unique JWT IDs (jti)
   - Implements token expiration
   - Uses secure random generation for keys
   - Validates signing method

2. **Password Security**
   - Uses bcrypt with appropriate cost factor
   - Implements secure password hashing
   - Never stores plain text passwords

3. **API Key Security**
   - Uses cryptographically secure random generation
   - Implements base64 URL-safe encoding
   - 32-byte length for strong security

## Testing

The authentication system includes comprehensive tests:
- Token generation and validation
- Password hashing and verification
- Permission checks
- API key generation
- Error cases and edge conditions

Run tests with:
```bash
go test -v ./internal/repository/auth
```

## Metrics and Monitoring

The system exports Prometheus metrics for:
- Authentication attempts
- Token operations
- Session management
- Permission checks

Metric names:
- `auth_attempts_total`
- `token_operations_total`
- `session_operations_total`
- `permission_checks_total`

## Error Handling

The system provides clear error types for different failure scenarios:
- `ErrInvalidCredentials`
- `ErrInvalidToken`
- `ErrSessionNotFound`
- `ErrUnauthorized`
- `ErrForbidden`

## Best Practices

1. **Token Management**
   - Use short-lived access tokens
   - Implement token refresh flow
   - Validate tokens on every request
   - Include minimal claims in refresh tokens

2. **Security**
   - Store secrets securely
   - Use environment variables for configuration
   - Implement rate limiting
   - Log security events

3. **Implementation**
   - Follow interface segregation
   - Implement proper error handling
   - Use dependency injection
   - Maintain test coverage
</file>

<file path="docs/queue_system.md">
# Queue System Documentation

## Overview

The Queue System is a Redis-based message processing system that implements a reliable pub/sub pattern with support for message retries, dead letter queues, and comprehensive monitoring. It follows clean architecture principles and provides a robust foundation for asynchronous task processing.

## Components

### Core Interfaces

#### Message
```go
type Message struct {
    ID            string
    Topic         string
    Data          []byte
    Status        MessageStatus
    RetryCount    int
    CreatedAt     time.Time
    UpdatedAt     time.Time
    DeadLetterAt  *time.Time
}
```

#### Publisher
```go
type Publisher interface {
    Publish(ctx context.Context, topic string, data []byte) error
    PublishBatch(ctx context.Context, topic string, messages [][]byte) error
}
```

#### Subscriber
```go
type Subscriber interface {
    Subscribe(ctx context.Context, topic string, handler MessageHandler) error
    Unsubscribe(ctx context.Context, topic string) error
}
```

### Message Lifecycle

1. **Publishing**: Messages are published to specific topics and stored in Redis with a "pending" status
2. **Processing**: Subscribers pick up messages and process them using registered handlers
3. **Completion**: Successfully processed messages are acknowledged and removed
4. **Retry**: Failed messages are retried based on configured policies
5. **Dead Letter**: Messages that exceed retry limits are moved to dead letter queues

## Configuration

### Queue Configuration
```go
type QueueConfig struct {
    RetryDelays        []time.Duration
    ProcessingTimeout  time.Duration
    BatchSize         int
    CleanupInterval   time.Duration
}
```

### Default Settings
- Retry Delays: [1m, 5m, 15m, 30m, 1h]
- Processing Timeout: 5 minutes
- Batch Size: 100
- Cleanup Interval: 1 hour

## Usage Examples

### Publishing Messages
```go
queue := queue.NewRedisQueue(redisClient, queueConfig)

// Single message
err := queue.Publish(ctx, "user.created", userData)

// Batch of messages
err := queue.PublishBatch(ctx, "user.created", userDataBatch)
```

### Subscribing to Topics
```go
handler := func(ctx context.Context, msg *domain.Message) error {
    // Process message
    return nil
}

err := queue.Subscribe(ctx, "user.created", handler)
```

### Dead Letter Queue Operations
```go
// List dead letter messages
messages, err := queue.ListDeadLetterMessages(ctx, "user.created")

// Replay dead letter message
err := queue.ReplayDeadLetterMessage(ctx, messageID)

// Purge dead letter queue
err := queue.PurgeDeadLetterQueue(ctx, "user.created")
```

## Monitoring & Metrics

### Prometheus Metrics

1. **Queue Operations**
   - `queue_operations_total{operation="publish|subscribe|ack|nack",status="success|failure"}`
   - Tracks total operations by type and status

2. **Message Processing**
   - `message_processing_duration_seconds{topic="..."}`
   - Histogram of message processing durations

3. **Message Status**
   - `message_status{status="pending|processing|completed|failed|retrying|dead_letter",topic="..."}`
   - Current count of messages by status

4. **Queue Size**
   - `queue_size{topic="..."}`
   - Current size of each queue

5. **Processing Errors**
   - `processing_errors_total{topic="...",error_type="timeout|validation|processing"}`
   - Total count of processing errors by type

### Grafana Dashboard

The queue system includes a pre-configured Grafana dashboard with panels for:
- Message throughput and latency
- Queue sizes and processing rates
- Error rates and types
- Dead letter queue metrics
- Resource utilization

## Error Handling

### Retry Policy
- Configurable retry delays with exponential backoff
- Maximum retry count per message
- Custom error types for different failure scenarios

### Error Types
1. `ErrMessageNotFound`: Message doesn't exist
2. `ErrTopicNotFound`: Topic doesn't exist
3. `ErrProcessingTimeout`: Message processing exceeded timeout
4. `ErrInvalidMessage`: Message validation failed
5. `ErrHandlerNotFound`: No handler registered for topic

## Best Practices

1. **Message Design**
   - Keep messages small and focused
   - Include necessary metadata for tracking
   - Use structured data formats (JSON/Protocol Buffers)

2. **Topic Naming**
   - Use hierarchical naming (e.g., "user.created", "order.updated")
   - Keep names consistent and descriptive
   - Document topic purposes and message formats

3. **Error Handling**
   - Implement idempotent message handlers
   - Log detailed error information
   - Monitor retry counts and dead letter queues

4. **Performance**
   - Use batch operations when possible
   - Configure appropriate timeouts
   - Monitor queue sizes and processing rates

5. **Monitoring**
   - Set up alerts for critical metrics
   - Review dead letter queues regularly
   - Monitor processing latency

## Testing

The queue system includes comprehensive tests:

1. **Unit Tests**
   - Message lifecycle
   - Retry mechanism
   - Dead letter handling
   - Configuration validation

2. **Integration Tests**
   - End-to-end message flow
   - Redis interaction
   - Concurrent processing
   - Error scenarios

3. **Performance Tests**
   - High throughput scenarios
   - Batch processing
   - Resource utilization

## Security Considerations

1. **Data Protection**
   - Messages are stored encrypted at rest
   - Sensitive data should be encrypted before publishing
   - Access control through Redis ACLs

2. **Resource Protection**
   - Rate limiting on publishing
   - Maximum message size limits
   - Queue size limits

3. **Monitoring**
   - Alerts for unusual activity
   - Audit logging of critical operations
   - Regular security reviews

## Troubleshooting

Common issues and solutions:

1. **Messages Not Being Processed**
   - Check subscriber health
   - Verify topic names
   - Review handler errors

2. **High Error Rates**
   - Monitor retry counts
   - Check handler timeouts
   - Review error patterns

3. **Performance Issues**
   - Monitor Redis metrics
   - Review batch sizes
   - Check network latency

4. **Dead Letter Queue Growth**
   - Review failed message patterns
   - Adjust retry policies
   - Implement manual review process
</file>

<file path="docs/queue.md">

</file>

<file path="docs/README.md">
# MetadataTool Documentation

## Overview
MetadataTool is a comprehensive solution for managing music metadata, providing AI-powered enrichment, DDEX compliance, and efficient storage management.

## Quick Start
```bash
# Clone the repository
git clone https://github.com/yourusername/metadatatool.git

# Install dependencies
go mod download

# Run the application
go run cmd/metadatatool/main.go
```

## Architecture
The application follows clean architecture principles with the following layers:
- Domain (core business logic)
- Use Cases (application logic)
- Repositories (data access)
- Handlers (HTTP/transport layer)

## Core Features
1. **Metadata Management**
   - Track metadata CRUD operations
   - Batch processing
   - Version control

2. **AI Integration**
   - Automatic metadata enrichment
   - Confidence scoring
   - Batch processing

3. **Storage Management**
   - S3-compatible storage
   - File versioning
   - Cleanup management

4. **DDEX Integration**
   - ERN 4.3 support
   - Validation
   - Import/Export

## API Reference
### Track Management
```graphql
type Track {
  id: ID!
  title: String!
  artist: String!
  # ... other fields
}

type Query {
  track(id: ID!): Track
  tracks(filter: TrackFilter): [Track!]!
}

type Mutation {
  createTrack(input: CreateTrackInput!): Track!
  updateTrack(id: ID!, input: UpdateTrackInput!): Track!
  deleteTrack(id: ID!): Boolean!
}
```

### File Management
```graphql
type File {
  id: ID!
  name: String!
  size: Int!
  url: String!
}

type Mutation {
  uploadFile(file: Upload!): File!
  deleteFile(id: ID!): Boolean!
}
```

## Configuration
```yaml
server:
  port: 8080
  timeout: 30s

database:
  host: localhost
  port: 5432
  name: metadatatool

ai:
  provider: openai
  model: gpt-4
  maxTokens: 1000

storage:
  provider: s3
  bucket: metadatatool
  region: us-west-2
```

## Development
### Prerequisites
- Go 1.21+
- PostgreSQL 14+
- Redis 6+
- Node.js 18+ (for frontend)

### Local Setup
1. Copy `.env.example` to `.env`
2. Configure environment variables
3. Run database migrations
4. Start the development server

### Testing
```bash
# Run unit tests
go test ./...

# Run integration tests
go test -tags=integration ./...

# Run with coverage
go test -cover ./...
```

## Deployment
### Docker
```bash
# Build image
docker build -t metadatatool .

# Run container
docker run -p 8080:8080 metadatatool
```

### Kubernetes
```bash
# Apply configurations
kubectl apply -f k8s/

# Check status
kubectl get pods -n metadatatool
```

## Troubleshooting
### Common Issues
1. **Database Connection**
   - Check credentials
   - Verify network access
   - Check PostgreSQL logs

2. **Storage Issues**
   - Verify S3 credentials
   - Check bucket permissions
   - Validate file paths

3. **AI Service**
   - Check API keys
   - Verify rate limits
   - Monitor response times

## Contributing
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License
MIT License - see LICENSE file for details
</file>

<file path="docs/TODO.md">
# MetadataTool v1 TODO

## High Priority (P0) - Core Functionality
- [ ] **Frontend Implementation**
  - [ ] Set up Next.js project with TypeScript
  - [ ] Implement basic UI components (Atomic Design)
  - [ ] Set up React Query for data fetching
  - [ ] Implement track list and detail views
  - [ ] Add metadata editing interface
  - [ ] Implement file upload component
  - [ ] Add basic error handling and loading states

- [x] **Authentication & Authorization**
  - [x] JWT-based authentication system
    - Implemented JWT token generation and validation
    - Added access and refresh token support
    - Implemented secure password hashing with bcrypt
  - [x] Role-based access control
    - Defined clear role hierarchy (Admin, User, Guest, System)
    - Implemented granular permissions system
    - Created role-permission mappings
  - [x] API key management
    - Added API key generation
    - Implemented API key authentication
  - [x] Session management
    - Implemented Redis-based session store
    - Added session cleanup mechanism
    - Implemented session middleware
    - Added session management endpoints (list, revoke)
    - Added concurrent session limits

- [ ] **Testing Infrastructure**
  - [x] Set up unit testing framework
    - Added tests for session store
    - Added tests for session middleware
    - Added tests for session management
  - [ ] Add integration tests for core flows
    - Test authentication flow
    - Test permission checks
    - Test API key usage
    - Test session management
  - [ ] Implement API endpoint tests
  - [ ] Add storage service tests
  - [ ] Set up CI pipeline with GitHub Actions

## Medium Priority (P1) - Enhanced Features
- [ ] **DDEX Integration**
  - [ ] Complete ERN 4.3 validation
  - [ ] Add DDEX export functionality
  - [ ] Implement batch export
  - [ ] Add DDEX version support
  - [ ] Implement import functionality

- [ ] **AI Processing**
  - [ ] Add retry mechanism for API calls
  - [ ] Implement rate limiting
  - [ ] Add progress tracking
  - [ ] Improve confidence scoring
  - [ ] Add batch processing optimization

- [ ] **Storage Service**
  - [ ] Add multi-part upload
  - [ ] Implement retry mechanism
  - [ ] Add file validation
  - [ ] Implement cleanup job
  - [ ] Add file versioning

## Low Priority (P2) - Nice to Have
- [ ] **Performance Optimization**
  - [ ] Implement caching layer
  - [ ] Add query optimization
  - [ ] Implement connection pooling
  - [ ] Add request rate limiting
  - [ ] Optimize batch operations

- [ ] **Monitoring & Analytics**
  - [ ] Set up basic metrics
  - [ ] Add error tracking
  - [ ] Implement audit logging
  - [ ] Add performance monitoring
  - [ ] Set up alerts

- [ ] **Documentation**
  - [x] Authentication & Authorization Design
    - Documented role hierarchy
    - Documented permission system
    - Documented auth middleware usage
    - Documented session management
  - [ ] API documentation
  - [ ] User guides
  - [ ] Deployment guides
  - [ ] Architecture documentation
  - [ ] Contributing guidelines

## Technical Debt
- [ ] Clean up error handling
- [ ] Standardize logging
- [ ] Improve configuration management
- [ ] Clean up dependencies
- [ ] Add code documentation

## Future Considerations (v2)
- [ ] Implement microservices architecture
- [ ] Add distributed caching
- [ ] Implement advanced AI features
- [ ] Add advanced analytics
- [ ] Implement advanced security features

## Next Steps (In Order)
1. **Testing Infrastructure**
   - Add unit tests for auth service
   - Add integration tests for auth flows
   - Set up CI pipeline

2. **Frontend Implementation**
   - Set up Next.js project
   - Implement auth components
   - Add session management UI

3. **Documentation**
   - Document API endpoints
   - Add sequence diagrams
   - Write deployment guide
</file>

<file path="internal/config/sentry.go">
package config
// SentryConfig holds Sentry error tracking configuration
type SentryConfig struct {
	DSN              string  `env:"SENTRY_DSN"`
	Environment      string  `env:"SENTRY_ENVIRONMENT" envDefault:"development"`
	Debug            bool    `env:"SENTRY_DEBUG" envDefault:"false"`
	SampleRate       float64 `env:"SENTRY_SAMPLE_RATE" envDefault:"1.0"`
	TracesSampleRate float64 `env:"SENTRY_TRACES_SAMPLE_RATE" envDefault:"0.2"`
}
</file>

<file path="internal/graphql/generated/resolver.go">
package generated
import (
	"context"
	"metadatatool/internal/pkg/domain"
)
// This file will not be regenerated automatically.
//
// It serves as dependency injection for your app, add any dependencies you require here.
type Resolver struct {
	TrackRepo      domain.TrackRepository
	AIService      domain.AIService
	DDEXService    domain.DDEXService
	AuthService    domain.AuthService
	StorageService domain.StorageService
}
// QueryResolver defines the query resolver interface
type QueryResolver interface {
	Track(ctx context.Context, id string) (*domain.Track, error)
	Tracks(ctx context.Context, first *int, after *string, filter *domain.TrackFilter, orderBy *string) (*domain.TrackConnection, error)
	SearchTracks(ctx context.Context, query string) ([]*domain.Track, error)
	TracksNeedingReview(ctx context.Context, first *int, after *string) (*domain.TrackConnection, error)
}
// MutationResolver defines the mutation resolver interface
type MutationResolver interface {
	CreateTrack(ctx context.Context, input domain.CreateTrackInput) (*domain.Track, error)
	UpdateTrack(ctx context.Context, input domain.UpdateTrackInput) (*domain.Track, error)
	DeleteTrack(ctx context.Context, id string) (bool, error)
	BatchProcessTracks(ctx context.Context, ids []string) (*domain.BatchResult, error)
	EnrichTrackMetadata(ctx context.Context, id string) (*domain.Track, error)
	ValidateTrackMetadata(ctx context.Context, id string) (*domain.BatchResult, error)
	ExportToDDEX(ctx context.Context, ids []string) (string, error)
}
// SubscriptionResolver defines the subscription resolver interface
type SubscriptionResolver interface {
	TrackUpdated(ctx context.Context, id string) (<-chan *domain.Track, error)
	BatchProcessingProgress(ctx context.Context, batchID string) (<-chan *domain.BatchResult, error)
}
// TrackResolver defines the track resolver interface
type TrackResolver interface {
	AIMetadata(ctx context.Context, obj *domain.Track) (*domain.AIMetadata, error)
	Metadata(ctx context.Context, obj *domain.Track) (*domain.Metadata, error)
}
func NewResolver(
	trackRepo domain.TrackRepository,
	aiService domain.AIService,
	ddexService domain.DDEXService,
	authService domain.AuthService,
	storageService domain.StorageService,
) *Resolver {
	return &Resolver{
		TrackRepo:      trackRepo,
		AIService:      aiService,
		DDEXService:    ddexService,
		AuthService:    authService,
		StorageService: storageService,
	}
}
// ResolverRoot defines the root resolver interface
type ResolverRoot interface {
	Query() QueryResolver
	Mutation() MutationResolver
	Subscription() SubscriptionResolver
	Track() TrackResolver
}
</file>

<file path="internal/graphql/resolvers/mutation.go">
package resolvers
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"path/filepath"
	"strings"
	"time"
	"github.com/google/uuid"
)
func (r *mutationResolver) CreateTrack(ctx context.Context, input domain.CreateTrackInput) (*domain.Track, error) {
	// Create track with basic fields
	track := &domain.Track{
		ID:        uuid.New().String(),
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
		Metadata: domain.CompleteTrackMetadata{
			BasicTrackMetadata: domain.BasicTrackMetadata{
				Title:  input.Title,
				Artist: input.Artist,
				Album:  stringValue(input.Album),
				Year:   intValue(input.Year),
				ISRC:   stringValue(input.ISRC),
			},
			Musical: domain.MusicalMetadata{
				Genre: stringValue(input.Genre),
			},
			Additional: domain.AdditionalMetadata{
				CustomFields: map[string]string{
					"label":     stringValue(input.Label),
					"territory": stringValue(input.Territory),
					"iswc":      stringValue(input.ISWC),
				},
			},
		},
	}
	// Handle audio file if provided
	if input.AudioFile != nil {
		// Create storage file
		storageFile := &domain.StorageFile{
			Key:         fmt.Sprintf("audio/%s/%s", track.ID, input.AudioFile.Filename),
			Name:        input.AudioFile.Filename,
			Size:        input.AudioFile.Size,
			Content:     input.AudioFile.File,
			ContentType: "audio/" + strings.TrimPrefix(filepath.Ext(input.AudioFile.Filename), "."),
		}
		// Upload file
		if err := r.StorageService.Upload(ctx, storageFile); err != nil {
			return nil, fmt.Errorf("failed to upload audio file: %w", err)
		}
		track.StoragePath = storageFile.Key
	}
	// Create track in database
	if err := r.TrackRepo.Create(ctx, track); err != nil {
		return nil, fmt.Errorf("failed to create track: %w", err)
	}
	return track, nil
}
// Helper functions for handling optional values
func stringValue(s *string) string {
	if s == nil {
		return ""
	}
	return *s
}
func intValue(i *int) int {
	if i == nil {
		return 0
	}
	return *i
}
func (r *mutationResolver) UpdateTrack(ctx context.Context, input domain.UpdateTrackInput) (*domain.Track, error) {
	// Get existing track
	track, err := r.TrackRepo.GetByID(ctx, input.ID)
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	// Update basic metadata fields if provided
	if input.Title != nil {
		track.Metadata.Title = *input.Title
	}
	if input.Artist != nil {
		track.Metadata.Artist = *input.Artist
	}
	if input.Album != nil {
		track.Metadata.Album = *input.Album
	}
	if input.Genre != nil {
		track.Metadata.Musical.Genre = *input.Genre
	}
	if input.Year != nil {
		track.Metadata.Year = *input.Year
	}
	if input.Label != nil {
		track.Metadata.Additional.CustomFields["label"] = *input.Label
	}
	if input.Territory != nil {
		track.Metadata.Additional.CustomFields["territory"] = *input.Territory
	}
	if input.ISRC != nil {
		track.Metadata.ISRC = *input.ISRC
	}
	if input.ISWC != nil {
		track.Metadata.Additional.CustomFields["iswc"] = *input.ISWC
	}
	// Update additional metadata if provided
	if input.Metadata != nil {
		if input.Metadata.BPM != nil {
			track.Metadata.Musical.BPM = *input.Metadata.BPM
		}
		if input.Metadata.Key != nil {
			track.Metadata.Musical.Key = *input.Metadata.Key
		}
		if input.Metadata.Mood != nil {
			track.Metadata.Musical.Mood = *input.Metadata.Mood
		}
		if input.Metadata.Labels != nil {
			// Convert string slice to map for tags
			tags := make(map[string]string)
			for _, label := range input.Metadata.Labels {
				tags[label] = "true"
			}
			track.Metadata.Additional.CustomTags = tags
		}
		if input.Metadata.CustomFields != nil {
			for k, v := range input.Metadata.CustomFields {
				track.Metadata.Additional.CustomFields[k] = v
			}
		}
	}
	track.UpdatedAt = time.Now()
	// Update track in database
	if err := r.TrackRepo.Update(ctx, track); err != nil {
		return nil, fmt.Errorf("failed to update track: %w", err)
	}
	return track, nil
}
func (r *mutationResolver) DeleteTrack(ctx context.Context, id string) (bool, error) {
	// Get track to check if it exists
	track, err := r.TrackRepo.GetByID(ctx, id)
	if err != nil {
		return false, fmt.Errorf("failed to get track: %w", err)
	}
	// Delete audio file if exists
	if track.StoragePath != "" {
		if err := r.StorageService.Delete(ctx, track.StoragePath); err != nil {
			return false, fmt.Errorf("failed to delete audio file: %w", err)
		}
	}
	// Delete track
	if err := r.TrackRepo.Delete(ctx, id); err != nil {
		return false, fmt.Errorf("failed to delete track: %w", err)
	}
	return true, nil
}
func (r *mutationResolver) BatchProcessTracks(ctx context.Context, ids []string) (*domain.BatchResult, error) {
	result := &domain.BatchResult{}
	for _, id := range ids {
		track, err := r.TrackRepo.GetByID(ctx, id)
		if err != nil {
			result.FailureCount++
			result.Errors = append(result.Errors, &domain.BatchError{
				TrackID: id,
				Message: fmt.Sprintf("failed to get track: %v", err),
				Code:    "NOT_FOUND",
			})
			continue
		}
		if err := r.AIService.EnrichMetadata(ctx, track); err != nil {
			result.FailureCount++
			result.Errors = append(result.Errors, &domain.BatchError{
				TrackID: id,
				Message: fmt.Sprintf("failed to enrich metadata: %v", err),
				Code:    "AI_ERROR",
			})
			continue
		}
		result.SuccessCount++
	}
	return result, nil
}
func (r *mutationResolver) EnrichTrackMetadata(ctx context.Context, id string) (*domain.Track, error) {
	// Get track
	track, err := r.TrackRepo.GetByID(ctx, id)
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	// Enrich metadata
	if err := r.AIService.EnrichMetadata(ctx, track); err != nil {
		return nil, fmt.Errorf("failed to enrich metadata: %w", err)
	}
	// Save changes
	if err := r.TrackRepo.Update(ctx, track); err != nil {
		return nil, fmt.Errorf("failed to update track: %w", err)
	}
	return track, nil
}
func (r *mutationResolver) ValidateTrackMetadata(ctx context.Context, id string) (*domain.BatchResult, error) {
	result := &domain.BatchResult{}
	// Get track
	track, err := r.TrackRepo.GetByID(ctx, id)
	if err != nil {
		result.FailureCount = 1
		result.Errors = append(result.Errors, &domain.BatchError{
			TrackID: id,
			Message: fmt.Sprintf("failed to get track: %v", err),
			Code:    "NOT_FOUND",
		})
		return result, nil
	}
	// Validate metadata
	confidence, err := r.AIService.ValidateMetadata(ctx, track)
	if err != nil {
		result.FailureCount = 1
		result.Errors = append(result.Errors, &domain.BatchError{
			TrackID: id,
			Message: fmt.Sprintf("failed to validate metadata: %v", err),
			Code:    "VALIDATION_ERROR",
		})
		return result, nil
	}
	if confidence < 0.85 {
		result.FailureCount = 1
		result.Errors = append(result.Errors, &domain.BatchError{
			TrackID: id,
			Message: fmt.Sprintf("low confidence score: %.2f", confidence),
			Code:    "LOW_CONFIDENCE",
		})
	} else {
		result.SuccessCount = 1
	}
	return result, nil
}
func (r *mutationResolver) ExportToDDEX(ctx context.Context, ids []string) (string, error) {
	var tracks []*domain.Track
	for _, id := range ids {
		track, err := r.TrackRepo.GetByID(ctx, id)
		if err != nil {
			return "", fmt.Errorf("failed to get track %s: %w", id, err)
		}
		tracks = append(tracks, track)
	}
	// Export to DDEX
	ddexXML, err := r.DDEXService.ExportTracks(ctx, tracks)
	if err != nil {
		return "", fmt.Errorf("failed to export tracks to DDEX: %w", err)
	}
	return ddexXML, nil
}
</file>

<file path="internal/graphql/resolvers/query.go">
package resolvers
import (
	"context"
	"encoding/base64"
	"fmt"
	"metadatatool/internal/pkg/domain"
)
func (r *queryResolver) Track(ctx context.Context, id string) (*domain.Track, error) {
	track, err := r.TrackRepo.GetByID(ctx, id)
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	return track, nil
}
func (r *queryResolver) Tracks(ctx context.Context, first *int, after *string, filter *domain.TrackFilter, orderBy *string) (*domain.TrackConnection, error) {
	// Default values
	limit := 10
	if first != nil {
		limit = *first
	}
	// Convert cursor to offset
	offset := 0
	if after != nil {
		// TODO: Implement cursor-based pagination
		// For now, using simple offset-based pagination
		offset = 0
	}
	// Build query from filter
	query := make(map[string]interface{})
	if filter != nil {
		if filter.Title != nil {
			query["title"] = *filter.Title
		}
		if filter.Artist != nil {
			query["artist"] = *filter.Artist
		}
		// ... add other filter fields
	}
	// Get tracks
	tracks, err := r.TrackRepo.SearchByMetadata(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to search tracks: %w", err)
	}
	// Build connection
	edges := make([]*domain.TrackEdge, 0, len(tracks))
	for _, track := range tracks {
		edges = append(edges, &domain.TrackEdge{
			Node:   track,
			Cursor: encodeCursor(track.ID),
		})
	}
	return &domain.TrackConnection{
		Edges: edges,
		PageInfo: &domain.PageInfo{
			HasNextPage:     len(tracks) == limit,
			HasPreviousPage: offset > 0,
			StartCursor:     encodeCursor(tracks[0].ID),
			EndCursor:       encodeCursor(tracks[len(tracks)-1].ID),
		},
		TotalCount: len(tracks),
	}, nil
}
func (r *queryResolver) SearchTracks(ctx context.Context, query string) ([]*domain.Track, error) {
	// Implement full-text search
	searchQuery := map[string]interface{}{
		"$text": map[string]interface{}{
			"$search": query,
		},
	}
	tracks, err := r.TrackRepo.SearchByMetadata(ctx, searchQuery)
	if err != nil {
		return nil, fmt.Errorf("failed to search tracks: %w", err)
	}
	return tracks, nil
}
func (r *queryResolver) TracksNeedingReview(ctx context.Context, first *int, after *string) (*domain.TrackConnection, error) {
	// Default values
	limit := 10
	if first != nil {
		limit = *first
	}
	// Query tracks needing review
	query := map[string]interface{}{
		"needs_review": true,
	}
	tracks, err := r.TrackRepo.SearchByMetadata(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to get tracks needing review: %w", err)
	}
	// Build connection
	edges := make([]*domain.TrackEdge, 0, len(tracks))
	for _, track := range tracks {
		edges = append(edges, &domain.TrackEdge{
			Node:   track,
			Cursor: encodeCursor(track.ID),
		})
	}
	return &domain.TrackConnection{
		Edges: edges,
		PageInfo: &domain.PageInfo{
			HasNextPage:     len(tracks) == limit,
			HasPreviousPage: false, // First page
			StartCursor:     encodeCursor(tracks[0].ID),
			EndCursor:       encodeCursor(tracks[len(tracks)-1].ID),
		},
		TotalCount: len(tracks),
	}, nil
}
// Helper function to encode cursor
func encodeCursor(id string) string {
	return base64.StdEncoding.EncodeToString([]byte(id))
}
// Helper function to decode cursor
func decodeCursor(cursor string) (string, error) {
	bytes, err := base64.StdEncoding.DecodeString(cursor)
	if err != nil {
		return "", fmt.Errorf("invalid cursor: %w", err)
	}
	return string(bytes), nil
}
</file>

<file path="internal/graphql/resolvers/resolver.go">
package resolvers
import (
	"metadatatool/internal/graphql/generated"
)
// Resolver is the base resolver for all GraphQL operations
type Resolver struct {
	*generated.Resolver
}
// NewResolver creates a new resolver instance
func NewResolver(r *generated.Resolver) *Resolver {
	return &Resolver{
		Resolver: r,
	}
}
// Query returns the query resolver
func (r *Resolver) Query() generated.QueryResolver {
	return &queryResolver{r.Resolver}
}
// Mutation returns the mutation resolver
func (r *Resolver) Mutation() generated.MutationResolver {
	return &mutationResolver{r.Resolver}
}
// Subscription returns the subscription resolver
func (r *Resolver) Subscription() generated.SubscriptionResolver {
	return &subscriptionResolver{r.Resolver}
}
// Track returns the track resolver
func (r *Resolver) Track() generated.TrackResolver {
	return &trackResolver{r.Resolver}
}
</file>

<file path="internal/graphql/resolvers/subscription.go">
package resolvers
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
)
func (r *subscriptionResolver) TrackUpdated(ctx context.Context, id string) (<-chan *domain.Track, error) {
	trackChan := make(chan *domain.Track, 1)
	// Subscribe to track updates
	go func() {
		defer close(trackChan)
		// TODO: Implement real-time updates using Redis pub/sub or similar
		// For now, just get the track once
		track, err := r.TrackRepo.GetByID(ctx, id)
		if err != nil {
			// Log error and return
			fmt.Printf("failed to get track: %v\n", err)
			return
		}
		select {
		case trackChan <- track:
		case <-ctx.Done():
			return
		}
	}()
	return trackChan, nil
}
func (r *subscriptionResolver) BatchProcessingProgress(ctx context.Context, batchID string) (<-chan *domain.BatchResult, error) {
	resultChan := make(chan *domain.BatchResult, 1)
	// Subscribe to batch processing progress
	go func() {
		defer close(resultChan)
		// TODO: Implement real-time progress updates
		// For now, just return a dummy result
		result := &domain.BatchResult{
			SuccessCount: 0,
			FailureCount: 0,
			Errors:       nil,
		}
		select {
		case resultChan <- result:
		case <-ctx.Done():
			return
		}
	}()
	return resultChan, nil
}
</file>

<file path="internal/graphql/resolvers/track.go">
package resolvers
import (
	"context"
	"metadatatool/internal/pkg/domain"
)
func (r *trackResolver) AIMetadata(ctx context.Context, obj *domain.Track) (*domain.AIMetadata, error) {
	// Convert TrackAIMetadata to AIMetadata
	if obj.Metadata.AI == nil {
		return nil, nil
	}
	return &domain.AIMetadata{
		Provider:     domain.AIProviderQwen2,
		Energy:       0, // These fields are not available in TrackAIMetadata
		Danceability: 0,
		ProcessedAt:  obj.Metadata.AI.ProcessedAt,
		ProcessingMs: 0,
		NeedsReview:  obj.Metadata.AI.NeedsReview,
		ReviewReason: obj.Metadata.AI.ReviewReason,
	}, nil
}
func (r *trackResolver) Metadata(ctx context.Context, obj *domain.Track) (*domain.Metadata, error) {
	// Convert CompleteTrackMetadata to Metadata
	var labels []string
	for tag := range obj.Metadata.Additional.CustomTags {
		labels = append(labels, tag)
	}
	return &domain.Metadata{
		ISRC:         obj.ISRC(),
		ISWC:         obj.ISWC(),
		BPM:          obj.BPM(),
		Key:          obj.Key(),
		Mood:         obj.Mood(),
		Labels:       labels,
		AITags:       obj.AITags(),
		Confidence:   obj.AIConfidence(),
		ModelVersion: obj.ModelVersion(),
		CustomFields: obj.Metadata.Additional.CustomFields,
	}, nil
}
</file>

<file path="internal/graphql/resolvers/types.go">
package resolvers
import "metadatatool/internal/graphql/generated"
// queryResolver implements the query resolver
type queryResolver struct {
	*generated.Resolver
}
// mutationResolver implements the mutation resolver
type mutationResolver struct {
	*generated.Resolver
}
// subscriptionResolver implements the subscription resolver
type subscriptionResolver struct {
	*generated.Resolver
}
// trackResolver implements the track resolver
type trackResolver struct {
	*generated.Resolver
}
</file>

<file path="internal/graphql/schema/schema.graphql">
scalar DateTime
scalar Upload

type Track {
  id: ID!
  title: String!
  artist: String!
  album: String
  genre: String
  duration: Float
  filePath: String
  year: Int
  label: String
  territory: String
  isrc: String
  iswc: String
  bpm: Float
  key: String
  mood: String
  publisher: String

  # Audio metadata
  audioFormat: String
  fileSize: Int

  # AI-related fields
  aiTags: [String!]
  aiConfidence: Float
  modelVersion: String
  needsReview: Boolean
  aiMetadata: AIMetadata

  # Base metadata
  metadata: Metadata
  createdAt: DateTime!
  updatedAt: DateTime!
  deletedAt: DateTime
}

type AIMetadata {
  provider: String!
  energy: Float
  danceability: Float
  processedAt: DateTime!
  processingMs: Int!
  needsReview: Boolean!
  reviewReason: String
}

type Metadata {
  isrc: String
  iswc: String
  bpm: Float
  key: String
  mood: String
  labels: [String!]
  aiTags: [String!]
  confidence: Float
  modelVersion: String
  customFields: JSON
}

type TrackConnection {
  edges: [TrackEdge!]!
  pageInfo: PageInfo!
  totalCount: Int!
}

type TrackEdge {
  node: Track!
  cursor: String!
}

type PageInfo {
  hasNextPage: Boolean!
  hasPreviousPage: Boolean!
  startCursor: String
  endCursor: String
}

input CreateTrackInput {
  title: String!
  artist: String!
  album: String
  genre: String
  year: Int
  label: String
  territory: String
  isrc: String
  iswc: String
  audioFile: Upload
}

input UpdateTrackInput {
  id: ID!
  title: String
  artist: String
  album: String
  genre: String
  year: Int
  label: String
  territory: String
  isrc: String
  iswc: String
  metadata: MetadataInput
}

input MetadataInput {
  isrc: String
  iswc: String
  bpm: Float
  key: String
  mood: String
  labels: [String!]
  customFields: JSON
}

input TrackFilter {
  title: String
  artist: String
  album: String
  genre: String
  label: String
  isrc: String
  iswc: String
  needsReview: Boolean
  createdFrom: DateTime
  createdTo: DateTime
}

type BatchResult {
  successCount: Int!
  failureCount: Int!
  errors: [BatchError!]
}

type BatchError {
  trackId: ID
  message: String!
  code: String!
}

type Query {
  # Get a single track by ID
  track(id: ID!): Track

  # List tracks with pagination and filtering
  tracks(
    first: Int
    after: String
    filter: TrackFilter
    orderBy: String
  ): TrackConnection!

  # Search tracks by metadata
  searchTracks(query: String!): [Track!]!

  # Get tracks that need review
  tracksNeedingReview(first: Int, after: String): TrackConnection!
}

type Mutation {
  # Create a new track
  createTrack(input: CreateTrackInput!): Track!

  # Update an existing track
  updateTrack(input: UpdateTrackInput!): Track!

  # Delete a track
  deleteTrack(id: ID!): Boolean!

  # Batch process tracks
  batchProcessTracks(ids: [ID!]!): BatchResult!

  # Enrich track metadata using AI
  enrichTrackMetadata(id: ID!): Track!

  # Validate track metadata
  validateTrackMetadata(id: ID!): BatchResult!

  # Export tracks to DDEX
  exportToDDEX(ids: [ID!]!): String!
}

type Subscription {
  # Subscribe to track updates
  trackUpdated(id: ID!): Track!

  # Subscribe to batch processing progress
  batchProcessingProgress(batchId: ID!): BatchResult!
}

scalar JSON
</file>

<file path="internal/handler/middleware/auth_test.go">
package middleware
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"net/http/httptest"
	"testing"
	"github.com/gin-gonic/gin"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
)
// MockAuthService is a mock implementation of domain.AuthService
type MockAuthService struct {
	mock.Mock
}
func (m *MockAuthService) GenerateTokens(user *domain.User) (*domain.TokenPair, error) {
	args := m.Called(user)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.TokenPair), args.Error(1)
}
func (m *MockAuthService) ValidateToken(token string) (*domain.Claims, error) {
	args := m.Called(token)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.Claims), args.Error(1)
}
func (m *MockAuthService) RefreshToken(refreshToken string) (*domain.TokenPair, error) {
	args := m.Called(refreshToken)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.TokenPair), args.Error(1)
}
func (m *MockAuthService) HashPassword(password string) (string, error) {
	args := m.Called(password)
	return args.String(0), args.Error(1)
}
func (m *MockAuthService) VerifyPassword(hash, password string) error {
	args := m.Called(hash, password)
	return args.Error(0)
}
func (m *MockAuthService) HasPermission(role domain.Role, permission domain.Permission) bool {
	args := m.Called(role, permission)
	return args.Bool(0)
}
func (m *MockAuthService) GetPermissions(role domain.Role) []domain.Permission {
	args := m.Called(role)
	return args.Get(0).([]domain.Permission)
}
func (m *MockAuthService) GenerateAPIKey() (string, error) {
	args := m.Called()
	return args.String(0), args.Error(1)
}
// MockUserRepository is a mock implementation of domain.UserRepository
type MockUserRepository struct {
	mock.Mock
}
func (m *MockUserRepository) Create(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	args := m.Called(ctx, id)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	args := m.Called(ctx, email)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	args := m.Called(ctx, apiKey)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) Update(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) Delete(ctx context.Context, id string) error {
	args := m.Called(ctx, id)
	return args.Error(0)
}
func (m *MockUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	args := m.Called(ctx, offset, limit)
	return args.Get(0).([]*domain.User), args.Error(1)
}
func (m *MockUserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	args := m.Called(ctx, userID, apiKey)
	return args.Error(0)
}
func TestAuth_Middleware(t *testing.T) {
	t.Run("valid_token", func(t *testing.T) {
		router := setupTestRouter()
		authService := &MockAuthService{}
		claims := &domain.Claims{
			UserID: "test-user",
			Role:   domain.RoleUser,
		}
		authService.On("ValidateToken", "valid-token").Return(claims, nil)
		var called bool
		router.Use(Auth(authService))
		router.GET("/test", func(c *gin.Context) {
			called = true
			userClaims, exists := c.Get("user")
			assert.True(t, exists)
			assert.Equal(t, claims, userClaims)
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		req.Header.Set("Authorization", "Bearer valid-token")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.True(t, called)
		authService.AssertExpectations(t)
	})
	t.Run("missing_token", func(t *testing.T) {
		router := setupTestRouter()
		authService := &MockAuthService{}
		var called bool
		router.Use(Auth(authService))
		router.GET("/test", func(c *gin.Context) {
			called = true
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusUnauthorized, w.Code)
		assert.False(t, called)
		authService.AssertExpectations(t)
	})
	t.Run("invalid_token", func(t *testing.T) {
		router := setupTestRouter()
		authService := &MockAuthService{}
		authService.On("ValidateToken", "invalid-token").Return(nil, fmt.Errorf("invalid token"))
		var called bool
		router.Use(Auth(authService))
		router.GET("/test", func(c *gin.Context) {
			called = true
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		req.Header.Set("Authorization", "Bearer invalid-token")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusUnauthorized, w.Code)
		assert.False(t, called)
		authService.AssertExpectations(t)
	})
}
func TestRequireRole_Middleware(t *testing.T) {
	tests := []struct {
		name           string
		setupSession   func() *domain.Session
		expectedStatus int
		shouldCall     bool
	}{
		{
			name: "has_required_role",
			setupSession: func() *domain.Session {
				return &domain.Session{
					ID:     "test-session",
					UserID: "test-user",
					Role:   domain.RoleAdmin,
				}
			},
			expectedStatus: http.StatusOK,
			shouldCall:     true,
		},
		{
			name: "insufficient_role",
			setupSession: func() *domain.Session {
				return &domain.Session{
					ID:     "test-session",
					UserID: "test-user",
					Role:   domain.RoleUser,
				}
			},
			expectedStatus: http.StatusForbidden,
			shouldCall:     false,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Create a new router for each test case
			router := setupTestRouter()
			var called bool
			// Create test handler
			handler := func(c *gin.Context) {
				called = true
				c.Status(http.StatusOK)
			}
			// Set up middleware chain in correct order
			router.Use(func(c *gin.Context) {
				// Set session in context before role check
				c.Set("session", tt.setupSession())
				c.Next()
			})
			// Add the role middleware after session setup
			router.Use(RequireRole(domain.RoleAdmin))
			// Add the test handler
			router.GET("/test", handler)
			// Make request
			w := httptest.NewRecorder()
			req := httptest.NewRequest("GET", "/test", nil)
			router.ServeHTTP(w, req)
			// Assert response
			assert.Equal(t, tt.expectedStatus, w.Code)
			assert.Equal(t, tt.shouldCall, called)
		})
	}
}
func TestRequirePermission_Middleware(t *testing.T) {
	t.Run("has_required_permission", func(t *testing.T) {
		router := setupTestRouter()
		session := &domain.Session{
			ID:          "test-session",
			UserID:      "test-user",
			Role:        domain.RoleAdmin,
			Permissions: []domain.Permission{domain.PermissionManageUsers},
		}
		var called bool
		router.Use(func(c *gin.Context) {
			c.Set("session", session)
			c.Next()
		})
		router.Use(RequirePermission(domain.PermissionManageUsers))
		router.GET("/test", func(c *gin.Context) {
			called = true
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.True(t, called)
	})
	t.Run("missing_required_permission", func(t *testing.T) {
		router := setupTestRouter()
		session := &domain.Session{
			ID:          "test-session",
			UserID:      "test-user",
			Role:        domain.RoleUser,
			Permissions: []domain.Permission{domain.PermissionReadTrack},
		}
		var called bool
		router.Use(func(c *gin.Context) {
			c.Set("session", session)
			c.Next()
		})
		router.Use(RequirePermission(domain.PermissionManageUsers))
		router.GET("/test", func(c *gin.Context) {
			called = true
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusForbidden, w.Code)
		assert.False(t, called)
	})
}
func TestAPIKeyAuth_Middleware(t *testing.T) {
	userRepo := new(MockUserRepository)
	gin.SetMode(gin.TestMode)
	t.Run("valid API key", func(t *testing.T) {
		router := gin.New()
		router.Use(APIKeyAuth(userRepo))
		user := &domain.User{
			ID:          "test-user",
			Email:       "test@example.com",
			Role:        domain.RoleUser,
			Permissions: []domain.Permission{domain.PermissionReadTrack},
		}
		userRepo.On("GetByAPIKey", mock.Anything, "valid-key").Return(user, nil)
		var gotClaims *domain.Claims
		router.GET("/test", func(c *gin.Context) {
			claims, exists := c.Get("user")
			assert.True(t, exists)
			gotClaims = claims.(*domain.Claims)
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		req.Header.Set("X-API-Key", "valid-key")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.Equal(t, user.ID, gotClaims.UserID)
		userRepo.AssertExpectations(t)
	})
	t.Run("invalid API key", func(t *testing.T) {
		router := gin.New()
		router.Use(APIKeyAuth(userRepo))
		userRepo.On("GetByAPIKey", mock.Anything, "invalid-key").Return(nil, assert.AnError)
		router.GET("/test", func(c *gin.Context) {
			t.Error("handler should not be called")
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		req.Header.Set("X-API-Key", "invalid-key")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusUnauthorized, w.Code)
		userRepo.AssertExpectations(t)
	})
	t.Run("no API key", func(t *testing.T) {
		router := gin.New()
		router.Use(APIKeyAuth(userRepo))
		var called bool
		router.GET("/test", func(c *gin.Context) {
			called = true
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.True(t, called)
	})
}
</file>

<file path="internal/handler/middleware/auth.go">
package middleware
import (
	"metadatatool/internal/pkg/domain"
	"net/http"
	"strings"
	"github.com/gin-gonic/gin"
)
// Auth middleware validates JWT tokens and sets user claims in context
func Auth(authService domain.AuthService) gin.HandlerFunc {
	return func(c *gin.Context) {
		token := extractToken(c)
		if token == "" {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "missing authorization token",
			})
			c.Abort()
			return
		}
		claims, err := authService.ValidateToken(token)
		if err != nil {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "invalid token",
			})
			c.Abort()
			return
		}
		// Set claims in context
		c.Set("user", claims)
		c.Next()
	}
}
// RequireRole middleware ensures the user has the required role
func RequireRole(role domain.Role) gin.HandlerFunc {
	return func(c *gin.Context) {
		session, exists := c.Get("session")
		if !exists {
			c.AbortWithStatus(http.StatusUnauthorized)
			return
		}
		userSession := session.(*domain.Session)
		// Check if user's role is sufficient (equal or higher)
		if userSession.Role != role && userSession.Role != domain.RoleAdmin {
			c.AbortWithStatus(http.StatusForbidden)
			return
		}
		c.Next()
	}
}
// RequirePermission middleware ensures the user has the required permission
func RequirePermission(permission domain.Permission) gin.HandlerFunc {
	return func(c *gin.Context) {
		session, exists := c.Get("session")
		if !exists {
			c.AbortWithStatus(http.StatusUnauthorized)
			return
		}
		userSession := session.(*domain.Session)
		hasPermission := false
		for _, p := range userSession.Permissions {
			if p == permission {
				hasPermission = true
				break
			}
		}
		if !hasPermission {
			c.AbortWithStatus(http.StatusForbidden)
			return
		}
		c.Next()
	}
}
// APIKeyAuth validates API key authentication
func APIKeyAuth(userRepo domain.UserRepository) gin.HandlerFunc {
	return func(c *gin.Context) {
		apiKey := c.GetHeader("X-API-Key")
		if apiKey == "" {
			c.Next() // Allow request to continue for other auth methods
			return
		}
		user, err := userRepo.GetByAPIKey(c, apiKey)
		if err != nil || user == nil {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "invalid API key",
			})
			c.Abort()
			return
		}
		// Create claims from user
		claims := &domain.Claims{
			UserID:      user.ID,
			Email:       user.Email,
			Role:        user.Role,
			Permissions: user.Permissions,
		}
		// Set claims in context
		c.Set("user", claims)
		c.Next()
	}
}
// Helper function to extract token from Authorization header
func extractToken(c *gin.Context) string {
	bearerToken := c.GetHeader("Authorization")
	if len(strings.Split(bearerToken, " ")) == 2 {
		return strings.Split(bearerToken, " ")[1]
	}
	return ""
}
</file>

<file path="internal/handler/middleware/metrics.go">
package middleware
import (
	"strconv"
	"sync"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/prometheus/client_golang/prometheus"
)
var (
	// Metrics singleton instance
	httpRequestsTotal   *prometheus.CounterVec
	httpRequestDuration *prometheus.HistogramVec
	initOnce            sync.Once
	metricsMutex        sync.Mutex
)
func initMetrics() {
	metricsMutex.Lock()
	defer metricsMutex.Unlock()
	// Unregister existing metrics if they exist
	prometheus.Unregister(httpRequestsTotal)
	prometheus.Unregister(httpRequestDuration)
	// Create new metrics
	httpRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Total number of HTTP requests",
		},
		[]string{"method", "path", "status"},
	)
	httpRequestDuration = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "http_request_duration_seconds",
			Help:    "HTTP request duration in seconds",
			Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
		},
		[]string{"method", "path"},
	)
	// Register new metrics
	prometheus.MustRegister(httpRequestsTotal, httpRequestDuration)
}
// Metrics middleware collects HTTP metrics
func Metrics() gin.HandlerFunc {
	// Initialize metrics only once
	initOnce.Do(initMetrics)
	return func(c *gin.Context) {
		start := time.Now()
		// Process request
		c.Next()
		// Record metrics
		duration := time.Since(start).Seconds()
		status := strconv.Itoa(c.Writer.Status())
		httpRequestsTotal.WithLabelValues(c.Request.Method, c.FullPath(), status).Inc()
		httpRequestDuration.WithLabelValues(c.Request.Method, c.FullPath()).Observe(duration)
	}
}
</file>

<file path="internal/handler/middleware/session_test.go">
package middleware
import (
	"context"
	"errors"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
)
// MockSessionStore is a mock implementation of domain.SessionStore
type MockSessionStore struct {
	mock.Mock
	ExpectedCalls []mock.Call
}
func (m *MockSessionStore) Create(ctx context.Context, session *domain.Session) error {
	args := m.Called(ctx, session)
	return args.Error(0)
}
func (m *MockSessionStore) Get(ctx context.Context, sessionID string) (*domain.Session, error) {
	args := m.Called(ctx, sessionID)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.Session), args.Error(1)
}
func (m *MockSessionStore) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	args := m.Called(ctx, userID)
	return args.Get(0).([]*domain.Session), args.Error(1)
}
func (m *MockSessionStore) Update(ctx context.Context, session *domain.Session) error {
	args := m.Called(ctx, session)
	return args.Error(0)
}
func (m *MockSessionStore) Delete(ctx context.Context, sessionID string) error {
	args := m.Called(ctx, sessionID)
	return args.Error(0)
}
func (m *MockSessionStore) DeleteUserSessions(ctx context.Context, userID string) error {
	args := m.Called(ctx, userID)
	return args.Error(0)
}
func (m *MockSessionStore) DeleteExpired(ctx context.Context) error {
	args := m.Called(ctx)
	return args.Error(0)
}
func (m *MockSessionStore) Touch(ctx context.Context, sessionID string) error {
	args := m.Called(ctx, sessionID)
	return args.Error(0)
}
func setupTestRouter() *gin.Engine {
	gin.SetMode(gin.TestMode)
	return gin.New()
}
func TestSession_Middleware(t *testing.T) {
	store := new(MockSessionStore)
	config := domain.SessionConfig{
		CookieName:     "session_id",
		CookiePath:     "/",
		CookieDomain:   "localhost",
		CookieSecure:   true,
		CookieHTTPOnly: true,
	}
	tests := []struct {
		name           string
		setupMocks     func()
		setupRequest   func(*http.Request)
		expectedStatus int
		checkContext   func(*testing.T, *gin.Context)
	}{
		{
			name: "valid_session",
			setupMocks: func() {
				store.ExpectedCalls = nil
				session := &domain.Session{
					ID:          "test-session",
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionReadTrack},
					ExpiresAt:   time.Now().Add(24 * time.Hour),
					CreatedAt:   time.Now(),
					LastSeenAt:  time.Now(),
				}
				store.On("Get", mock.Anything, "test-session").Return(session, nil)
				store.On("Touch", mock.Anything, "test-session").Return(nil)
			},
			setupRequest: func(req *http.Request) {
				req.AddCookie(&http.Cookie{
					Name:  "session_id",
					Value: "test-session",
				})
			},
			expectedStatus: http.StatusOK,
			checkContext: func(t *testing.T, c *gin.Context) {
				session, exists := c.Get("session")
				assert.True(t, exists)
				assert.NotNil(t, session)
				userSession, ok := session.(*domain.Session)
				assert.True(t, ok)
				assert.Equal(t, "test-user", userSession.UserID)
			},
		},
		{
			name: "session_store_error",
			setupMocks: func() {
				store.ExpectedCalls = nil
				store.On("Get", mock.Anything, "test-session").Return(nil, errors.New("store error"))
			},
			setupRequest: func(req *http.Request) {
				req.AddCookie(&http.Cookie{
					Name:  "session_id",
					Value: "test-session",
				})
			},
			expectedStatus: http.StatusUnauthorized,
			checkContext: func(t *testing.T, c *gin.Context) {
				_, exists := c.Get("session")
				assert.False(t, exists)
			},
		},
		{
			name: "no_session_cookie",
			setupMocks: func() {
				store.ExpectedCalls = nil
			},
			setupRequest:   func(req *http.Request) {},
			expectedStatus: http.StatusOK,
			checkContext: func(t *testing.T, c *gin.Context) {
				_, exists := c.Get("session")
				assert.False(t, exists)
			},
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Setup
			router := gin.New()
			router.Use(Session(store, config))
			router.GET("/test", func(c *gin.Context) {
				tt.checkContext(t, c)
				c.Status(tt.expectedStatus)
			})
			// Setup mocks
			tt.setupMocks()
			// Create request
			w := httptest.NewRecorder()
			req, _ := http.NewRequest("GET", "/test", nil)
			tt.setupRequest(req)
			// Perform request
			router.ServeHTTP(w, req)
			// Assert response
			assert.Equal(t, tt.expectedStatus, w.Code)
			// Verify mocks
			store.AssertExpectations(t)
		})
	}
}
func TestCreateSession_Middleware(t *testing.T) {
	store := &MockSessionStore{}
	cfg := domain.SessionConfig{
		CookieName:         "session_id",
		CookiePath:         "/",
		CookieDomain:       "localhost",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		MaxSessionsPerUser: 5,
		SessionDuration:    24 * time.Hour,
	}
	t.Run("creates_session_for_authenticated_user", func(t *testing.T) {
		store.ExpectedCalls = nil
		claims := &domain.Claims{
			UserID:      "test-user",
			Role:        domain.RoleUser,
			Permissions: []domain.Permission{domain.PermissionReadTrack},
		}
		// Set up mock expectations
		store.On("GetUserSessions", mock.Anything, "test-user").Return([]*domain.Session{}, nil).Once()
		store.On("Create", mock.Anything, mock.MatchedBy(func(s *domain.Session) bool {
			return s.UserID == claims.UserID &&
				s.Role == claims.Role &&
				len(s.Permissions) == len(claims.Permissions) &&
				s.Permissions[0] == claims.Permissions[0] &&
				!s.ExpiresAt.IsZero() &&
				!s.CreatedAt.IsZero() &&
				!s.LastSeenAt.IsZero()
		})).Return(nil).Once()
		router := setupTestRouter()
		// First middleware to set claims
		router.Use(func(c *gin.Context) {
			c.Set("claims", claims)
			c.Next()
		})
		// Then the CreateSession middleware
		router.Use(CreateSession(store, cfg))
		router.GET("/test", func(c *gin.Context) {
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		store.AssertExpectations(t)
		// Verify session cookie was set
		cookies := w.Result().Cookies()
		var sessionCookie *http.Cookie
		for _, cookie := range cookies {
			if cookie.Name == cfg.CookieName {
				sessionCookie = cookie
				break
			}
		}
		assert.NotNil(t, sessionCookie)
		assert.NotEmpty(t, sessionCookie.Value)
		assert.Equal(t, cfg.CookiePath, sessionCookie.Path)
		assert.Equal(t, cfg.CookieDomain, sessionCookie.Domain)
		assert.Equal(t, cfg.CookieSecure, sessionCookie.Secure)
		assert.Equal(t, cfg.CookieHTTPOnly, sessionCookie.HttpOnly)
	})
	t.Run("no_session_created_for_unauthenticated_user", func(t *testing.T) {
		store.ExpectedCalls = nil
		router := setupTestRouter()
		router.Use(CreateSession(store, cfg))
		router.GET("/test", func(c *gin.Context) {
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		store.AssertNotCalled(t, "Create")
		store.AssertNotCalled(t, "GetUserSessions")
	})
}
func TestClearSession_Middleware(t *testing.T) {
	sessionStore := new(MockSessionStore)
	cfg := domain.SessionConfig{
		CookieName:     "session_id",
		CookiePath:     "/",
		CookieDomain:   "localhost",
		CookieSecure:   true,
		CookieHTTPOnly: true,
	}
	t.Run("clear existing session", func(t *testing.T) {
		router := setupTestRouter()
		router.Use(ClearSession(sessionStore, cfg))
		sessionStore.On("Delete", mock.Anything, "test-session").Return(nil)
		var called bool
		router.GET("/test", func(c *gin.Context) {
			called = true
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		req.AddCookie(&http.Cookie{Name: cfg.CookieName, Value: "test-session"})
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.True(t, called)
		sessionStore.AssertExpectations(t)
		// Check that cookie was removed
		var found bool
		for _, cookie := range w.Result().Cookies() {
			if cookie.Name == cfg.CookieName {
				found = true
				assert.Equal(t, -1, cookie.MaxAge)
				assert.Equal(t, cfg.CookiePath, cookie.Path)
				assert.Equal(t, cfg.CookieDomain, cookie.Domain)
				assert.Equal(t, cfg.CookieSecure, cookie.Secure)
				assert.Equal(t, cfg.CookieHTTPOnly, cookie.HttpOnly)
			}
		}
		assert.True(t, found, "session cookie should be present with negative MaxAge")
	})
	t.Run("no existing session", func(t *testing.T) {
		router := setupTestRouter()
		router.Use(ClearSession(sessionStore, cfg))
		var called bool
		router.GET("/test", func(c *gin.Context) {
			called = true
			c.Status(http.StatusOK)
		})
		w := httptest.NewRecorder()
		req := httptest.NewRequest("GET", "/test", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		assert.True(t, called)
		sessionStore.AssertNotCalled(t, "Delete")
	})
}
func TestRequireSession_Middleware(t *testing.T) {
	store := new(MockSessionStore)
	config := domain.SessionConfig{
		CookieName:     "session_id",
		CookiePath:     "/",
		CookieDomain:   "localhost",
		CookieSecure:   true,
		CookieHTTPOnly: true,
	}
	tests := []struct {
		name           string
		setupContext   func(*gin.Context)
		expectedStatus int
	}{
		{
			name: "valid_session",
			setupContext: func(c *gin.Context) {
				session := &domain.Session{
					ID:          "test-session",
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionReadTrack},
					ExpiresAt:   time.Now().Add(24 * time.Hour),
					CreatedAt:   time.Now(),
					LastSeenAt:  time.Now(),
				}
				c.Set("session", session)
			},
			expectedStatus: http.StatusOK,
		},
		{
			name:           "no_session",
			setupContext:   func(c *gin.Context) {},
			expectedStatus: http.StatusUnauthorized,
		},
		{
			name: "expired_session",
			setupContext: func(c *gin.Context) {
				session := &domain.Session{
					ID:          "test-session",
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionReadTrack},
					ExpiresAt:   time.Now().Add(-24 * time.Hour), // Expired
					CreatedAt:   time.Now().Add(-48 * time.Hour),
					LastSeenAt:  time.Now().Add(-24 * time.Hour),
				}
				c.Set("session", session)
			},
			expectedStatus: http.StatusUnauthorized,
		},
		{
			name: "invalid_session_type",
			setupContext: func(c *gin.Context) {
				c.Set("session", "invalid-session-type")
			},
			expectedStatus: http.StatusInternalServerError,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			router := setupTestRouter()
			router.Use(Session(store, config))
			router.GET("/test", RequireSession(), func(c *gin.Context) {
				c.Status(http.StatusOK)
			})
			w := httptest.NewRecorder()
			req := httptest.NewRequest(http.MethodGet, "/test", nil)
			// Setup context
			c, _ := gin.CreateTestContext(w)
			c.Request = req
			tt.setupContext(c)
			router.ServeHTTP(w, req)
			assert.Equal(t, tt.expectedStatus, w.Code)
		})
	}
}
</file>

<file path="internal/handler/middleware/session.go">
package middleware
import (
	"bytes"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
)
// Session middleware attaches the session to the context if present
func Session(store domain.SessionStore, config domain.SessionConfig) gin.HandlerFunc {
	return func(c *gin.Context) {
		sessionID, err := c.Cookie(config.CookieName)
		if err != nil {
			// No cookie is not an error, just continue
			c.Next()
			return
		}
		session, err := store.Get(c.Request.Context(), sessionID)
		if err != nil {
			c.AbortWithStatusJSON(http.StatusUnauthorized, gin.H{
				"error": "Invalid session",
			})
			return
		}
		if session == nil {
			c.Next()
			return
		}
		// Check if session is expired
		if session.ExpiresAt.Before(time.Now()) {
			// Delete expired session
			if err := store.Delete(c.Request.Context(), sessionID); err != nil {
				c.Error(fmt.Errorf("failed to delete expired session: %w", err))
			}
			c.Next()
			return
		}
		// Update last seen time
		if err := store.Touch(c.Request.Context(), sessionID); err != nil {
			// Log the error but continue with the session
			c.Error(fmt.Errorf("failed to update session last seen time: %w", err))
		}
		c.Set("session", session)
		c.Set("session_id", session.ID)
		c.Set("user_id", session.UserID)
		c.Set("role", session.Role)
		c.Set("permissions", session.Permissions)
		c.Next()
	}
}
// CreateSession middleware creates a new session for authenticated users
func CreateSession(store domain.SessionStore, cfg domain.SessionConfig) gin.HandlerFunc {
	return func(c *gin.Context) {
		claims, exists := c.Get("claims")
		if !exists {
			// No claims means no authentication, just continue
			c.Next()
			return
		}
		userClaims, ok := claims.(*domain.Claims)
		if !ok {
			c.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{
				"error": "Invalid claims type",
			})
			return
		}
		// Get existing sessions count
		sessions, err := store.GetUserSessions(c.Request.Context(), userClaims.UserID)
		if err != nil {
			c.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{
				"error": "Failed to check existing sessions",
			})
			return
		}
		// Check session limit
		if len(sessions) >= cfg.MaxSessionsPerUser {
			// Delete oldest session
			oldestSession := sessions[0]
			for _, s := range sessions[1:] {
				if s.CreatedAt.Before(oldestSession.CreatedAt) {
					oldestSession = s
				}
			}
			if err := store.Delete(c.Request.Context(), oldestSession.ID); err != nil {
				c.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{
					"error": "Failed to manage session limit",
				})
				return
			}
		}
		now := time.Now()
		session := &domain.Session{
			ID:          uuid.New().String(),
			UserID:      userClaims.UserID,
			Role:        userClaims.Role,
			Permissions: userClaims.Permissions,
			UserAgent:   c.Request.UserAgent(),
			IP:          c.ClientIP(),
			ExpiresAt:   now.Add(cfg.SessionDuration),
			CreatedAt:   now,
			LastSeenAt:  now,
		}
		if err := store.Create(c.Request.Context(), session); err != nil {
			c.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{
				"error": "Failed to create session",
			})
			return
		}
		// Set session cookie with config values
		c.SetCookie(
			cfg.CookieName,
			session.ID,
			int(cfg.SessionDuration.Seconds()),
			cfg.CookiePath,
			cfg.CookieDomain,
			cfg.CookieSecure,
			cfg.CookieHTTPOnly,
		)
		// Set session in context
		c.Set("session", session)
		c.Set("session_id", session.ID)
		c.Set("user_id", session.UserID)
		c.Set("role", session.Role)
		c.Set("permissions", session.Permissions)
		c.Next()
	}
}
// responseWriter is a wrapper around http.ResponseWriter that captures the response body
type responseWriter struct {
	gin.ResponseWriter
	body *bytes.Buffer
}
// Write captures the response body and writes to the underlying ResponseWriter
func (w *responseWriter) Write(b []byte) (int, error) {
	return w.body.Write(b)
}
// WriteString captures the response body as string and writes to the underlying ResponseWriter
func (w *responseWriter) WriteString(s string) (int, error) {
	return w.body.WriteString(s)
}
// WriteHeader captures the status code and writes it to the underlying ResponseWriter
func (w *responseWriter) WriteHeader(code int) {
	w.ResponseWriter.WriteHeader(code)
}
// ClearSession middleware clears the current session
func ClearSession(store domain.SessionStore, cfg domain.SessionConfig) gin.HandlerFunc {
	return func(c *gin.Context) {
		cookie, err := c.Cookie(cfg.CookieName)
		if err != nil {
			c.Next()
			return
		}
		if err := store.Delete(c.Request.Context(), cookie); err != nil {
			c.Error(fmt.Errorf("failed to delete session: %w", err))
			// Don't abort on delete error, just log it
		}
		// Clear session cookie using config values
		c.SetCookie(
			cfg.CookieName,
			"",
			-1,
			cfg.CookiePath,
			cfg.CookieDomain,
			cfg.CookieSecure,
			cfg.CookieHTTPOnly,
		)
		c.Next()
	}
}
// RequireSession middleware ensures a valid session exists
func RequireSession() gin.HandlerFunc {
	return func(c *gin.Context) {
		session, exists := c.Get("session")
		if !exists {
			c.AbortWithStatusJSON(http.StatusUnauthorized, gin.H{
				"error": "No active session",
			})
			return
		}
		userSession, ok := session.(*domain.Session)
		if !ok {
			c.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{
				"error": "Invalid session type",
			})
			return
		}
		// Check if session is expired
		if userSession.ExpiresAt.Before(time.Now()) {
			c.AbortWithStatusJSON(http.StatusUnauthorized, gin.H{
				"error": "Session expired",
			})
			return
		}
		c.Next()
	}
}
</file>

<file path="internal/handler/middleware/tracing.go">
package middleware
import (
	"github.com/gin-gonic/gin"
	"go.opentelemetry.io/otel"
)
// Tracing returns a middleware that adds OpenTelemetry tracing to requests
func Tracing() gin.HandlerFunc {
	return func(c *gin.Context) {
		tracer := otel.Tracer("http")
		ctx, span := tracer.Start(c.Request.Context(), c.Request.URL.Path)
		defer span.End()
		c.Request = c.Request.WithContext(ctx)
		c.Next()
	}
}
</file>

<file path="internal/handler/audio_handler.go">
package handler
import (
	"fmt"
	"metadatatool/internal/pkg/domain"
	"path/filepath"
	"time"
	"metadatatool/internal/pkg/metrics"
	"github.com/gin-gonic/gin"
)
type AudioHandler struct {
	storage   domain.StorageService
	trackRepo domain.TrackRepository
}
// NewAudioHandler creates a new audio handler
func NewAudioHandler(storage domain.StorageService, trackRepo domain.TrackRepository) *AudioHandler {
	return &AudioHandler{
		storage:   storage,
		trackRepo: trackRepo,
	}
}
// UploadAudio handles audio file upload
// @Summary Upload audio file
// @Description Upload an audio file and store it in cloud storage
// @Tags audio
// @Accept multipart/form-data
// @Produce json
// @Param file formData file true "Audio file"
// @Success 201 {object} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /audio/upload [post]
func (h *AudioHandler) UploadAudio(c *gin.Context) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("upload"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("upload", "started").Inc()
	file, err := c.FormFile("file")
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("upload", "form_error").Inc()
		c.JSON(400, gin.H{"error": "No file provided"})
		return
	}
	// Generate storage key
	key := fmt.Sprintf("audio/%s/%s", time.Now().Format("2006/01/02"), file.Filename)
	// Open the uploaded file
	src, err := file.Open()
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("upload", "open_error").Inc()
		c.JSON(500, gin.H{"error": "Failed to open file"})
		return
	}
	defer src.Close()
	// Create storage file
	storageFile := &domain.StorageFile{
		Key:         key,
		Name:        file.Filename,
		Size:        file.Size,
		ContentType: file.Header.Get("Content-Type"),
		Content:     src,
	}
	// Upload to storage
	if err := h.storage.Upload(c, storageFile); err != nil {
		metrics.AudioOpErrors.WithLabelValues("upload", "storage_error").Inc()
		c.JSON(500, gin.H{"error": "Failed to upload file"})
		return
	}
	// Create track record
	track := &domain.Track{
		StoragePath: key,
		FileSize:    file.Size,
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
		Metadata: domain.CompleteTrackMetadata{
			BasicTrackMetadata: domain.BasicTrackMetadata{
				Title:     filepath.Base(file.Filename),
				CreatedAt: time.Now(),
				UpdatedAt: time.Now(),
			},
			Technical: domain.AudioTechnicalMetadata{
				Format:   domain.AudioFormat(filepath.Ext(file.Filename)[1:]),
				FileSize: file.Size,
			},
		},
	}
	if err := h.trackRepo.Create(c, track); err != nil {
		metrics.AudioOpErrors.WithLabelValues("upload", "db_error").Inc()
		c.JSON(500, gin.H{"error": "Failed to create track record"})
		return
	}
	metrics.AudioOps.WithLabelValues("upload", "completed").Inc()
	c.JSON(200, gin.H{
		"id":  track.ID,
		"url": key,
	})
}
// GetAudioURL generates a pre-signed URL for audio download
// @Summary Get audio download URL
// @Description Get a pre-signed URL for downloading an audio file
// @Tags audio
// @Produce json
// @Param id path string true "Track ID"
// @Success 200 {object} map[string]string
// @Failure 404 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /audio/{id} [get]
func (h *AudioHandler) GetAudioURL(c *gin.Context) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("get_url"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("get_url", "started").Inc()
	id := c.Param("id")
	track, err := h.trackRepo.GetByID(c, id)
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("get_url", "not_found").Inc()
		c.JSON(404, gin.H{"error": "Track not found"})
		return
	}
	url, err := h.storage.GetURL(c, track.StoragePath)
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("get_url", "storage_error").Inc()
		c.JSON(500, gin.H{"error": "Failed to generate URL"})
		return
	}
	metrics.AudioOps.WithLabelValues("get_url", "completed").Inc()
	c.JSON(200, gin.H{"url": url})
}
// Helper functions moved to audio_utils.go
</file>

<file path="internal/handler/auth_handler_test.go">
package handler
import (
	"bytes"
	"context"
	"encoding/json"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"
	"metadatatool/internal/handler/middleware"
	"metadatatool/internal/usecase"
	"github.com/gin-gonic/gin"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
	"github.com/stretchr/testify/require"
)
// MockUserRepository is a mock implementation of domain.UserRepository
type MockUserRepository struct {
	mock.Mock
}
func (m *MockUserRepository) Create(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	args := m.Called(ctx, id)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	args := m.Called(ctx, email)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	args := m.Called(ctx, apiKey)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) Update(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) Delete(ctx context.Context, id string) error {
	args := m.Called(ctx, id)
	return args.Error(0)
}
func (m *MockUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	args := m.Called(ctx, offset, limit)
	return args.Get(0).([]*domain.User), args.Error(1)
}
func (m *MockUserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	args := m.Called(ctx, userID, apiKey)
	return args.Error(0)
}
// MockAuthService is a mock implementation of domain.AuthService
type MockAuthService struct {
	mock.Mock
}
func (m *MockAuthService) GenerateTokens(user *domain.User) (*domain.TokenPair, error) {
	args := m.Called(user)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.TokenPair), args.Error(1)
}
func (m *MockAuthService) ValidateToken(token string) (*domain.Claims, error) {
	args := m.Called(token)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.Claims), args.Error(1)
}
func (m *MockAuthService) RefreshToken(refreshToken string) (*domain.TokenPair, error) {
	args := m.Called(refreshToken)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.TokenPair), args.Error(1)
}
func (m *MockAuthService) HashPassword(password string) (string, error) {
	args := m.Called(password)
	return args.String(0), args.Error(1)
}
func (m *MockAuthService) VerifyPassword(hash, password string) error {
	args := m.Called(hash, password)
	return args.Error(0)
}
func (m *MockAuthService) HasPermission(role domain.Role, permission domain.Permission) bool {
	args := m.Called(role, permission)
	return args.Bool(0)
}
func (m *MockAuthService) GetPermissions(role domain.Role) []domain.Permission {
	args := m.Called(role)
	return args.Get(0).([]domain.Permission)
}
func (m *MockAuthService) GenerateAPIKey() (string, error) {
	args := m.Called()
	return args.String(0), args.Error(1)
}
// MockSessionStore is a mock implementation of domain.SessionStore
type MockSessionStore struct {
	mock.Mock
}
func (m *MockSessionStore) Create(ctx context.Context, session *domain.Session) error {
	args := m.Called(ctx, session)
	return args.Error(0)
}
func (m *MockSessionStore) Get(ctx context.Context, sessionID string) (*domain.Session, error) {
	args := m.Called(ctx, sessionID)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.Session), args.Error(1)
}
func (m *MockSessionStore) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	args := m.Called(ctx, userID)
	return args.Get(0).([]*domain.Session), args.Error(1)
}
func (m *MockSessionStore) Update(ctx context.Context, session *domain.Session) error {
	args := m.Called(ctx, session)
	return args.Error(0)
}
func (m *MockSessionStore) Delete(ctx context.Context, sessionID string) error {
	args := m.Called(ctx, sessionID)
	return args.Error(0)
}
func (m *MockSessionStore) DeleteUserSessions(ctx context.Context, userID string) error {
	args := m.Called(ctx, userID)
	return args.Error(0)
}
func (m *MockSessionStore) DeleteExpired(ctx context.Context) error {
	args := m.Called(ctx)
	return args.Error(0)
}
func (m *MockSessionStore) Touch(ctx context.Context, sessionID string) error {
	args := m.Called(ctx, sessionID)
	return args.Error(0)
}
func setupAuthHandler() (*AuthHandler, *MockAuthService, *MockUserRepository, *MockSessionStore, *gin.Engine) {
	gin.SetMode(gin.TestMode)
	authService := new(MockAuthService)
	userRepo := new(MockUserRepository)
	sessionStore := new(MockSessionStore)
	// Initialize usecases
	authUseCase := usecase.NewAuthUseCase(userRepo, sessionStore, authService)
	userUseCase := usecase.NewUserUseCase(userRepo)
	// Initialize handler
	handler := NewAuthHandler(authUseCase, userUseCase, sessionStore)
	// Setup session config for tests
	sessionConfig := domain.SessionConfig{
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 5,
	}
	router := gin.New()
	router.Use(gin.Recovery())
	// Auth routes
	auth := router.Group("/auth")
	{
		// Public routes
		auth.POST("/register", handler.Register)
		auth.POST("/login", middleware.CreateSession(sessionStore, sessionConfig), handler.Login)
		auth.POST("/refresh", handler.RefreshToken)
		// Protected routes
		protected := auth.Group("")
		protected.Use(middleware.Session(sessionStore, sessionConfig))
		protected.Use(middleware.RequireSession())
		{
			protected.POST("/logout", middleware.ClearSession(sessionStore, domain.SessionConfig{
				CookieName:     "session_id",
				CookiePath:     "/",
				CookieDomain:   "localhost",
				CookieSecure:   true,
				CookieHTTPOnly: true,
			}), handler.Logout)
			protected.POST("/api-key", handler.GenerateAPIKey)
			protected.GET("/sessions", handler.GetActiveSessions)
			protected.DELETE("/sessions/:session_id", handler.RevokeSession)
			protected.DELETE("/sessions", handler.RevokeAllSessions)
		}
	}
	return handler, authService, userRepo, sessionStore, router
}
func TestAuthHandler_Register(t *testing.T) {
	_, authService, userRepo, _, router := setupAuthHandler()
	t.Run("successful registration", func(t *testing.T) {
		reqBody := map[string]interface{}{
			"email":    "test@example.com",
			"password": "password123",
			"name":     "Test User",
		}
		body, _ := json.Marshal(reqBody)
		hashedPassword := "hashed-password"
		authService.On("HashPassword", "password123").Return(hashedPassword, nil)
		userRepo.On("GetByEmail", mock.Anything, "test@example.com").Return(nil, nil)
		userRepo.On("Create", mock.Anything, mock.MatchedBy(func(u *domain.User) bool {
			return u.Email == "test@example.com" && u.Password == hashedPassword
		})).Return(nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("POST", "/auth/register", bytes.NewBuffer(body))
		req.Header.Set("Content-Type", "application/json")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusCreated, w.Code)
		authService.AssertExpectations(t)
		userRepo.AssertExpectations(t)
	})
	t.Run("email already exists", func(t *testing.T) {
		reqBody := map[string]interface{}{
			"email":    "existing@example.com",
			"password": "password123",
			"name":     "Test User",
		}
		body, _ := json.Marshal(reqBody)
		existingUser := &domain.User{
			Email: "existing@example.com",
		}
		userRepo.On("GetByEmail", mock.Anything, "existing@example.com").Return(existingUser, nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("POST", "/auth/register", bytes.NewBuffer(body))
		req.Header.Set("Content-Type", "application/json")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusConflict, w.Code)
		userRepo.AssertExpectations(t)
	})
	t.Run("invalid request body", func(t *testing.T) {
		reqBody := map[string]interface{}{
			"email": "invalid-email",
		}
		body, _ := json.Marshal(reqBody)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("POST", "/auth/register", bytes.NewBuffer(body))
		req.Header.Set("Content-Type", "application/json")
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusBadRequest, w.Code)
	})
}
func TestAuthHandler_Login(t *testing.T) {
	_, authService, userRepo, sessionStore, router := setupAuthHandler()
	tests := []struct {
		name           string
		body           map[string]string
		setupMocks     func()
		expectedStatus int
		expectSession  bool
	}{
		{
			name: "valid_credentials",
			body: map[string]string{
				"email":    "test@example.com",
				"password": "password123",
			},
			setupMocks: func() {
				user := &domain.User{
					ID:       "test-user",
					Email:    "test@example.com",
					Password: "hashed_password",
					Role:     "user",
				}
				userRepo.On("GetByEmail", mock.Anything, "test@example.com").Return(user, nil)
				authService.On("VerifyPassword", "hashed_password", "password123").Return(nil)
				authService.On("GenerateTokens", mock.MatchedBy(func(u *domain.User) bool {
					return u.ID == "test-user" && u.Email == "test@example.com"
				})).Return(&domain.TokenPair{
					AccessToken:  "test-access-token",
					RefreshToken: "test-refresh-token",
				}, nil)
				sessionStore.On("Create", mock.Anything, mock.AnythingOfType("*domain.Session")).Return(nil)
			},
			expectedStatus: http.StatusOK,
			expectSession:  true,
		},
		{
			name: "invalid_credentials",
			body: map[string]string{
				"email":    "test@example.com",
				"password": "wrongpass",
			},
			setupMocks: func() {
				user := &domain.User{
					ID:       "test-user",
					Email:    "test@example.com",
					Password: "hashed_password",
				}
				userRepo.On("GetByEmail", mock.Anything, "test@example.com").Return(user, nil)
				authService.On("VerifyPassword", "hashed_password", "wrongpass").Return(domain.ErrInvalidCredentials)
			},
			expectedStatus: http.StatusUnauthorized,
			expectSession:  false,
		},
		{
			name: "user_not_found",
			body: map[string]string{
				"email":    "nonexistent@example.com",
				"password": "password123",
			},
			setupMocks: func() {
				userRepo.On("GetByEmail", mock.Anything, "nonexistent@example.com").Return(nil, domain.ErrUserNotFound)
			},
			expectedStatus: http.StatusUnauthorized,
			expectSession:  false,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Reset mocks
			authService.ExpectedCalls = nil
			userRepo.ExpectedCalls = nil
			sessionStore.ExpectedCalls = nil
			// Setup mocks
			tt.setupMocks()
			// Create request
			body, _ := json.Marshal(tt.body)
			w := httptest.NewRecorder()
			req, _ := http.NewRequest("POST", "/auth/login", bytes.NewBuffer(body))
			req.Header.Set("Content-Type", "application/json")
			// Perform request
			router.ServeHTTP(w, req)
			// Assert response
			assert.Equal(t, tt.expectedStatus, w.Code)
			// Check session cookie
			if tt.expectSession {
				cookie := w.Header().Get("Set-Cookie")
				assert.Contains(t, cookie, "session_id")
				assert.Contains(t, cookie, "HttpOnly")
				assert.Contains(t, cookie, "Secure")
			} else {
				assert.Empty(t, w.Header().Get("Set-Cookie"))
			}
			// Verify all mocks
			authService.AssertExpectations(t)
			userRepo.AssertExpectations(t)
			sessionStore.AssertExpectations(t)
		})
	}
}
func TestAuthHandler_RefreshToken(t *testing.T) {
	_, authService, userRepo, sessionStore, router := setupAuthHandler()
	tests := []struct {
		name           string
		body           map[string]string
		setupMocks     func()
		expectedStatus int
		expectSession  bool
	}{
		{
			name: "valid_refresh_token",
			body: map[string]string{
				"refresh_token": "valid-refresh-token",
			},
			setupMocks: func() {
				claims := &domain.Claims{
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionReadTrack},
				}
				authService.On("ValidateToken", "valid-refresh-token").Return(claims, nil)
				user := &domain.User{
					ID:          "test-user",
					Email:       "test@example.com",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionReadTrack},
				}
				userRepo.On("GetByID", mock.Anything, "test-user").Return(user, nil)
				authService.On("GenerateTokens", mock.MatchedBy(func(u *domain.User) bool {
					return u.ID == "test-user" && u.Email == "test@example.com"
				})).Return(&domain.TokenPair{
					AccessToken:  "new-access-token",
					RefreshToken: "new-refresh-token",
				}, nil)
				sessionStore.On("Create", mock.Anything, mock.AnythingOfType("*domain.Session")).Return(nil)
			},
			expectedStatus: http.StatusOK,
			expectSession:  true,
		},
		{
			name: "invalid_refresh_token",
			body: map[string]string{
				"refresh_token": "invalid-token",
			},
			setupMocks: func() {
				authService.On("ValidateToken", "invalid-token").Return(nil, domain.ErrInvalidToken)
			},
			expectedStatus: http.StatusUnauthorized,
			expectSession:  false,
		},
		{
			name: "user_not_found",
			body: map[string]string{
				"refresh_token": "valid-token-deleted-user",
			},
			setupMocks: func() {
				claims := &domain.Claims{
					UserID: "deleted-user",
					Role:   domain.RoleUser,
				}
				authService.On("ValidateToken", "valid-token-deleted-user").Return(claims, nil)
				userRepo.On("GetByID", mock.Anything, "deleted-user").Return(nil, domain.ErrUserNotFound)
			},
			expectedStatus: http.StatusUnauthorized,
			expectSession:  false,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Reset mocks
			authService.ExpectedCalls = nil
			userRepo.ExpectedCalls = nil
			sessionStore.ExpectedCalls = nil
			// Setup mocks
			tt.setupMocks()
			// Create request
			body, _ := json.Marshal(tt.body)
			w := httptest.NewRecorder()
			req, _ := http.NewRequest("POST", "/auth/refresh", bytes.NewBuffer(body))
			req.Header.Set("Content-Type", "application/json")
			// Perform request
			router.ServeHTTP(w, req)
			// Assert response
			assert.Equal(t, tt.expectedStatus, w.Code)
			// Check session cookie
			if tt.expectSession {
				cookie := w.Header().Get("Set-Cookie")
				assert.Contains(t, cookie, "session_id")
				assert.Contains(t, cookie, "HttpOnly")
				assert.Contains(t, cookie, "Secure")
			} else {
				assert.Empty(t, w.Header().Get("Set-Cookie"))
			}
			// Verify all mocks
			authService.AssertExpectations(t)
			userRepo.AssertExpectations(t)
			sessionStore.AssertExpectations(t)
		})
	}
}
func TestAuthHandler_GenerateAPIKey(t *testing.T) {
	_, authService, userRepo, sessionStore, router := setupAuthHandler()
	// Add session middleware to router
	router.Use(middleware.Session(sessionStore, domain.SessionConfig{
		CookieName:     "session_id",
		CookiePath:     "/",
		CookieDomain:   "localhost",
		CookieSecure:   true,
		CookieHTTPOnly: true,
	}))
	tests := []struct {
		name           string
		setupMocks     func()
		setupSession   func() *domain.Session
		expectedStatus int
	}{
		{
			name: "successful_api_key_generation",
			setupSession: func() *domain.Session {
				return &domain.Session{
					ID:          "test-session",
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionManageAPIKeys},
					ExpiresAt:   time.Now().Add(24 * time.Hour),
					CreatedAt:   time.Now(),
					LastSeenAt:  time.Now(),
				}
			},
			setupMocks: func() {
				user := &domain.User{
					ID:    "test-user",
					Email: "test@example.com",
					Role:  domain.RoleUser,
				}
				userRepo.On("GetByID", mock.Anything, "test-user").Return(user, nil)
				authService.On("GenerateAPIKey").Return("test-api-key", nil)
				userRepo.On("UpdateAPIKey", mock.Anything, "test-user", "test-api-key").Return(nil)
			},
			expectedStatus: http.StatusOK,
		},
		{
			name: "missing_permission",
			setupSession: func() *domain.Session {
				return &domain.Session{
					ID:          "test-session",
					UserID:      "test-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{},
					ExpiresAt:   time.Now().Add(24 * time.Hour),
					CreatedAt:   time.Now(),
					LastSeenAt:  time.Now(),
				}
			},
			setupMocks:     func() {},
			expectedStatus: http.StatusForbidden,
		},
		{
			name: "user_not_found",
			setupSession: func() *domain.Session {
				return &domain.Session{
					ID:          "test-session",
					UserID:      "deleted-user",
					Role:        domain.RoleUser,
					Permissions: []domain.Permission{domain.PermissionManageAPIKeys},
					ExpiresAt:   time.Now().Add(24 * time.Hour),
					CreatedAt:   time.Now(),
					LastSeenAt:  time.Now(),
				}
			},
			setupMocks: func() {
				userRepo.On("GetByID", mock.Anything, "deleted-user").Return(nil, domain.ErrUserNotFound)
			},
			expectedStatus: http.StatusNotFound,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Reset mocks
			authService.ExpectedCalls = nil
			userRepo.ExpectedCalls = nil
			sessionStore.ExpectedCalls = nil
			// Setup mocks
			tt.setupMocks()
			// Create request
			w := httptest.NewRecorder()
			req, _ := http.NewRequest("POST", "/auth/api-key", nil)
			req.Header.Set("Content-Type", "application/json")
			// Set up session cookie and mock
			session := tt.setupSession()
			cookie := &http.Cookie{
				Name:     "session_id",
				Value:    session.ID,
				Path:     "/",
				Domain:   "localhost",
				Secure:   true,
				HttpOnly: true,
			}
			req.AddCookie(cookie)
			sessionStore.On("Get", mock.Anything, session.ID).Return(session, nil)
			sessionStore.On("Touch", mock.Anything, session.ID).Return(nil)
			// Perform request
			router.ServeHTTP(w, req)
			// Assert response
			assert.Equal(t, tt.expectedStatus, w.Code)
			if tt.expectedStatus == http.StatusOK {
				var response map[string]string
				err := json.NewDecoder(w.Body).Decode(&response)
				assert.NoError(t, err)
				assert.Contains(t, response, "api_key")
				assert.NotEmpty(t, response["api_key"])
			}
			// Verify all mocks
			authService.AssertExpectations(t)
			userRepo.AssertExpectations(t)
			sessionStore.AssertExpectations(t)
		})
	}
}
func TestAuthHandler_GetActiveSessions(t *testing.T) {
	_, authService, _, sessionStore, router := setupAuthHandler()
	t.Run("get active sessions", func(t *testing.T) {
		// Reset mocks
		authService.ExpectedCalls = nil
		sessionStore.ExpectedCalls = nil
		user := &domain.User{
			ID:    "test-user",
			Email: "test@example.com",
			Role:  domain.RoleUser,
		}
		sessions := []*domain.Session{
			{
				ID:          "session-1",
				UserID:      user.ID,
				Role:        user.Role,
				Permissions: domain.RolePermissions[user.Role],
				ExpiresAt:   time.Now().Add(24 * time.Hour),
				CreatedAt:   time.Now(),
				LastSeenAt:  time.Now(),
			},
			{
				ID:          "session-2",
				UserID:      user.ID,
				Role:        user.Role,
				Permissions: domain.RolePermissions[user.Role],
				ExpiresAt:   time.Now().Add(24 * time.Hour),
				CreatedAt:   time.Now(),
				LastSeenAt:  time.Now(),
			},
		}
		// Set up session cookie and mock
		currentSession := &domain.Session{
			ID:          "current-session",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		cookie := &http.Cookie{
			Name:     "session_id",
			Value:    currentSession.ID,
			Path:     "/",
			Domain:   "localhost",
			Secure:   true,
			HttpOnly: true,
		}
		sessionStore.On("Get", mock.Anything, currentSession.ID).Return(currentSession, nil)
		sessionStore.On("Touch", mock.Anything, currentSession.ID).Return(nil)
		sessionStore.On("GetUserSessions", mock.Anything, user.ID).Return(sessions, nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("GET", "/auth/sessions", nil)
		req.Header.Set("Content-Type", "application/json")
		req.AddCookie(cookie)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		var response map[string]interface{}
		err := json.NewDecoder(w.Body).Decode(&response)
		require.NoError(t, err)
		data, ok := response["data"].([]interface{})
		require.True(t, ok)
		assert.Len(t, data, 2)
		authService.AssertExpectations(t)
		sessionStore.AssertExpectations(t)
	})
	t.Run("unauthorized", func(t *testing.T) {
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("GET", "/auth/sessions", nil)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusUnauthorized, w.Code)
	})
}
func TestAuthHandler_RevokeSession(t *testing.T) {
	_, authService, _, sessionStore, router := setupAuthHandler()
	t.Run("revoke own session", func(t *testing.T) {
		// Reset mocks
		authService.ExpectedCalls = nil
		sessionStore.ExpectedCalls = nil
		user := &domain.User{
			ID:    "test-user",
			Email: "test@example.com",
			Role:  domain.RoleUser,
		}
		// Set up current session
		currentSession := &domain.Session{
			ID:          "current-session",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up target session to revoke
		targetSession := &domain.Session{
			ID:          "session-1",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up session cookie and mock
		cookie := &http.Cookie{
			Name:     "session_id",
			Value:    currentSession.ID,
			Path:     "/",
			Domain:   "localhost",
			Secure:   true,
			HttpOnly: true,
		}
		sessionStore.On("Get", mock.Anything, currentSession.ID).Return(currentSession, nil)
		sessionStore.On("Touch", mock.Anything, currentSession.ID).Return(nil)
		sessionStore.On("Get", mock.Anything, "session-1").Return(targetSession, nil)
		sessionStore.On("Delete", mock.Anything, "session-1").Return(nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("DELETE", "/auth/sessions/session-1", nil)
		req.Header.Set("Content-Type", "application/json")
		req.AddCookie(cookie)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		sessionStore.AssertExpectations(t)
	})
	t.Run("revoke another user's session", func(t *testing.T) {
		// Reset mocks
		authService.ExpectedCalls = nil
		sessionStore.ExpectedCalls = nil
		user := &domain.User{
			ID:    "test-user",
			Email: "test@example.com",
			Role:  domain.RoleUser,
		}
		// Set up current session
		currentSession := &domain.Session{
			ID:          "current-session",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up target session to revoke
		targetSession := &domain.Session{
			ID:          "session-1",
			UserID:      "other-user",
			Role:        domain.RoleUser,
			Permissions: domain.RolePermissions[domain.RoleUser],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up session cookie and mock
		cookie := &http.Cookie{
			Name:     "session_id",
			Value:    currentSession.ID,
			Path:     "/",
			Domain:   "localhost",
			Secure:   true,
			HttpOnly: true,
		}
		sessionStore.On("Get", mock.Anything, currentSession.ID).Return(currentSession, nil)
		sessionStore.On("Touch", mock.Anything, currentSession.ID).Return(nil)
		sessionStore.On("Get", mock.Anything, "session-1").Return(targetSession, nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("DELETE", "/auth/sessions/session-1", nil)
		req.Header.Set("Content-Type", "application/json")
		req.AddCookie(cookie)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusForbidden, w.Code)
		sessionStore.AssertExpectations(t)
	})
	t.Run("session not found", func(t *testing.T) {
		// Reset mocks
		authService.ExpectedCalls = nil
		sessionStore.ExpectedCalls = nil
		user := &domain.User{
			ID:    "test-user",
			Email: "test@example.com",
			Role:  domain.RoleUser,
		}
		// Set up current session
		currentSession := &domain.Session{
			ID:          "current-session",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up session cookie and mock
		cookie := &http.Cookie{
			Name:     "session_id",
			Value:    currentSession.ID,
			Path:     "/",
			Domain:   "localhost",
			Secure:   true,
			HttpOnly: true,
		}
		sessionStore.On("Get", mock.Anything, currentSession.ID).Return(currentSession, nil)
		sessionStore.On("Touch", mock.Anything, currentSession.ID).Return(nil)
		sessionStore.On("Get", mock.Anything, "session-1").Return(nil, domain.ErrSessionNotFound)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("DELETE", "/auth/sessions/session-1", nil)
		req.Header.Set("Content-Type", "application/json")
		req.AddCookie(cookie)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusNotFound, w.Code)
		sessionStore.AssertExpectations(t)
	})
}
func TestAuthHandler_RevokeAllSessions(t *testing.T) {
	_, authService, _, sessionStore, router := setupAuthHandler()
	t.Run("revoke all sessions", func(t *testing.T) {
		// Reset mocks
		authService.ExpectedCalls = nil
		sessionStore.ExpectedCalls = nil
		user := &domain.User{
			ID:    "test-user",
			Email: "test@example.com",
			Role:  domain.RoleUser,
		}
		// Set up current session
		currentSession := &domain.Session{
			ID:          "current-session",
			UserID:      user.ID,
			Role:        user.Role,
			Permissions: domain.RolePermissions[user.Role],
			ExpiresAt:   time.Now().Add(24 * time.Hour),
			CreatedAt:   time.Now(),
			LastSeenAt:  time.Now(),
		}
		// Set up session cookie and mock
		cookie := &http.Cookie{
			Name:     "session_id",
			Value:    currentSession.ID,
			Path:     "/",
			Domain:   "localhost",
			Secure:   true,
			HttpOnly: true,
		}
		sessionStore.On("Get", mock.Anything, currentSession.ID).Return(currentSession, nil)
		sessionStore.On("Touch", mock.Anything, currentSession.ID).Return(nil)
		sessionStore.On("DeleteUserSessions", mock.Anything, user.ID).Return(nil)
		w := httptest.NewRecorder()
		req, _ := http.NewRequest("DELETE", "/auth/sessions", nil)
		req.Header.Set("Content-Type", "application/json")
		req.AddCookie(cookie)
		router.ServeHTTP(w, req)
		assert.Equal(t, http.StatusOK, w.Code)
		sessionStore.AssertExpectations(t)
	})
}
</file>

<file path="internal/handler/auth_handler.go">
package handler
import (
	"errors"
	"net/http"
	"strings"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/usecase"
)
// AuthHandler handles HTTP requests related to authentication
type AuthHandler struct {
	authUseCase  *usecase.AuthUseCase
	userUseCase  *usecase.UserUseCase
	sessionStore domain.SessionStore
}
// NewAuthHandler creates a new auth handler
func NewAuthHandler(authUseCase *usecase.AuthUseCase, userUseCase *usecase.UserUseCase, sessionStore domain.SessionStore) *AuthHandler {
	return &AuthHandler{
		authUseCase:  authUseCase,
		userUseCase:  userUseCase,
		sessionStore: sessionStore,
	}
}
// Register handles user registration
func (h *AuthHandler) Register(c *gin.Context) {
	var input usecase.RegisterInput
	if err := c.ShouldBindJSON(&input); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request body"})
		return
	}
	// Validate input
	if input.Email == "" || input.Password == "" {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Email and password are required"})
		return
	}
	user, err := h.authUseCase.Register(c.Request.Context(), input)
	if err != nil {
		if strings.Contains(err.Error(), "already registered") {
			c.JSON(http.StatusConflict, gin.H{"error": err.Error()})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Error registering user"})
		return
	}
	c.JSON(http.StatusCreated, gin.H{"data": user})
}
// Login handles user login and returns JWT tokens
func (h *AuthHandler) Login(c *gin.Context) {
	var input struct {
		Email    string `json:"email" binding:"required,email"`
		Password string `json:"password" binding:"required"`
	}
	if err := c.ShouldBindJSON(&input); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid input"})
		return
	}
	loginOutput, err := h.authUseCase.Login(c.Request.Context(), usecase.LoginInput{
		Email:    input.Email,
		Password: input.Password,
	})
	if err != nil {
		if errors.Is(err, domain.ErrInvalidCredentials) {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "invalid credentials"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "internal error"})
		return
	}
	// Create session for authenticated user
	session := &domain.Session{
		ID:          uuid.New().String(),
		UserID:      loginOutput.User.ID,
		Role:        loginOutput.User.Role,
		Permissions: loginOutput.User.Permissions,
		UserAgent:   c.Request.UserAgent(),
		IP:          c.ClientIP(),
		ExpiresAt:   time.Now().Add(24 * time.Hour),
		CreatedAt:   time.Now(),
		LastSeenAt:  time.Now(),
	}
	if err := h.sessionStore.Create(c.Request.Context(), session); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "failed to create session"})
		return
	}
	c.SetCookie("session_id", session.ID, int(24*time.Hour.Seconds()), "/", "", true, true)
	c.JSON(http.StatusOK, gin.H{
		"access_token":  loginOutput.AccessToken,
		"refresh_token": loginOutput.RefreshToken,
		"user": gin.H{
			"id":          loginOutput.User.ID,
			"email":       loginOutput.User.Email,
			"role":        loginOutput.User.Role,
			"permissions": loginOutput.User.Permissions,
		},
	})
}
// Logout handles user logout
func (h *AuthHandler) Logout(c *gin.Context) {
	sessionID := c.GetString("session_id")
	if err := h.authUseCase.Logout(c.Request.Context(), sessionID); err != nil {
		c.Error(err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Error logging out: " + err.Error()})
		return
	}
	c.JSON(http.StatusOK, gin.H{"message": "Logged out successfully"})
}
// GetCurrentUser returns the current authenticated user
func (h *AuthHandler) GetCurrentUser(c *gin.Context) {
	userID := c.GetString("user_id")
	user, err := h.userUseCase.GetUser(c.Request.Context(), userID)
	if err != nil {
		c.Error(err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Error getting user: " + err.Error()})
		return
	}
	c.JSON(http.StatusOK, gin.H{"data": user})
}
// RefreshToken handles token refresh requests
func (h *AuthHandler) RefreshToken(c *gin.Context) {
	var input struct {
		RefreshToken string `json:"refresh_token" binding:"required"`
	}
	if err := c.ShouldBindJSON(&input); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid input"})
		return
	}
	newAccessToken, newRefreshToken, user, err := h.authUseCase.RefreshToken(c.Request.Context(), input.RefreshToken)
	if err != nil {
		if errors.Is(err, domain.ErrInvalidToken) || errors.Is(err, domain.ErrUserNotFound) {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "invalid refresh token"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "internal error"})
		return
	}
	// Create new session
	session := &domain.Session{
		ID:          uuid.New().String(),
		UserID:      user.ID,
		Role:        user.Role,
		Permissions: user.Permissions,
		UserAgent:   c.Request.UserAgent(),
		IP:          c.ClientIP(),
		ExpiresAt:   time.Now().Add(24 * time.Hour),
		CreatedAt:   time.Now(),
		LastSeenAt:  time.Now(),
	}
	if err := h.sessionStore.Create(c.Request.Context(), session); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "failed to create session"})
		return
	}
	c.SetCookie("session_id", session.ID, int(24*time.Hour.Seconds()), "/", "", true, true)
	c.JSON(http.StatusOK, gin.H{
		"access_token":  newAccessToken,
		"refresh_token": newRefreshToken,
		"user": gin.H{
			"id":          user.ID,
			"email":       user.Email,
			"role":        user.Role,
			"permissions": user.Permissions,
		},
	})
}
// GenerateAPIKey generates a new API key for the authenticated user
func (h *AuthHandler) GenerateAPIKey(c *gin.Context) {
	session, exists := c.Get("session")
	if !exists {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "No active session"})
		return
	}
	userSession, ok := session.(*domain.Session)
	if !ok {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid session type"})
		return
	}
	// Check if user has permission to generate API key
	hasPermission := false
	for _, p := range userSession.Permissions {
		if p == domain.PermissionManageAPIKeys {
			hasPermission = true
			break
		}
	}
	if !hasPermission {
		c.JSON(http.StatusForbidden, gin.H{"error": "Insufficient permissions"})
		return
	}
	// Get user to verify existence
	user, err := h.userUseCase.GetUser(c.Request.Context(), userSession.UserID)
	if err != nil {
		if errors.Is(err, domain.ErrUserNotFound) {
			c.JSON(http.StatusNotFound, gin.H{"error": "User not found"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get user"})
		return
	}
	apiKey, err := h.authUseCase.GenerateAPIKey(c.Request.Context(), user.ID)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to generate API key"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"api_key": apiKey})
}
// GetActiveSessions returns all active sessions for the authenticated user
func (h *AuthHandler) GetActiveSessions(c *gin.Context) {
	session, exists := c.Get("session")
	if !exists {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "No active session"})
		return
	}
	userSession, ok := session.(*domain.Session)
	if !ok {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid session type"})
		return
	}
	sessions, err := h.sessionStore.GetUserSessions(c.Request.Context(), userSession.UserID)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get sessions"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"data": sessions})
}
// RevokeSession revokes a specific session
func (h *AuthHandler) RevokeSession(c *gin.Context) {
	session, exists := c.Get("session")
	if !exists {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "No active session"})
		return
	}
	userSession, ok := session.(*domain.Session)
	if !ok {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid session type"})
		return
	}
	sessionID := c.Param("session_id")
	targetSession, err := h.sessionStore.Get(c.Request.Context(), sessionID)
	if err != nil {
		if errors.Is(err, domain.ErrSessionNotFound) {
			c.JSON(http.StatusNotFound, gin.H{"error": "Session not found"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get session"})
		return
	}
	// Only allow revoking own sessions unless admin
	if targetSession.UserID != userSession.UserID && userSession.Role != domain.RoleAdmin {
		c.JSON(http.StatusForbidden, gin.H{"error": "Cannot revoke another user's session"})
		return
	}
	if err := h.sessionStore.Delete(c.Request.Context(), sessionID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to revoke session"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"message": "Session revoked successfully"})
}
// RevokeAllSessions revokes all sessions for the current user
func (h *AuthHandler) RevokeAllSessions(c *gin.Context) {
	session, exists := c.Get("session")
	if !exists {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "No active session"})
		return
	}
	userSession, ok := session.(*domain.Session)
	if !ok {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid session type"})
		return
	}
	if err := h.sessionStore.DeleteUserSessions(c.Request.Context(), userSession.UserID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to revoke sessions"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"message": "All sessions revoked successfully"})
}
// AuthMiddleware authenticates requests
func (h *AuthHandler) AuthMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		// First try to get token from Authorization header
		authHeader := c.GetHeader("Authorization")
		var token string
		if authHeader != "" {
			parts := strings.Split(authHeader, " ")
			if len(parts) == 2 && parts[0] == "Bearer" {
				token = parts[1]
			}
		}
		// If no token in header, try to get from session cookie
		if token == "" {
			sessionID, err := c.Cookie("session_id")
			if err == nil {
				token = sessionID
			}
		}
		// If still no token, return unauthorized
		if token == "" {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Missing authorization"})
			c.Abort()
			return
		}
		// Validate token
		user, err := h.authUseCase.ValidateToken(c.Request.Context(), token)
		if err != nil {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token"})
			c.Abort()
			return
		}
		// Add user and session info to context
		c.Set("user_id", user.ID)
		c.Set("user_role", user.Role)
		c.Set("session_id", token)
		c.Next()
	}
}
// RoleMiddleware checks if the user has the required role
func (h *AuthHandler) RoleMiddleware(role domain.Role) gin.HandlerFunc {
	return func(c *gin.Context) {
		userRole, exists := c.Get("user_role")
		if !exists {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Unauthorized"})
			c.Abort()
			return
		}
		if userRole.(domain.Role) != role && userRole.(domain.Role) != domain.RoleAdmin {
			c.JSON(http.StatusForbidden, gin.H{"error": "Insufficient permissions"})
			c.Abort()
			return
		}
		c.Next()
	}
}
</file>

<file path="internal/handler/ddex_handler.go">
package handler
import (
	"encoding/xml"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"github.com/gin-gonic/gin"
)
type DDEXHandler struct {
	trackRepo domain.TrackRepository
}
// NewDDEXHandler creates a new DDEX handler
func NewDDEXHandler(trackRepo domain.TrackRepository) *DDEXHandler {
	return &DDEXHandler{
		trackRepo: trackRepo,
	}
}
// ValidateERN validates a DDEX ERN file
// @Summary Validate DDEX ERN
// @Description Validate a DDEX ERN XML file
// @Tags ddex
// @Accept xml
// @Produce json
// @Param file formData file true "DDEX ERN XML file"
// @Success 200 {object} ValidationResponse
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /ddex/validate [post]
func (h *DDEXHandler) ValidateERN(c *gin.Context) {
	file, err := c.FormFile("file")
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"error": "invalid file upload",
		})
		return
	}
	// Open and read the file
	src, err := file.Open()
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{
			"error": "failed to read file",
		})
		return
	}
	defer src.Close()
	// Parse XML
	var ern domain.ERNMessage
	if err := xml.NewDecoder(src).Decode(&ern); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"error": "invalid ERN XML format",
		})
		return
	}
	// Validate ERN
	valid, errors := validateERN(&ern)
	c.JSON(http.StatusOK, ValidationResponse{
		Valid:  valid,
		Errors: errors,
	})
}
// ImportERN imports a DDEX ERN file
// @Summary Import DDEX ERN
// @Description Import tracks from a DDEX ERN XML file
// @Tags ddex
// @Accept xml
// @Produce json
// @Param file formData file true "DDEX ERN XML file"
// @Success 201 {array} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /ddex/import [post]
func (h *DDEXHandler) ImportERN(c *gin.Context) {
	file, err := c.FormFile("file")
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"error": "invalid file upload",
		})
		return
	}
	// Open and read the file
	src, err := file.Open()
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{
			"error": "failed to read file",
		})
		return
	}
	defer src.Close()
	// Parse XML
	var ern domain.ERNMessage
	if err := xml.NewDecoder(src).Decode(&ern); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"error": "invalid ERN XML format",
		})
		return
	}
	// Convert ERN to tracks
	tracks := convertERNToTracks(&ern)
	// Save tracks
	var savedTracks []*domain.Track
	for _, track := range tracks {
		if err := h.trackRepo.Create(c, track); err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{
				"error": "failed to save track",
			})
			return
		}
		savedTracks = append(savedTracks, track)
	}
	c.JSON(http.StatusCreated, savedTracks)
}
// ExportERN exports tracks as a DDEX ERN file
// @Summary Export DDEX ERN
// @Description Export tracks as a DDEX ERN XML file
// @Tags ddex
// @Produce xml
// @Success 200 {string} string "ERN XML file"
// @Failure 500 {object} ErrorResponse
// @Router /ddex/export [post]
func (h *DDEXHandler) ExportERN(c *gin.Context) {
	// Get all tracks
	tracks, err := h.trackRepo.List(c, map[string]interface{}{}, 0, 1000) // TODO: Add pagination
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{
			"error": "failed to get tracks",
		})
		return
	}
	// Convert tracks to ERN
	ern := convertTracksToERN(tracks)
	// Marshal to XML
	xmlData, err := xml.MarshalIndent(ern, "", "  ")
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{
			"error": "failed to generate XML",
		})
		return
	}
	// Set headers
	c.Header("Content-Type", "application/xml")
	c.Header("Content-Disposition", "attachment; filename=export.xml")
	c.String(http.StatusOK, xml.Header+string(xmlData))
}
type ValidationResponse struct {
	Valid  bool     `json:"valid"`
	Errors []string `json:"errors,omitempty"`
}
func validateERN(ern *domain.ERNMessage) (bool, []string) {
	var errors []string
	// Validate MessageHeader
	if ern.MessageHeader.MessageID == "" {
		errors = append(errors, "missing MessageID")
	}
	if ern.MessageHeader.MessageSender == "" {
		errors = append(errors, "missing MessageSender")
	}
	if ern.MessageHeader.MessageRecipient == "" {
		errors = append(errors, "missing MessageRecipient")
	}
	// Validate ResourceList
	for _, track := range ern.ResourceList.SoundRecordings {
		if track.ISRC == "" {
			errors = append(errors, "missing ISRC")
		}
		if track.Title.TitleText == "" {
			errors = append(errors, "missing Title")
		}
	}
	return len(errors) == 0, errors
}
func convertERNToTracks(ern *domain.ERNMessage) []*domain.Track {
	var tracks []*domain.Track
	for _, recording := range ern.ResourceList.SoundRecordings {
		track := &domain.Track{
			Metadata: domain.CompleteTrackMetadata{
				BasicTrackMetadata: domain.BasicTrackMetadata{
					Title: recording.Title.TitleText,
					ISRC:  recording.ISRC,
				},
			},
		}
		tracks = append(tracks, track)
	}
	return tracks
}
func convertTracksToERN(tracks []*domain.Track) *domain.ERNMessage {
	ern := &domain.ERNMessage{
		MessageHeader: domain.MessageHeader{
			MessageID:              "MSG001", // TODO: Generate unique ID
			MessageSender:          "YourCompany",
			MessageRecipient:       "DSP",
			MessageCreatedDateTime: "2024-04-20T12:00:00Z",
		},
	}
	// Add resources
	for _, track := range tracks {
		recording := domain.SoundRecording{
			ISRC:  track.ISRC(),
			Title: domain.Title{TitleText: track.Title()},
			// Add more fields as needed
		}
		ern.ResourceList.SoundRecordings = append(ern.ResourceList.SoundRecordings, recording)
	}
	return ern
}
</file>

<file path="internal/handler/health_handler.go">
package handler
import (
	"net/http"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/redis/go-redis/v9"
)
// HealthHandler handles health check requests
type HealthHandler struct {
	redis *redis.Client
}
// NewHealthHandler creates a new health check handler
func NewHealthHandler(redis *redis.Client) *HealthHandler {
	return &HealthHandler{
		redis: redis,
	}
}
// ServiceStatus represents the status of an individual service
type ServiceStatus struct {
	Status    string `json:"status"`
	Message   string `json:"message,omitempty"`
	Latency   int64  `json:"latency_ms"`
	Timestamp string `json:"timestamp"`
}
// HealthStatus represents the overall health check response
type HealthStatus struct {
	Status   string                   `json:"status"`
	Services map[string]ServiceStatus `json:"services"`
}
// Check performs health checks on all services
func (h *HealthHandler) Check(c *gin.Context) {
	status := HealthStatus{
		Status:   "healthy",
		Services: make(map[string]ServiceStatus),
	}
	// Check Redis
	redisCtx := c.Request.Context()
	redisStart := time.Now()
	_, err := h.redis.Ping(redisCtx).Result()
	if err != nil {
		status.Services["redis"] = ServiceStatus{
			Status:    "error",
			Message:   "Failed to ping Redis",
			Latency:   time.Since(redisStart).Milliseconds(),
			Timestamp: time.Now().UTC().Format(time.RFC3339),
		}
		status.Status = "degraded"
	} else {
		status.Services["redis"] = ServiceStatus{
			Status:    "healthy",
			Latency:   time.Since(redisStart).Milliseconds(),
			Timestamp: time.Now().UTC().Format(time.RFC3339),
		}
	}
	// Set appropriate status code
	if status.Status == "healthy" {
		c.JSON(http.StatusOK, status)
		return
	}
	c.JSON(http.StatusServiceUnavailable, status)
}
</file>

<file path="internal/handler/metrics_handler.go">
// Package handler provides HTTP request handlers
package handler
import (
	"github.com/gin-gonic/gin"
	"github.com/prometheus/client_golang/prometheus/promhttp"
)
// MetricsHandler handles Prometheus metrics requests
type MetricsHandler struct{}
// NewMetricsHandler creates a new metrics handler
func NewMetricsHandler() *MetricsHandler {
	return &MetricsHandler{}
}
// PrometheusHandler exposes Prometheus metrics
func (h *MetricsHandler) PrometheusHandler() gin.HandlerFunc {
	handler := promhttp.Handler()
	return func(c *gin.Context) {
		// Disable Gin's default recovery for this endpoint
		// as Prometheus handler has its own recovery
		defer func() {
			if err := recover(); err != nil {
				c.AbortWithStatus(500)
			}
		}()
		handler.ServeHTTP(c.Writer, c.Request)
	}
}
// HealthCheck provides a basic health check endpoint
func (h *MetricsHandler) HealthCheck(c *gin.Context) {
	c.JSON(200, gin.H{
		"status": "healthy",
	})
}
</file>

<file path="internal/handler/track_handler.go">
package handler
import (
	"fmt"
	"metadatatool/internal/pkg/domain"
	apperrors "metadatatool/internal/pkg/errors"
	"metadatatool/internal/pkg/errortracking"
	"metadatatool/internal/pkg/metrics"
	"metadatatool/internal/pkg/utils"
	"net/http"
	"path/filepath"
	"strconv"
	"strings"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
)
// TrackHandler handles HTTP requests for track operations
type TrackHandler struct {
	trackRepo      domain.TrackRepository
	aiService      domain.AIService
	storageService domain.StorageService
	validator      domain.Validator
	errorTracker   *errortracking.ErrorTracker
}
// NewTrackHandler creates a new track handler
func NewTrackHandler(
	trackRepo domain.TrackRepository,
	aiService domain.AIService,
	storageService domain.StorageService,
	validator domain.Validator,
	errorTracker *errortracking.ErrorTracker,
) *TrackHandler {
	return &TrackHandler{
		trackRepo:      trackRepo,
		aiService:      aiService,
		storageService: storageService,
		validator:      validator,
		errorTracker:   errorTracker,
	}
}
// UploadTrack handles track file upload and metadata creation
// @Summary Upload new track
// @Description Upload an audio file and create track metadata
// @Tags tracks
// @Accept multipart/form-data
// @Produce json
// @Param file formData file true "Audio file"
// @Param title formData string true "Track title"
// @Param artist formData string true "Artist name"
// @Success 201 {object} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/upload [post]
func (h *TrackHandler) UploadTrack(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("upload", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("upload").Observe(time.Since(start).Seconds())
	}()
	file, header, err := c.Request.FormFile("file")
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "no file uploaded"})
		return
	}
	defer file.Close()
	if !utils.IsValidAudioFormat(header.Filename) {
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid audio format"})
		return
	}
	trackID := uuid.New().String()
	audioFormat := utils.GetAudioFormat(header.Filename)
	storageKey := fmt.Sprintf("tracks/%s/audio%s", trackID, filepath.Ext(header.Filename))
	// Upload file to storage
	storageFile := &domain.StorageFile{
		Key:         storageKey,
		Name:        header.Filename,
		Size:        header.Size,
		ContentType: "audio/" + audioFormat,
		Content:     file,
		UploadedAt:  time.Now(),
	}
	if err := h.storageService.Upload(c.Request.Context(), storageFile); err != nil {
		h.handleError(c, apperrors.NewInternalError("failed to upload file", err))
		return
	}
	track := &domain.Track{
		ID:          trackID,
		StoragePath: storageKey,
		FileSize:    header.Size,
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
		Status:      domain.TrackStatusPending,
		Metadata: domain.CompleteTrackMetadata{
			BasicTrackMetadata: domain.BasicTrackMetadata{
				Title:     c.PostForm("title"),
				Artist:    c.PostForm("artist"),
				Album:     c.PostForm("album"),
				CreatedAt: time.Now(),
				UpdatedAt: time.Now(),
			},
			Technical: domain.AudioTechnicalMetadata{
				Format:   domain.AudioFormat(audioFormat),
				FileSize: header.Size,
			},
		},
	}
	// Validate track
	result := h.validator.Validate(track)
	if !result.IsValid {
		details := make([]string, len(result.Errors))
		for i, err := range result.Errors {
			details[i] = fmt.Sprintf("%s: %s", err.Field, err.Message)
		}
		h.handleError(c, apperrors.NewValidationError("invalid track data", strings.Join(details, "; ")))
		return
	}
	if err := h.trackRepo.Create(c, track); err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to create track", err))
		return
	}
	// Trigger async AI processing
	go func() {
		if err := h.aiService.EnrichMetadata(c, track); err != nil {
			h.errorTracker.CaptureError(err, map[string]string{
				"operation": "ai_enrich",
				"track_id":  track.ID,
			})
		}
	}()
	c.JSON(http.StatusCreated, track)
}
// CreateTrack handles track creation requests
// @Summary Create track
// @Description Create a new track with metadata
// @Tags tracks
// @Accept json
// @Produce json
// @Param track body domain.Track true "Track object"
// @Success 201 {object} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks [post]
func (h *TrackHandler) CreateTrack(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("create", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("create").Observe(time.Since(start).Seconds())
	}()
	var track domain.Track
	if err := c.ShouldBindJSON(&track); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid request body", err.Error()))
		return
	}
	// Basic validation
	if err := validateTrack(&track); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid track data", err.Error()))
		return
	}
	// Set default values
	track.ID = uuid.New().String()
	track.CreatedAt = time.Now()
	track.UpdatedAt = time.Now()
	track.Status = domain.TrackStatusPending
	// Additional validation using validator
	result := h.validator.Validate(&track)
	if !result.IsValid {
		details := make([]string, len(result.Errors))
		for i, err := range result.Errors {
			details[i] = fmt.Sprintf("%s: %s", err.Field, err.Message)
		}
		h.handleError(c, apperrors.NewValidationError("invalid track data", strings.Join(details, "; ")))
		return
	}
	if err := h.trackRepo.Create(c, &track); err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to create track", err))
		return
	}
	c.JSON(http.StatusCreated, track)
}
// GetTrack retrieves a track by ID
// @Summary Get track
// @Description Get a track by ID
// @Tags tracks
// @Produce json
// @Param id path string true "Track ID"
// @Success 200 {object} domain.Track
// @Failure 404 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/{id} [get]
func (h *TrackHandler) GetTrack(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("get", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("get").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, apperrors.NewValidationError("missing track ID", "track ID is required"))
		return
	}
	track, err := h.trackRepo.GetByID(c, id)
	if err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to get track", err))
		return
	}
	if track == nil {
		h.handleError(c, apperrors.NewNotFoundError("track not found"))
		return
	}
	c.JSON(http.StatusOK, track)
}
// UpdateTrack modifies an existing track
// @Summary Update track
// @Description Update an existing track
// @Tags tracks
// @Accept json
// @Produce json
// @Param id path string true "Track ID"
// @Param track body domain.Track true "Track object"
// @Success 200 {object} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 404 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/{id} [put]
func (h *TrackHandler) UpdateTrack(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("update", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("update").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, apperrors.NewValidationError("missing track ID", "track ID is required"))
		return
	}
	// Get existing track
	existingTrack, err := h.trackRepo.GetByID(c, id)
	if err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to get track", err))
		return
	}
	if existingTrack == nil {
		h.handleError(c, apperrors.NewNotFoundError("track not found"))
		return
	}
	// Parse update data
	var updateData domain.Track
	if err := c.ShouldBindJSON(&updateData); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid request body", err.Error()))
		return
	}
	// Basic validation
	if err := validateTrack(&updateData); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid track data", err.Error()))
		return
	}
	// Apply updates while preserving certain fields
	updateData.ID = id
	updateData.CreatedAt = existingTrack.CreatedAt
	updateData.UpdatedAt = time.Now()
	updateData.StoragePath = existingTrack.StoragePath
	updateData.FileSize = existingTrack.FileSize
	// Additional validation using validator
	result := h.validator.Validate(&updateData)
	if !result.IsValid {
		details := make([]string, len(result.Errors))
		for i, err := range result.Errors {
			details[i] = fmt.Sprintf("%s: %s", err.Field, err.Message)
		}
		h.handleError(c, apperrors.NewValidationError("invalid track data", strings.Join(details, "; ")))
		return
	}
	// Increment version
	updateData.Version = existingTrack.Version + 1
	updateData.PreviousID = existingTrack.ID
	if err := h.trackRepo.Update(c, &updateData); err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to update track", err))
		return
	}
	c.JSON(http.StatusOK, updateData)
}
// DeleteTrack removes a track
// @Summary Delete track
// @Description Delete a track by ID
// @Tags tracks
// @Produce json
// @Param id path string true "Track ID"
// @Success 204 "No Content"
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/{id} [delete]
func (h *TrackHandler) DeleteTrack(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("delete", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("delete").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, apperrors.NewValidationError("missing track ID", "track ID is required"))
		return
	}
	// Get existing track
	existingTrack, err := h.trackRepo.GetByID(c, id)
	if err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to get track", err))
		return
	}
	if existingTrack == nil {
		h.handleError(c, apperrors.NewNotFoundError("track not found"))
		return
	}
	if err := h.trackRepo.Delete(c, id); err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to delete track", err))
		return
	}
	c.Status(http.StatusNoContent)
}
// ListTracks retrieves a paginated list of tracks
// @Summary List tracks
// @Description Get a paginated list of tracks
// @Tags tracks
// @Produce json
// @Param page query int false "Page number"
// @Param limit query int false "Items per page"
// @Success 200 {object} ListResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks [get]
func (h *TrackHandler) ListTracks(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("list", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("list").Observe(time.Since(start).Seconds())
	}()
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	limit, _ := strconv.Atoi(c.DefaultQuery("limit", "10"))
	if page < 1 {
		page = 1
	}
	if limit < 1 || limit > 100 {
		limit = 10
	}
	offset := (page - 1) * limit
	tracks, err := h.trackRepo.List(c, map[string]interface{}{}, offset, limit)
	if err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to list tracks", err))
		return
	}
	c.JSON(http.StatusOK, ListResponse{
		Tracks: tracks,
		Page:   page,
		Limit:  limit,
	})
}
// SearchTracks searches tracks by metadata
// @Summary Search tracks
// @Description Search tracks by metadata fields
// @Tags tracks
// @Accept json
// @Produce json
// @Param query body SearchQuery true "Search query"
// @Success 200 {array} domain.Track
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/search [post]
func (h *TrackHandler) SearchTracks(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("search", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("search").Observe(time.Since(start).Seconds())
	}()
	var query SearchQuery
	if err := c.ShouldBindJSON(&query); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid search query", err.Error()))
		return
	}
	tracks, err := h.trackRepo.SearchByMetadata(c, query.toMap())
	if err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to search tracks", err))
		return
	}
	c.JSON(http.StatusOK, tracks)
}
// BatchProcess processes multiple tracks
func (h *TrackHandler) BatchProcess(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("batch", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("batch").Observe(time.Since(start).Seconds())
	}()
	var req BatchProcessRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid request body", err.Error()))
		return
	}
	var tracks []*domain.Track
	for _, id := range req.TrackIDs {
		track, err := h.trackRepo.GetByID(c, id)
		if err != nil {
			h.handleError(c, apperrors.NewDatabaseError("failed to get track", err))
			return
		}
		if track == nil {
			h.handleError(c, apperrors.NewNotFoundError(fmt.Sprintf("track not found: %s", id)))
			return
		}
		tracks = append(tracks, track)
	}
	// Process tracks in batch
	if err := h.aiService.BatchProcess(c, tracks); err != nil {
		h.handleError(c, apperrors.NewAIError("failed to process tracks", err))
		return
	}
	// Update tracks in database
	if err := h.trackRepo.BatchUpdate(c, tracks); err != nil {
		h.handleError(c, apperrors.NewDatabaseError("failed to update tracks", err))
		return
	}
	c.JSON(http.StatusOK, tracks)
}
// ExportTracks exports tracks in the specified format
// @Summary Export tracks
// @Description Export tracks in the specified format
// @Tags tracks
// @Accept json
// @Produce json
// @Param request body ExportRequest true "Export request"
// @Success 200 {object} ExportResponse
// @Failure 400 {object} ErrorResponse
// @Failure 500 {object} ErrorResponse
// @Router /tracks/export [post]
func (h *TrackHandler) ExportTracks(c *gin.Context) {
	var req ExportRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		h.handleError(c, apperrors.NewValidationError("invalid request body", err.Error()))
		return
	}
	// Get tracks
	var tracks []*domain.Track
	for _, id := range req.TrackIDs {
		track, err := h.trackRepo.GetByID(c, id)
		if err != nil {
			h.handleError(c, apperrors.NewDatabaseError("failed to get track", err))
			return
		}
		if track != nil {
			tracks = append(tracks, track)
		}
	}
	// Generate export data based on format
	var exportData interface{}
	switch req.Format {
	case "json":
		exportData = tracks
	case "csv":
		csvData := [][]string{
			{"ID", "Title", "Artist", "Album", "ISRC", "Duration", "Created At"},
		}
		for _, track := range tracks {
			row := []string{
				track.ID,
				track.Title(),
				track.Artist(),
				track.Album(),
				track.ISRC(),
				fmt.Sprintf("%d", int(track.Duration())),
				track.CreatedAt.Format(time.RFC3339),
			}
			csvData = append(csvData, row)
		}
		exportData = csvData
	default:
		h.handleError(c, apperrors.NewValidationError("unsupported export format", fmt.Sprintf("format '%s' is not supported", req.Format)))
		return
	}
	c.JSON(http.StatusOK, ExportResponse{
		Format: req.Format,
		Data:   exportData,
	})
}
// Helper functions and types
type ErrorResponse struct {
	Error   string `json:"error"`
	Details string `json:"details,omitempty"`
}
type ListResponse struct {
	Tracks []*domain.Track `json:"tracks"`
	Page   int             `json:"page"`
	Limit  int             `json:"limit"`
}
type SearchQuery struct {
	Title       string    `json:"title,omitempty"`
	Artist      string    `json:"artist,omitempty"`
	Album       string    `json:"album,omitempty"`
	Genre       string    `json:"genre,omitempty"`
	Label       string    `json:"label,omitempty"`
	ISRC        string    `json:"isrc,omitempty"`
	ISWC        string    `json:"iswc,omitempty"`
	CreatedFrom time.Time `json:"created_from,omitempty"`
	CreatedTo   time.Time `json:"created_to,omitempty"`
	NeedsReview *bool     `json:"needs_review,omitempty"`
}
func (q *SearchQuery) toMap() map[string]interface{} {
	m := make(map[string]interface{})
	if q.Title != "" {
		m["title"] = q.Title
	}
	if q.Artist != "" {
		m["artist"] = q.Artist
	}
	if q.Album != "" {
		m["album"] = q.Album
	}
	if q.Genre != "" {
		m["genre"] = q.Genre
	}
	if q.Label != "" {
		m["label"] = q.Label
	}
	if q.ISRC != "" {
		m["isrc"] = q.ISRC
	}
	if q.ISWC != "" {
		m["iswc"] = q.ISWC
	}
	if !q.CreatedFrom.IsZero() {
		m["created_from"] = q.CreatedFrom
	}
	if !q.CreatedTo.IsZero() {
		m["created_to"] = q.CreatedTo
	}
	if q.NeedsReview != nil {
		m["needs_review"] = *q.NeedsReview
	}
	return m
}
func (h *TrackHandler) handleError(c *gin.Context, err *apperrors.AppError) {
	if h.errorTracker != nil {
		h.errorTracker.CaptureError(err, map[string]string{
			"handler": "track",
			"method":  c.Request.Method,
			"path":    c.Request.URL.Path,
		})
	}
	c.JSON(err.StatusCode, gin.H{
		"error": gin.H{
			"type":    err.Type,
			"message": err.Message,
			"details": err.Details,
		},
	})
}
func validateTrack(track *domain.Track) error {
	if track.Title() == "" {
		return fmt.Errorf("title is required")
	}
	if track.Artist() == "" {
		return fmt.Errorf("artist is required")
	}
	if track.ISRC() != "" && len(track.ISRC()) != 12 {
		return fmt.Errorf("ISRC must be 12 characters")
	}
	if track.ISWC() != "" && len(track.ISWC()) != 11 {
		return fmt.Errorf("ISWC must be 11 characters")
	}
	return nil
}
type BatchProcessRequest struct {
	TrackIDs []string `json:"track_ids" binding:"required"`
}
type ExportRequest struct {
	TrackIDs []string `json:"track_ids" binding:"required"`
	Format   string   `json:"format" binding:"required,oneof=json csv"`
}
type ExportResponse struct {
	Format string      `json:"format"`
	Data   interface{} `json:"data"`
}
func (h *TrackHandler) UploadAudio(c *gin.Context) {
	// TODO: Implement audio upload
	c.JSON(http.StatusNotImplemented, gin.H{"error": "Not implemented"})
}
func (h *TrackHandler) GetAudioURL(c *gin.Context) {
	// TODO: Implement get audio URL
	c.JSON(http.StatusNotImplemented, gin.H{"error": "Not implemented"})
}
func (h *TrackHandler) ValidateERN(c *gin.Context) {
	// TODO: Implement ERN validation
	c.JSON(http.StatusNotImplemented, gin.H{"error": "Not implemented"})
}
func (h *TrackHandler) ImportERN(c *gin.Context) {
	// TODO: Implement ERN import
	c.JSON(http.StatusNotImplemented, gin.H{"error": "Not implemented"})
}
func (h *TrackHandler) ExportERN(c *gin.Context) {
	// TODO: Implement ERN export
	c.JSON(http.StatusNotImplemented, gin.H{"error": "Not implemented"})
}
</file>

<file path="internal/handler/user_handler.go">
package handler
import (
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/errortracking"
	"metadatatool/internal/pkg/metrics"
	"net/http"
	"strconv"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	"golang.org/x/crypto/bcrypt"
)
// UserHandler handles HTTP requests for user operations
type UserHandler struct {
	userRepo     domain.UserRepository
	errorTracker *errortracking.ErrorTracker
}
// NewUserHandler creates a new user handler
func NewUserHandler(
	userRepo domain.UserRepository,
	errorTracker *errortracking.ErrorTracker,
) *UserHandler {
	return &UserHandler{
		userRepo:     userRepo,
		errorTracker: errorTracker,
	}
}
// CreateUser handles user creation requests
func (h *UserHandler) CreateUser(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("create_user", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("create_user").Observe(time.Since(start).Seconds())
	}()
	var user domain.User
	if err := c.ShouldBindJSON(&user); err != nil {
		h.handleError(c, http.StatusBadRequest, "invalid request body", err)
		return
	}
	if err := validateUser(&user); err != nil {
		h.handleError(c, http.StatusBadRequest, "invalid user data", err)
		return
	}
	// Hash password
	hashedPassword, err := bcrypt.GenerateFromPassword([]byte(user.Password), bcrypt.DefaultCost)
	if err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to hash password", err)
		return
	}
	user.Password = string(hashedPassword)
	// Generate API key
	user.APIKey = uuid.New().String()
	// Set default values
	user.Role = domain.RoleUser
	user.Plan = domain.PlanBasic
	user.TrackQuota = 100
	user.TracksUsed = 0
	user.QuotaResetDate = time.Now().AddDate(0, 1, 0) // Reset in 1 month
	user.LastLoginAt = time.Now()
	if err := h.userRepo.Create(c, &user); err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to create user", err)
		return
	}
	// Don't return the hashed password
	user.Password = ""
	c.JSON(http.StatusCreated, user)
}
// GetUser retrieves a user by ID
func (h *UserHandler) GetUser(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("get_user", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("get_user").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, http.StatusBadRequest, "missing user ID", nil)
		return
	}
	user, err := h.userRepo.GetByID(c, id)
	if err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to get user", err)
		return
	}
	if user == nil {
		h.handleError(c, http.StatusNotFound, "user not found", nil)
		return
	}
	// Don't return the hashed password
	user.Password = ""
	c.JSON(http.StatusOK, user)
}
// UpdateUser modifies an existing user
func (h *UserHandler) UpdateUser(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("update_user", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("update_user").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, http.StatusBadRequest, "missing user ID", nil)
		return
	}
	var user domain.User
	if err := c.ShouldBindJSON(&user); err != nil {
		h.handleError(c, http.StatusBadRequest, "invalid request body", err)
		return
	}
	user.ID = id
	if err := validateUser(&user); err != nil {
		h.handleError(c, http.StatusBadRequest, "invalid user data", err)
		return
	}
	// If password is provided, hash it
	if user.Password != "" {
		hashedPassword, err := bcrypt.GenerateFromPassword([]byte(user.Password), bcrypt.DefaultCost)
		if err != nil {
			h.handleError(c, http.StatusInternalServerError, "failed to hash password", err)
			return
		}
		user.Password = string(hashedPassword)
	}
	if err := h.userRepo.Update(c, &user); err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to update user", err)
		return
	}
	// Don't return the hashed password
	user.Password = ""
	c.JSON(http.StatusOK, user)
}
// DeleteUser removes a user
func (h *UserHandler) DeleteUser(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("delete_user", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("delete_user").Observe(time.Since(start).Seconds())
	}()
	id := c.Param("id")
	if id == "" {
		h.handleError(c, http.StatusBadRequest, "missing user ID", nil)
		return
	}
	if err := h.userRepo.Delete(c, id); err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to delete user", err)
		return
	}
	c.Status(http.StatusNoContent)
}
// ListUsers retrieves a paginated list of users
func (h *UserHandler) ListUsers(c *gin.Context) {
	start := time.Now()
	defer func() {
		metrics.DatabaseOperationsTotal.WithLabelValues("list_users", "total").Inc()
		metrics.DatabaseQueryDuration.WithLabelValues("list_users").Observe(time.Since(start).Seconds())
	}()
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	limit, _ := strconv.Atoi(c.DefaultQuery("limit", "10"))
	if page < 1 {
		page = 1
	}
	if limit < 1 || limit > 100 {
		limit = 10
	}
	offset := (page - 1) * limit
	users, err := h.userRepo.List(c, offset, limit)
	if err != nil {
		h.handleError(c, http.StatusInternalServerError, "failed to list users", err)
		return
	}
	// Don't return hashed passwords
	for _, user := range users {
		user.Password = ""
	}
	c.JSON(http.StatusOK, ListUsersResponse{
		Users: users,
		Page:  page,
		Limit: limit,
	})
}
// Helper functions and types
func (h *UserHandler) handleError(c *gin.Context, status int, message string, err error) {
	if err != nil {
		h.errorTracker.CaptureError(err, map[string]string{
			"status":    strconv.Itoa(status),
			"message":   message,
			"path":      c.FullPath(),
			"method":    c.Request.Method,
			"client_ip": c.ClientIP(),
		})
	}
	metrics.DatabaseOperationsTotal.WithLabelValues(c.Request.Method, "error").Inc()
	response := ErrorResponse{
		Error: message,
	}
	if err != nil && status >= 500 {
		response.Details = err.Error()
	}
	c.JSON(status, response)
}
func validateUser(user *domain.User) error {
	if user.Email == "" {
		return fmt.Errorf("email is required")
	}
	if user.Password == "" {
		return fmt.Errorf("password is required")
	}
	if user.Name == "" {
		return fmt.Errorf("name is required")
	}
	return nil
}
type ListUsersResponse struct {
	Users []*domain.User `json:"users"`
	Page  int            `json:"page"`
	Limit int            `json:"limit"`
}
</file>

<file path="internal/pkg/analytics/bigquery.go">
// Package analytics provides analytics and metrics collection functionality.
//
// This package implements analytics services for tracking and analyzing AI
// experiment results, using Google BigQuery as the backend storage.
//
// Key features:
//   - Recording AI experiment results
//   - Calculating experiment metrics
//   - Comparing control and experiment groups
//
// Usage example:
//
//	service, err := analytics.NewBigQueryService(projectID, dataset)
//	if err != nil {
//	    log.Fatal(err)
//	}
//	defer service.Close()
//
//	record := &analytics.AIExperimentRecord{
//	    Timestamp:       time.Now(),
//	    TrackID:        "track123",
//	    ModelProvider:  "openai",
//	    ModelVersion:   "v1",
//	    ProcessingTime: 1.5,
//	    Confidence:    0.95,
//	    Success:       true,
//	}
//
//	if err := service.RecordAIExperiment(ctx, record); err != nil {
//	    log.Printf("Failed to record experiment: %v", err)
//	}
package analytics
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
	"cloud.google.com/go/bigquery"
	"google.golang.org/api/iterator"
	"google.golang.org/api/option"
)
// AIExperimentRecord represents a record of an AI experiment.
// Each record contains information about a single AI operation,
// including timing, confidence scores, and success/failure status.
type AIExperimentRecord struct {
	// Timestamp when the experiment was conducted
	Timestamp time.Time
	// TrackID is the unique identifier of the processed track
	TrackID string
	// ModelProvider identifies the AI model provider (e.g., "openai", "qwen2")
	ModelProvider string
	// ModelVersion identifies the specific version of the model used
	ModelVersion string
	// ProcessingTime is the time taken to process the track in seconds
	ProcessingTime float64
	// Confidence is the model's confidence score for the operation (0-1)
	Confidence float64
	// Success indicates whether the operation was successful
	Success bool
	// ErrorMessage contains error details if Success is false
	ErrorMessage string
	// ExperimentGroup indicates whether this was part of the control or experiment group
	ExperimentGroup string // "control" or "experiment"
}
// BigQueryService handles analytics data storage in Google BigQuery.
// It provides methods for recording experiment results and retrieving
// aggregated metrics for analysis.
type BigQueryService struct {
	client          *bigquery.Client
	projectID       string
	dataset         string
	experimentTable *bigquery.Table
}
// NewBigQueryService creates a new BigQuery service instance.
// It establishes a connection to BigQuery and initializes the required
// dataset and table references.
//
// Parameters:
//   - projectID: Google Cloud project ID
//   - dataset: BigQuery dataset name
//
// Returns:
//   - *BigQueryService: Initialized service
//   - error: Any error that occurred during initialization
func NewBigQueryService(projectID, dataset string) (*BigQueryService, error) {
	ctx := context.Background()
	client, err := bigquery.NewClient(ctx, projectID, option.WithScopes(bigquery.Scope))
	if err != nil {
		return nil, fmt.Errorf("failed to create BigQuery client: %w", err)
	}
	ds := client.Dataset(dataset)
	table := ds.Table("ai_experiments")
	return &BigQueryService{
		client:          client,
		projectID:       projectID,
		dataset:         dataset,
		experimentTable: table,
	}, nil
}
// RecordAIExperiment records an AI experiment result in BigQuery.
// This method is used to track individual AI operations for later analysis.
//
// Parameters:
//   - ctx: Context for the operation
//   - record: The experiment record to store
//
// Returns:
//   - error: Any error that occurred during the operation
func (s *BigQueryService) RecordAIExperiment(ctx context.Context, record *AIExperimentRecord) error {
	inserter := s.experimentTable.Inserter()
	return inserter.Put(ctx, record)
}
// GetExperimentMetrics retrieves aggregated metrics for the experiment.
// This method calculates various metrics for both control and experiment groups
// within the specified time range.
//
// Parameters:
//   - ctx: Context for the operation
//   - start: Start time for the analysis period
//   - end: End time for the analysis period
//
// Returns:
//   - *domain.ExperimentMetrics: Aggregated metrics for both groups
//   - error: Any error that occurred during the operation
func (s *BigQueryService) GetExperimentMetrics(ctx context.Context, start, end time.Time) (*domain.ExperimentMetrics, error) {
	query := fmt.Sprintf(`
		SELECT
			ExperimentGroup,
			COUNT(*) as TotalRequests,
			AVG(ProcessingTime) as AvgProcessingTime,
			AVG(Confidence) as AvgConfidence,
			COUNTIF(Success) / COUNT(*) as SuccessRate
		FROM %s.ai_experiments
		WHERE Timestamp BETWEEN @start AND @end
		GROUP BY ExperimentGroup
	`, s.dataset)
	q := s.client.Query(query)
	q.Parameters = []bigquery.QueryParameter{
		{Name: "start", Value: start},
		{Name: "end", Value: end},
	}
	it, err := q.Read(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to execute query: %w", err)
	}
	metrics := &domain.ExperimentMetrics{
		Control:    &domain.ModelMetrics{},
		Experiment: &domain.ModelMetrics{},
	}
	for {
		var row struct {
			ExperimentGroup   string
			TotalRequests     int64
			AvgProcessingTime float64
			AvgConfidence     float64
			SuccessRate       float64
		}
		err := it.Next(&row)
		if err == iterator.Done {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("failed to read row: %w", err)
		}
		if row.ExperimentGroup == "control" {
			metrics.Control.TotalRequests = row.TotalRequests
			metrics.Control.AvgProcessingTime = row.AvgProcessingTime
			metrics.Control.AvgConfidence = row.AvgConfidence
			metrics.Control.SuccessRate = row.SuccessRate
		} else {
			metrics.Experiment.TotalRequests = row.TotalRequests
			metrics.Experiment.AvgProcessingTime = row.AvgProcessingTime
			metrics.Experiment.AvgConfidence = row.AvgConfidence
			metrics.Experiment.SuccessRate = row.SuccessRate
		}
	}
	return metrics, nil
}
// Close closes the BigQuery client and releases associated resources.
// This method should be called when the service is no longer needed.
func (s *BigQueryService) Close() error {
	return s.client.Close()
}
</file>

<file path="internal/pkg/audio/analysis.go">
package audio
import (
	"context"
	"fmt"
	"os/exec"
	"strconv"
	"strings"
)
// MusicalKey represents a musical key
type MusicalKey struct {
	Root       string // e.g., "C", "F#"
	Mode       string // "major" or "minor"
	Camelot    string // Camelot notation (e.g., "8A")
	OpenKey    string // Open Key notation (e.g., "8d")
	Confidence float64
}
// AudioAnalysis contains detailed audio analysis results
type AudioAnalysis struct {
	BPM           float64    // Beats per minute
	BPMConfidence float64    // Confidence level of BPM detection
	Key           MusicalKey // Detected musical key
	Beats         []float64  // Beat positions in seconds
	Segments      []Segment  // Audio segments analysis
	Energy        float64    // Overall energy level
	Danceability  float64    // Danceability score
}
// Segment represents an analyzed segment of audio
type Segment struct {
	Start      float64   // Start time in seconds
	Duration   float64   // Duration in seconds
	Loudness   float64   // Segment loudness in dB
	Pitches    []float64 // Pitch values for 12 semitones
	Timbre     []float64 // Timbre coefficients
	Confidence float64   // Analysis confidence
}
// AudioAnalyzer handles advanced audio analysis
type AudioAnalyzer struct {
	ffmpeg   *FFmpegProcessor
	essentia string // Path to Essentia extractors
	aubio    string // Path to Aubio tools
}
// NewAudioAnalyzer creates a new audio analyzer
func NewAudioAnalyzer(ffmpeg *FFmpegProcessor) (*AudioAnalyzer, error) {
	// Check for Essentia tools
	essentia, err := exec.LookPath("essentia_streaming_extractor_music")
	if err != nil {
		essentia = "" // Optional: will fall back to alternative methods
	}
	// Check for Aubio tools
	aubio, err := exec.LookPath("aubio")
	if err != nil {
		aubio = "" // Optional: will fall back to alternative methods
	}
	return &AudioAnalyzer{
		ffmpeg:   ffmpeg,
		essentia: essentia,
		aubio:    aubio,
	}, nil
}
// AnalyzeTrack performs comprehensive audio analysis
func (a *AudioAnalyzer) AnalyzeTrack(ctx context.Context, inputPath string) (*AudioAnalysis, error) {
	analysis := &AudioAnalysis{}
	// Detect BPM using multiple methods for accuracy
	bpm, confidence, err := a.detectBPM(ctx, inputPath)
	if err != nil {
		return nil, fmt.Errorf("BPM detection failed: %w", err)
	}
	analysis.BPM = bpm
	analysis.BPMConfidence = confidence
	// Detect musical key
	key, err := a.detectKey(ctx, inputPath)
	if err != nil {
		return nil, fmt.Errorf("key detection failed: %w", err)
	}
	analysis.Key = key
	// Detect beats
	beats, err := a.detectBeats(ctx, inputPath)
	if err != nil {
		return nil, fmt.Errorf("beat detection failed: %w", err)
	}
	analysis.Beats = beats
	// Analyze segments if Essentia is available
	if a.essentia != "" {
		segments, err := a.analyzeSegments(ctx, inputPath)
		if err != nil {
			return nil, fmt.Errorf("segment analysis failed: %w", err)
		}
		analysis.Segments = segments
	}
	// Calculate energy and danceability
	energy, danceability, err := a.calculateFeatures(ctx, inputPath)
	if err != nil {
		return nil, fmt.Errorf("feature calculation failed: %w", err)
	}
	analysis.Energy = energy
	analysis.Danceability = danceability
	return analysis, nil
}
// detectBPM detects beats per minute using multiple methods
func (a *AudioAnalyzer) detectBPM(ctx context.Context, inputPath string) (float64, float64, error) {
	var bpms []float64
	// Method 1: FFmpeg with ebur128 filter
	args := []string{
		"-i", inputPath,
		"-filter:a", "ebur128=framelog=verbose",
		"-f", "null",
		"-",
	}
	cmd := exec.CommandContext(ctx, a.ffmpeg.ffmpegPath, args...)
	output, err := cmd.CombinedOutput()
	if err == nil {
		// Parse BPM from output
		if bpm := parseBPMFromFFmpeg(string(output)); bpm > 0 {
			bpms = append(bpms, bpm)
		}
	}
	// Method 2: Aubio if available
	if a.aubio != "" {
		args = []string{
			"tempo",
			inputPath,
		}
		cmd = exec.CommandContext(ctx, a.aubio, args...)
		output, err = cmd.Output()
		if err == nil {
			if bpm, err := strconv.ParseFloat(strings.TrimSpace(string(output)), 64); err == nil {
				bpms = append(bpms, bpm)
			}
		}
	}
	// Calculate average BPM and confidence
	if len(bpms) == 0 {
		return 0, 0, fmt.Errorf("no valid BPM detection results")
	}
	var sum float64
	for _, bpm := range bpms {
		sum += bpm
	}
	avgBPM := sum / float64(len(bpms))
	// Calculate confidence based on consistency between methods
	var confidence float64
	if len(bpms) > 1 {
		var variance float64
		for _, bpm := range bpms {
			diff := bpm - avgBPM
			variance += diff * diff
		}
		variance /= float64(len(bpms))
		confidence = 1.0 / (1.0 + variance/10.0) // Normalize confidence
	} else {
		confidence = 0.7 // Single method confidence
	}
	return avgBPM, confidence, nil
}
// detectKey detects the musical key
func (a *AudioAnalyzer) detectKey(ctx context.Context, inputPath string) (MusicalKey, error) {
	// Use Essentia if available for accurate key detection
	if a.essentia != "" {
		return a.detectKeyWithEssentia(ctx, inputPath)
	}
	// Fallback to FFmpeg chromagram analysis
	args := []string{
		"-i", inputPath,
		"-filter:a", "achromagram=s=4096:overlap=0.75",
		"-f", "null",
		"-",
	}
	cmd := exec.CommandContext(ctx, a.ffmpeg.ffmpegPath, args...)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return MusicalKey{}, fmt.Errorf("key detection failed: %w", err)
	}
	// Parse key from chromagram
	key := parseKeyFromChromagram(string(output))
	return key, nil
}
// detectBeats detects beat positions
func (a *AudioAnalyzer) detectBeats(ctx context.Context, inputPath string) ([]float64, error) {
	args := []string{
		"-i", inputPath,
		"-filter:a", "abeat",
		"-f", "null",
		"-",
	}
	cmd := exec.CommandContext(ctx, a.ffmpeg.ffmpegPath, args...)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return nil, fmt.Errorf("beat detection failed: %w", err)
	}
	return parseBeatsFromOutput(string(output)), nil
}
// analyzeSegments performs detailed segment analysis
func (a *AudioAnalyzer) analyzeSegments(ctx context.Context, inputPath string) ([]Segment, error) {
	if a.essentia == "" {
		return nil, fmt.Errorf("Essentia not available for segment analysis")
	}
	args := []string{
		inputPath,
		inputPath + ".json",
	}
	cmd := exec.CommandContext(ctx, a.essentia, args...)
	if err := cmd.Run(); err != nil {
		return nil, fmt.Errorf("segment analysis failed: %w", err)
	}
	// Parse segments from Essentia output
	segments, err := parseEssentiaSegments(inputPath + ".json")
	if err != nil {
		return nil, fmt.Errorf("failed to parse segments: %w", err)
	}
	return segments, nil
}
// calculateFeatures calculates energy and danceability
func (a *AudioAnalyzer) calculateFeatures(ctx context.Context, inputPath string) (energy float64, danceability float64, err error) {
	// Calculate energy from RMS levels
	args := []string{
		"-i", inputPath,
		"-filter:a", "volumedetect,astats=measure_perchannel=0:measure_overall=1",
		"-f", "null",
		"-",
	}
	cmd := exec.CommandContext(ctx, a.ffmpeg.ffmpegPath, args...)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return 0, 0, fmt.Errorf("feature calculation failed: %w", err)
	}
	// Parse features from output
	energy = parseEnergyFromStats(string(output))
	// Get BPM for danceability calculation
	bpm, _, err := a.detectBPM(ctx, inputPath)
	if err != nil {
		return energy, 0, fmt.Errorf("failed to get BPM for danceability: %w", err)
	}
	danceability = calculateDanceability(energy, bpm)
	return energy, danceability, nil
}
// Helper functions for parsing outputs
func parseBPMFromFFmpeg(output string) float64 {
	// Implementation for parsing BPM from FFmpeg output
	return 0
}
func parseKeyFromChromagram(output string) MusicalKey {
	// Implementation for parsing key from chromagram
	return MusicalKey{}
}
func parseBeatsFromOutput(output string) []float64 {
	// Implementation for parsing beat positions
	return nil
}
func parseEssentiaSegments(jsonPath string) ([]Segment, error) {
	// Implementation for parsing Essentia JSON output
	return nil, nil
}
func parseEnergyFromStats(output string) float64 {
	// Implementation for parsing energy from stats
	return 0
}
func calculateDanceability(energy float64, bpm float64) float64 {
	// Implementation for calculating danceability score
	return 0
}
// detectKeyWithEssentia detects musical key using Essentia
func (a *AudioAnalyzer) detectKeyWithEssentia(ctx context.Context, inputPath string) (MusicalKey, error) {
	args := []string{
		inputPath,
		inputPath + ".json",
		"--music",
	}
	cmd := exec.CommandContext(ctx, a.essentia, args...)
	if err := cmd.Run(); err != nil {
		return MusicalKey{}, fmt.Errorf("Essentia key detection failed: %w", err)
	}
	// Parse key from Essentia output
	key, err := parseEssentiaKey(inputPath + ".json")
	if err != nil {
		return MusicalKey{}, fmt.Errorf("failed to parse key: %w", err)
	}
	return key, nil
}
func parseEssentiaKey(jsonPath string) (MusicalKey, error) {
	// Implementation for parsing key from Essentia JSON output
	return MusicalKey{}, nil
}
</file>

<file path="internal/pkg/audio/batch.go">
package audio
import (
	"context"
	"fmt"
	"path/filepath"
	"sync"
)
// BatchResult represents the result of processing a single file in a batch
type BatchResult struct {
	FilePath string
	Error    error
	Analysis *AudioAnalysis
}
// BatchProgress represents the progress of batch processing
type BatchProgress struct {
	TotalFiles     int
	ProcessedFiles int
	CurrentFile    string
	Results        []BatchResult
}
// BatchProcessor handles processing of multiple audio files
type BatchProcessor struct {
	analyzer   *AudioAnalyzer
	processor  *FFmpegProcessor
	maxWorkers int
	progressCh chan BatchProgress
	results    []BatchResult
	mu         sync.Mutex
}
// NewBatchProcessor creates a new batch processor
func NewBatchProcessor(analyzer *AudioAnalyzer, processor *FFmpegProcessor, maxWorkers int) *BatchProcessor {
	if maxWorkers <= 0 {
		maxWorkers = 4 // Default to 4 workers
	}
	return &BatchProcessor{
		analyzer:   analyzer,
		processor:  processor,
		maxWorkers: maxWorkers,
		progressCh: make(chan BatchProgress, 1),
		results:    make([]BatchResult, 0),
	}
}
// ProcessFiles processes multiple audio files in parallel
func (b *BatchProcessor) ProcessFiles(ctx context.Context, files []string, opts ProcessingOptions) (<-chan BatchProgress, error) {
	if len(files) == 0 {
		return nil, fmt.Errorf("no files to process")
	}
	// Create a buffered channel for work items
	workCh := make(chan string, len(files))
	for _, file := range files {
		workCh <- file
	}
	close(workCh)
	// Create worker pool
	var wg sync.WaitGroup
	for i := 0; i < b.maxWorkers; i++ {
		wg.Add(1)
		go b.worker(ctx, &wg, workCh, opts)
	}
	// Start progress monitoring in a separate goroutine
	go func() {
		wg.Wait()
		close(b.progressCh)
	}()
	return b.progressCh, nil
}
// worker processes files from the work channel
func (b *BatchProcessor) worker(ctx context.Context, wg *sync.WaitGroup, workCh <-chan string, opts ProcessingOptions) {
	defer wg.Done()
	for filePath := range workCh {
		select {
		case <-ctx.Done():
			return
		default:
			result := b.processFile(ctx, filePath, opts)
			b.addResult(result)
			b.updateProgress(filePath)
		}
	}
}
// processFile processes a single file
func (b *BatchProcessor) processFile(ctx context.Context, filePath string, opts ProcessingOptions) BatchResult {
	result := BatchResult{
		FilePath: filePath,
	}
	// Process the file
	outputPath := filepath.Join(filepath.Dir(filePath), "processed_"+filepath.Base(filePath))
	if err := b.processor.ProcessAudio(ctx, filePath, outputPath, opts); err != nil {
		result.Error = fmt.Errorf("processing failed: %w", err)
		return result
	}
	// Analyze the processed file
	analysis, err := b.analyzer.AnalyzeTrack(ctx, outputPath)
	if err != nil {
		result.Error = fmt.Errorf("analysis failed: %w", err)
		return result
	}
	result.Analysis = analysis
	return result
}
// addResult adds a result to the results slice thread-safely
func (b *BatchProcessor) addResult(result BatchResult) {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.results = append(b.results, result)
}
// updateProgress sends a progress update
func (b *BatchProcessor) updateProgress(currentFile string) {
	b.mu.Lock()
	progress := BatchProgress{
		TotalFiles:     len(b.results),
		ProcessedFiles: len(b.results),
		CurrentFile:    currentFile,
		Results:        make([]BatchResult, len(b.results)),
	}
	copy(progress.Results, b.results)
	b.mu.Unlock()
	// Send progress update non-blocking
	select {
	case b.progressCh <- progress:
	default:
		// Channel is full, skip this update
	}
}
// GetResults returns all processing results
func (b *BatchProcessor) GetResults() []BatchResult {
	b.mu.Lock()
	defer b.mu.Unlock()
	results := make([]BatchResult, len(b.results))
	copy(results, b.results)
	return results
}
// ClearResults clears all processing results
func (b *BatchProcessor) ClearResults() {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.results = b.results[:0]
}
</file>

<file path="internal/pkg/audio/effects.go">
package audio
import (
	"context"
	"fmt"
	"os/exec"
	"path/filepath"
	"strings"
)
// AudioEffect represents a type of audio effect
type AudioEffect string
const (
	EffectCompression AudioEffect = "compression"
	EffectEQ          AudioEffect = "eq"
	EffectReverb      AudioEffect = "reverb"
	EffectDelay       AudioEffect = "delay"
	EffectLimiter     AudioEffect = "limiter"
)
// CompressorSettings defines audio compression parameters
type CompressorSettings struct {
	Threshold  float64 // dB level where compression begins
	Ratio      float64 // compression ratio (e.g., 4.0 for 4:1)
	Attack     float64 // attack time in milliseconds
	Release    float64 // release time in milliseconds
	MakeupGain float64 // makeup gain in dB
	KneeWidth  float64 // knee width in dB
}
// EQSettings defines equalizer parameters
type EQSettings struct {
	Bands []EQBand
}
// EQBand represents a single equalizer band
type EQBand struct {
	Frequency float64 // center frequency in Hz
	Gain      float64 // gain in dB
	Q         float64 // Q factor (bandwidth)
}
// ReverbSettings defines reverb parameters
type ReverbSettings struct {
	RoomSize float64 // room size (0.0-1.0)
	Damping  float64 // high frequency damping (0.0-1.0)
	WetLevel float64 // wet (processed) signal level (0.0-1.0)
	DryLevel float64 // dry (unprocessed) signal level (0.0-1.0)
	Width    float64 // stereo width (0.0-1.0)
	PreDelay float64 // pre-delay in milliseconds
}
// DelaySettings defines delay parameters
type DelaySettings struct {
	Time     float64 // delay time in milliseconds
	Feedback float64 // feedback amount (0.0-1.0)
	Mix      float64 // wet/dry mix (0.0-1.0)
}
// LimiterSettings defines limiter parameters
type LimiterSettings struct {
	Threshold float64 // limiting threshold in dB
	Release   float64 // release time in milliseconds
}
// Effect represents an audio effect
type Effect interface {
	// GetFFmpegFilter returns the FFmpeg filter string for this effect
	GetFFmpegFilter() string
}
// EffectChain represents a chain of audio effects
type EffectChain struct {
	Effects []Effect
}
// CompressorEffect represents dynamic range compression
type CompressorEffect struct {
	Threshold  float64 // dB threshold (e.g., -20)
	Ratio      float64 // compression ratio (e.g., 4)
	Attack     float64 // attack time in milliseconds
	Release    float64 // release time in milliseconds
	MakeupGain float64 // makeup gain in dB
}
// GetFFmpegFilter implements the Effect interface
func (e CompressorEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("acompressor=threshold=%f:ratio=%f:attack=%f:release=%f:makeup=%f",
		e.Threshold, e.Ratio, e.Attack, e.Release, e.MakeupGain)
}
// EQEffect represents a parametric equalizer band
type EQEffect struct {
	Frequency float64 // center frequency in Hz
	Gain      float64 // gain in dB
	Q         float64 // Q factor (bandwidth)
}
// GetFFmpegFilter implements the Effect interface
func (e EQEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("equalizer=f=%f:t=h:w=%f:g=%f",
		e.Frequency, e.Q, e.Gain)
}
// ReverbEffect represents reverb parameters
type ReverbEffect struct {
	RoomSize float64 // 0-1 room size
	Damping  float64 // 0-1 damping factor
	WetLevel float64 // 0-1 wet level
	DryLevel float64 // 0-1 dry level
	Width    float64 // 0-1 stereo width
	PreDelay float64 // pre-delay in ms
}
// GetFFmpegFilter implements the Effect interface
func (e ReverbEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("aecho=0.8:%f:%f:0.5",
		e.PreDelay, e.RoomSize*1000)
}
// DelayEffect represents delay parameters
type DelayEffect struct {
	Time     float64 // delay time in milliseconds
	Feedback float64 // 0-1 feedback amount
	Mix      float64 // 0-1 wet/dry mix
}
// GetFFmpegFilter implements the Effect interface
func (e DelayEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("adelay=%d|%d,amix=2:1",
		int(e.Time), int(e.Time))
}
// LimiterEffect represents a peak limiter
type LimiterEffect struct {
	Threshold float64 // dB threshold
	Release   float64 // release time in seconds
}
// GetFFmpegFilter implements the Effect interface
func (e LimiterEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("alimiter=level_in=%f:level_out=%f:limit=%f:release=%f",
		1.0, 1.0, e.Threshold, e.Release)
}
// StereoWidthEffect represents stereo width adjustment
type StereoWidthEffect struct {
	Width float64 // 0-2 width factor (1 = normal, 0 = mono, 2 = extra wide)
}
// GetFFmpegFilter implements the Effect interface
func (e StereoWidthEffect) GetFFmpegFilter() string {
	return fmt.Sprintf("stereotools=mwidth=%f",
		e.Width)
}
// NewEffectChain creates a new effect chain
func NewEffectChain() *EffectChain {
	return &EffectChain{
		Effects: make([]Effect, 0),
	}
}
// Add adds an effect to the chain
func (c *EffectChain) Add(effect Effect) {
	c.Effects = append(c.Effects, effect)
}
// GetFFmpegFilterChain returns the complete FFmpeg filter chain
func (c *EffectChain) GetFFmpegFilterChain() string {
	var filters []string
	for _, effect := range c.Effects {
		filters = append(filters, effect.GetFFmpegFilter())
	}
	return strings.Join(filters, ",")
}
// Common presets
var (
	DefaultCompressor = CompressorEffect{
		Threshold:  -20,
		Ratio:      4,
		Attack:     20,
		Release:    100,
		MakeupGain: 0,
	}
	VocalCompressor = CompressorEffect{
		Threshold:  -24,
		Ratio:      6,
		Attack:     10,
		Release:    60,
		MakeupGain: 3,
	}
	SmallRoom = ReverbEffect{
		RoomSize: 0.2,
		Damping:  0.3,
		WetLevel: 0.3,
		DryLevel: 0.7,
		Width:    1.0,
		PreDelay: 20,
	}
	LargeHall = ReverbEffect{
		RoomSize: 0.8,
		Damping:  0.2,
		WetLevel: 0.4,
		DryLevel: 0.6,
		Width:    1.0,
		PreDelay: 40,
	}
	QuarterNote = DelayEffect{
		Time:     250, // Assuming 120 BPM
		Feedback: 0.3,
		Mix:      0.4,
	}
	MasterLimiter = LimiterEffect{
		Threshold: -1.0,
		Release:   0.1,
	}
)
// CreatePresetChain creates an effect chain with common presets
func CreatePresetChain(preset string) *EffectChain {
	chain := NewEffectChain()
	switch strings.ToLower(preset) {
	case "vocal":
		chain.Add(VocalCompressor)
		chain.Add(SmallRoom)
		chain.Add(MasterLimiter)
	case "master":
		chain.Add(DefaultCompressor)
		chain.Add(MasterLimiter)
	case "ambient":
		chain.Add(LargeHall)
		chain.Add(StereoWidthEffect{Width: 1.5})
		chain.Add(MasterLimiter)
	}
	return chain
}
// ApplyEffects applies a chain of audio effects to the input file
func (p *FFmpegProcessor) ApplyEffects(ctx context.Context, inputPath string, chain EffectChain) (string, error) {
	outputPath := filepath.Join(p.tempDir, fmt.Sprintf("%s_processed%s",
		strings.TrimSuffix(filepath.Base(inputPath), filepath.Ext(inputPath)),
		filepath.Ext(inputPath)))
	// Build FFmpeg filter chain
	var filters []string
	for _, effect := range chain.Effects {
		filter, err := buildEffectFilter(effect)
		if err != nil {
			return "", fmt.Errorf("failed to build effect filter: %w", err)
		}
		filters = append(filters, filter)
	}
	// Construct FFmpeg command
	args := []string{
		"-i", inputPath,
		"-filter:a", strings.Join(filters, ","),
		"-y", // overwrite output file
		outputPath,
	}
	// Run FFmpeg
	cmd := exec.CommandContext(ctx, p.ffmpegPath, args...)
	if err := cmd.Run(); err != nil {
		return "", fmt.Errorf("failed to apply effects: %w", err)
	}
	return outputPath, nil
}
// buildEffectFilter constructs FFmpeg filter string for an effect
func buildEffectFilter(effect Effect) (string, error) {
	switch e := effect.(type) {
	case CompressorEffect:
		return fmt.Sprintf(
			"acompressor=threshold=%f:ratio=%f:attack=%f:release=%f:makeup=%f",
			e.Threshold,
			e.Ratio,
			e.Attack,
			e.Release,
			e.MakeupGain,
		), nil
	case EQEffect:
		return fmt.Sprintf(
			"equalizer=f=%f:width_type=q:w=%f:g=%f",
			e.Frequency,
			e.Q,
			e.Gain,
		), nil
	case ReverbEffect:
		return fmt.Sprintf(
			"aecho=0.8:%f:%f:0.5",
			e.PreDelay,
			e.RoomSize*1000,
		), nil
	case DelayEffect:
		return fmt.Sprintf(
			"adelay=%d|%d,amix=2:1",
			int(e.Time),
			int(e.Time),
		), nil
	case LimiterEffect:
		return fmt.Sprintf(
			"alimiter=level_in=%f:level_out=%f:limit=%f:release=%f",
			1.0,
			1.0,
			e.Threshold,
			e.Release,
		), nil
	case StereoWidthEffect:
		return fmt.Sprintf(
			"stereotools=mwidth=%f",
			e.Width,
		), nil
	default:
		return "", fmt.Errorf("unsupported effect type: %T", effect)
	}
}
</file>

<file path="internal/pkg/audio/ffmpeg.go">
package audio
import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"strconv"
	"strings"
)
// AudioFormat represents supported output formats
type AudioFormat string
const (
	FormatMP3  AudioFormat = "mp3"
	FormatFLAC AudioFormat = "flac"
	FormatWAV  AudioFormat = "wav"
	FormatAAC  AudioFormat = "aac"
	FormatOGG  AudioFormat = "ogg"
)
// AudioQuality represents audio quality settings
type AudioQuality struct {
	Bitrate      int     // in kbps
	SampleRate   int     // in Hz
	Channels     int     // number of channels
	NoiseLevel   float64 // in dB
	Clipping     float64 // percentage of clipped samples
	DynamicRange float64 // in dB
}
// ConversionOptions defines options for audio conversion
type ConversionOptions struct {
	Format     AudioFormat
	Bitrate    int     // in kbps
	SampleRate int     // in Hz
	Channels   int     // number of channels
	Normalize  bool    // apply audio normalization
	TargetLUFS float64 // target loudness (default: -14 LUFS for streaming)
	Quality    int     // encoding quality (0-9, where 0 is best)
}
// FFmpegProcessor handles audio processing using FFmpeg
type FFmpegProcessor struct {
	ffmpegPath  string
	ffprobePath string
	tempDir     string
}
// FFprobeOutput represents the JSON output from FFprobe
type FFprobeOutput struct {
	Streams []struct {
		CodecType  string `json:"codec_type"`
		BitRate    string `json:"bit_rate"`
		SampleRate string `json:"sample_rate"`
		Channels   int    `json:"channels"`
	} `json:"streams"`
	Format struct {
		Duration string `json:"duration"`
		BitRate  string `json:"bit_rate"`
		Size     string `json:"size"`
	} `json:"format"`
}
// NewFFmpegProcessor creates a new FFmpeg processor
func NewFFmpegProcessor() (*FFmpegProcessor, error) {
	ffmpegPath, err := exec.LookPath("ffmpeg")
	if err != nil {
		return nil, fmt.Errorf("ffmpeg not found: %w", err)
	}
	ffprobePath, err := exec.LookPath("ffprobe")
	if err != nil {
		return nil, fmt.Errorf("ffprobe not found: %w", err)
	}
	return &FFmpegProcessor{
		ffmpegPath:  ffmpegPath,
		ffprobePath: ffprobePath,
		tempDir:     os.TempDir(),
	}, nil
}
// ConvertAudio converts audio to the specified format with given options
func (p *FFmpegProcessor) ConvertAudio(ctx context.Context, inputPath string, opts ConversionOptions) (string, error) {
	outputPath := filepath.Join(p.tempDir, fmt.Sprintf("%s.%s",
		strings.TrimSuffix(filepath.Base(inputPath), filepath.Ext(inputPath)),
		opts.Format))
	args := []string{
		"-i", inputPath,
		"-y", // overwrite output file
	}
	// Add conversion options
	if opts.Bitrate > 0 {
		args = append(args, "-b:a", fmt.Sprintf("%dk", opts.Bitrate))
	}
	if opts.SampleRate > 0 {
		args = append(args, "-ar", strconv.Itoa(opts.SampleRate))
	}
	if opts.Channels > 0 {
		args = append(args, "-ac", strconv.Itoa(opts.Channels))
	}
	if opts.Quality >= 0 && opts.Quality <= 9 {
		args = append(args, "-q:a", strconv.Itoa(opts.Quality))
	}
	// Apply normalization if requested
	if opts.Normalize {
		// First pass: analyze loudness
		loudnessArgs := []string{
			"-i", inputPath,
			"-af", "loudnorm=print_format=json",
			"-f", "null",
			"-",
		}
		cmd := exec.CommandContext(ctx, p.ffmpegPath, loudnessArgs...)
		var stderr bytes.Buffer
		cmd.Stderr = &stderr
		if err := cmd.Run(); err != nil {
			return "", fmt.Errorf("loudness analysis failed: %w", err)
		}
		// Parse loudness stats
		var stats struct {
			InputI      float64 `json:"input_i"`
			InputTP     float64 `json:"input_tp"`
			InputLRA    float64 `json:"input_lra"`
			InputThresh float64 `json:"input_thresh"`
		}
		if err := json.Unmarshal(stderr.Bytes(), &stats); err != nil {
			return "", fmt.Errorf("failed to parse loudness stats: %w", err)
		}
		// Add normalization filter
		args = append(args, "-af", fmt.Sprintf(
			"loudnorm=I=%f:TP=-1.5:LRA=11",
			opts.TargetLUFS,
		))
	}
	// Add output path
	args = append(args, outputPath)
	// Run conversion
	cmd := exec.CommandContext(ctx, p.ffmpegPath, args...)
	if err := cmd.Run(); err != nil {
		return "", fmt.Errorf("conversion failed: %w", err)
	}
	return outputPath, nil
}
// AnalyzeAudio performs audio quality analysis
func (p *FFmpegProcessor) AnalyzeAudio(ctx context.Context, inputPath string) (*AudioQuality, error) {
	// Get basic audio information
	args := []string{
		"-v", "quiet",
		"-print_format", "json",
		"-show_format",
		"-show_streams",
		inputPath,
	}
	cmd := exec.CommandContext(ctx, p.ffprobePath, args...)
	output, err := cmd.Output()
	if err != nil {
		return nil, fmt.Errorf("audio analysis failed: %w", err)
	}
	// Parse FFprobe output
	var probeData struct {
		Streams []struct {
			CodecType  string `json:"codec_type"`
			BitRate    string `json:"bit_rate"`
			SampleRate string `json:"sample_rate"`
			Channels   int    `json:"channels"`
		} `json:"streams"`
	}
	if err := json.Unmarshal(output, &probeData); err != nil {
		return nil, fmt.Errorf("failed to parse audio info: %w", err)
	}
	// Find the audio stream
	var audioStream struct {
		CodecType  string `json:"codec_type"`
		BitRate    string `json:"bit_rate"`
		SampleRate string `json:"sample_rate"`
		Channels   int    `json:"channels"`
	}
	for _, stream := range probeData.Streams {
		if stream.CodecType == "audio" {
			audioStream = stream
			break
		}
	}
	// Convert bitrate to int
	bitrate, _ := strconv.Atoi(audioStream.BitRate)
	bitrate /= 1000 // Convert to kbps
	// Convert sample rate to int
	sampleRate, _ := strconv.Atoi(audioStream.SampleRate)
	// Analyze audio quality
	quality := &AudioQuality{
		Bitrate:    bitrate,
		SampleRate: sampleRate,
		Channels:   audioStream.Channels,
	}
	// Analyze noise level and clipping
	silenceArgs := []string{
		"-i", inputPath,
		"-af", "silencedetect=noise=-50dB:d=0.1",
		"-f", "null",
		"-",
	}
	cmd = exec.CommandContext(ctx, p.ffmpegPath, silenceArgs...)
	var stderr bytes.Buffer
	cmd.Stderr = &stderr
	if err := cmd.Run(); err != nil {
		return nil, fmt.Errorf("silence detection failed: %w", err)
	}
	// Parse silence detection output
	output = stderr.Bytes()
	if idx := bytes.Index(output, []byte("mean_volume:")); idx != -1 {
		fmt.Sscanf(string(output[idx:]), "mean_volume: %f dB", &quality.NoiseLevel)
	}
	// Analyze clipping
	volumeArgs := []string{
		"-i", inputPath,
		"-af", "volumedetect",
		"-f", "null",
		"-",
	}
	cmd = exec.CommandContext(ctx, p.ffmpegPath, volumeArgs...)
	stderr.Reset()
	cmd.Stderr = &stderr
	if err := cmd.Run(); err != nil {
		return nil, fmt.Errorf("volume detection failed: %w", err)
	}
	// Parse volume detection output
	output = stderr.Bytes()
	var maxVolume float64
	if idx := bytes.Index(output, []byte("max_volume:")); idx != -1 {
		fmt.Sscanf(string(output[idx:]), "max_volume: %f dB", &maxVolume)
	}
	quality.Clipping = maxVolume / 0.0 // Calculate percentage of clipping
	// Calculate dynamic range
	quality.DynamicRange = maxVolume - quality.NoiseLevel
	return quality, nil
}
// GenerateWaveform generates a waveform data for visualization
func (p *FFmpegProcessor) GenerateWaveform(ctx context.Context, inputPath string, samples int) ([]float64, error) {
	// Use ffmpeg to generate waveform data
	args := []string{
		"-i", inputPath,
		"-filter_complex", fmt.Sprintf("aformat=channel_layouts=mono,compand,showwavespic=s=%d:1", samples),
		"-f", "null",
		"-",
	}
	cmd := exec.CommandContext(ctx, p.ffmpegPath, args...)
	var stderr bytes.Buffer
	cmd.Stderr = &stderr
	if err := cmd.Run(); err != nil {
		return nil, fmt.Errorf("waveform generation failed: %w", err)
	}
	// Parse waveform data
	waveform := make([]float64, samples)
	lines := strings.Split(stderr.String(), "\n")
	for i, line := range lines {
		if i >= samples {
			break
		}
		if value, err := strconv.ParseFloat(strings.TrimSpace(line), 64); err == nil {
			waveform[i] = value
		}
	}
	return waveform, nil
}
</file>

<file path="internal/pkg/audio/processor.go">
package audio
import (
	"context"
	"fmt"
	"io"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"time"
	"github.com/dhowden/tag"
)
// ProcessingOptions defines options for audio file processing
type ProcessingOptions struct {
	ValidateOnly bool              // Only validate without processing
	MaxDuration  time.Duration     // Maximum allowed audio duration
	Progress     chan float64      // Optional channel for progress updates
	Metadata     map[string]string // Additional metadata to store
	Format       AudioFormat       // Target format for conversion
	Normalize    bool              // Apply audio normalization
	TargetLUFS   float64           // Target loudness for normalization
	Quality      int               // Encoding quality (0-9)
}
// ProcessingResult contains the results of audio processing
type ProcessingResult struct {
	Duration    time.Duration // Audio duration
	Format      string        // Audio format
	Bitrate     int           // Audio bitrate
	Channels    int           // Number of audio channels
	Metadata    tag.Metadata  // Extracted metadata
	FileSize    int64         // Processed file size
	ProcessedAt time.Time     // Processing timestamp
	Quality     *AudioQuality // Quality analysis results
	Waveform    []float64     // Waveform data for visualization
}
// ProcessAudio processes an audio file with the given options
func ProcessAudio(ctx context.Context, content io.Reader, size int64, filename string, opts *ProcessingOptions) (*ProcessingResult, error) {
	// Set default options if not provided
	if opts == nil {
		opts = &ProcessingOptions{
			MaxDuration: 4 * time.Hour,
			Format:      FormatMP3,
			TargetLUFS:  -14.0, // Standard streaming target
			Quality:     3,     // Medium-high quality
		}
	}
	// Create temporary file
	tempFile, err := os.CreateTemp("", "audio-*"+filepath.Ext(filename))
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	defer os.Remove(tempFile.Name())
	defer tempFile.Close()
	// Copy content to temp file
	if _, err := io.Copy(tempFile, content); err != nil {
		return nil, fmt.Errorf("failed to write temp file: %w", err)
	}
	// Initialize FFmpeg processor
	ffmpeg, err := NewFFmpegProcessor()
	if err != nil {
		return nil, fmt.Errorf("failed to initialize FFmpeg: %w", err)
	}
	// Analyze original audio quality
	quality, err := ffmpeg.AnalyzeAudio(ctx, tempFile.Name())
	if err != nil {
		return nil, fmt.Errorf("failed to analyze audio: %w", err)
	}
	// Generate waveform data
	waveform, err := ffmpeg.GenerateWaveform(ctx, tempFile.Name(), 200) // 200 samples for visualization
	if err != nil {
		return nil, fmt.Errorf("failed to generate waveform: %w", err)
	}
	// If only validating, return analysis results
	if opts.ValidateOnly {
		return &ProcessingResult{
			Format:      filepath.Ext(filename)[1:],
			FileSize:    size,
			ProcessedAt: time.Now(),
			Quality:     quality,
			Waveform:    waveform,
		}, nil
	}
	// Convert audio if needed
	outputPath, err := ffmpeg.ConvertAudio(ctx, tempFile.Name(), ConversionOptions{
		Format:     opts.Format,
		Normalize:  opts.Normalize,
		TargetLUFS: opts.TargetLUFS,
		Quality:    opts.Quality,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to convert audio: %w", err)
	}
	defer os.Remove(outputPath)
	// Read converted file for metadata
	convertedFile, err := os.Open(outputPath)
	if err != nil {
		return nil, fmt.Errorf("failed to open converted file: %w", err)
	}
	defer convertedFile.Close()
	// Extract metadata from converted file
	metadata, err := tag.ReadFrom(convertedFile)
	if err != nil {
		return nil, fmt.Errorf("failed to extract metadata: %w", err)
	}
	// Analyze converted audio quality
	convertedQuality, err := ffmpeg.AnalyzeAudio(ctx, outputPath)
	if err != nil {
		return nil, fmt.Errorf("failed to analyze converted audio: %w", err)
	}
	// Get file info
	fileInfo, err := convertedFile.Stat()
	if err != nil {
		return nil, fmt.Errorf("failed to get file info: %w", err)
	}
	// Send progress updates if channel is provided
	if opts.Progress != nil {
		opts.Progress <- 1.0 // 100% complete
	}
	return &ProcessingResult{
		Format:      string(opts.Format),
		Bitrate:     convertedQuality.Bitrate,
		Channels:    convertedQuality.Channels,
		Metadata:    metadata,
		FileSize:    fileInfo.Size(),
		ProcessedAt: time.Now(),
		Quality:     convertedQuality,
		Waveform:    waveform,
	}, nil
}
// ValidateAndExtractMetadata combines validation and metadata extraction
func ValidateAndExtractMetadata(content io.Reader, size int64, filename string) (tag.Metadata, error) {
	// Create temporary file
	tempFile, err := os.CreateTemp("", "audio-*"+filepath.Ext(filename))
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	defer os.Remove(tempFile.Name())
	defer tempFile.Close()
	// Copy content to temp file
	if _, err := io.Copy(tempFile, content); err != nil {
		return nil, fmt.Errorf("failed to write temp file: %w", err)
	}
	// Reopen file for reading
	file, err := os.Open(tempFile.Name())
	if err != nil {
		return nil, fmt.Errorf("failed to open temp file: %w", err)
	}
	defer file.Close()
	// Extract metadata
	return tag.ReadFrom(file)
}
// ProcessAudio processes an audio file with the given options
func (p *FFmpegProcessor) ProcessAudio(ctx context.Context, inputPath string, outputPath string, opts ProcessingOptions) error {
	args := []string{
		"-i", inputPath,
	}
	// Add audio filters based on options
	var filters []string
	// Normalize audio if requested
	if opts.Normalize {
		filters = append(filters, fmt.Sprintf("loudnorm=I=%f:TP=-1.0:LRA=11", opts.TargetLUFS))
	}
	// Add quality-specific options
	switch opts.Format {
	case FormatMP3:
		args = append(args, "-c:a", "libmp3lame")
		args = append(args, "-q:a", fmt.Sprintf("%d", opts.Quality))
	case FormatFLAC:
		args = append(args, "-c:a", "flac")
		args = append(args, "-compression_level", fmt.Sprintf("%d", opts.Quality))
	case FormatAAC:
		args = append(args, "-c:a", "aac")
		args = append(args, "-b:a", fmt.Sprintf("%dk", 96+(32*opts.Quality))) // Scale bitrate with quality
	case FormatOGG:
		args = append(args, "-c:a", "libvorbis")
		args = append(args, "-q:a", fmt.Sprintf("%d", opts.Quality))
	default:
		return fmt.Errorf("unsupported format: %s", opts.Format)
	}
	// Apply filters if any
	if len(filters) > 0 {
		args = append(args, "-af", strings.Join(filters, ","))
	}
	// Add output path
	args = append(args, outputPath)
	// Run FFmpeg command
	cmd := exec.CommandContext(ctx, p.ffmpegPath, args...)
	if err := cmd.Run(); err != nil {
		return fmt.Errorf("FFmpeg processing failed: %w", err)
	}
	return nil
}
</file>

<file path="internal/pkg/audio/validator.go">
package audio
import (
	"bytes"
	"fmt"
	"io"
	"path/filepath"
	"github.com/dhowden/tag"
)
// SupportedFormats defines the supported audio file formats and their MIME types
var SupportedFormats = map[string][]string{
	".mp3":  {"audio/mpeg", "audio/mp3"},
	".wav":  {"audio/wav", "audio/x-wav", "audio/wave"},
	".flac": {"audio/flac", "audio/x-flac"},
	".m4a":  {"audio/mp4", "audio/x-m4a"},
	".ogg":  {"audio/ogg", "application/ogg"},
	".aac":  {"audio/aac", "audio/aacp"},
}
// MaxFileSize defines the maximum allowed file size (100MB)
const MaxFileSize = 100 * 1024 * 1024
// ValidationError represents an audio file validation error
type ValidationError struct {
	Code    string
	Message string
}
func (e *ValidationError) Error() string {
	return fmt.Sprintf("%s: %s", e.Code, e.Message)
}
// Common validation error codes
const (
	ErrUnsupportedFormat = "UNSUPPORTED_FORMAT"
	ErrFileTooLarge      = "FILE_TOO_LARGE"
	ErrInvalidContent    = "INVALID_CONTENT"
	ErrCorruptMetadata   = "CORRUPT_METADATA"
)
// ValidateFormat checks if the file extension and MIME type are supported
func ValidateFormat(filename, mimeType string) error {
	ext := filepath.Ext(filename)
	if ext == "" {
		return &ValidationError{
			Code:    ErrUnsupportedFormat,
			Message: "File has no extension",
		}
	}
	validMimeTypes, supported := SupportedFormats[ext]
	if !supported {
		return &ValidationError{
			Code:    ErrUnsupportedFormat,
			Message: fmt.Sprintf("Unsupported file format: %s", ext),
		}
	}
	// Check MIME type if provided
	if mimeType != "" {
		mimeValid := false
		for _, validType := range validMimeTypes {
			if mimeType == validType {
				mimeValid = true
				break
			}
		}
		if !mimeValid {
			return &ValidationError{
				Code:    ErrUnsupportedFormat,
				Message: fmt.Sprintf("Invalid MIME type for %s: %s", ext, mimeType),
			}
		}
	}
	return nil
}
// ValidateContent performs content-based validation of the audio file
func ValidateContent(content io.Reader, size int64) error {
	if size > MaxFileSize {
		return &ValidationError{
			Code:    ErrFileTooLarge,
			Message: fmt.Sprintf("File size exceeds maximum allowed size of %d bytes", MaxFileSize),
		}
	}
	// Read a sample of the file to validate format
	sample := make([]byte, 512)
	n, err := content.Read(sample)
	if err != nil && err != io.EOF {
		return &ValidationError{
			Code:    ErrInvalidContent,
			Message: "Failed to read file content",
		}
	}
	// Try to parse metadata to validate file integrity
	if _, err := tag.ReadFrom(bytes.NewReader(sample[:n])); err != nil {
		return &ValidationError{
			Code:    ErrCorruptMetadata,
			Message: "Failed to read audio metadata",
		}
	}
	return nil
}
// ExtractMetadata extracts metadata from the audio file
func ExtractMetadata(content io.Reader) (tag.Metadata, error) {
	// Read the entire content into a buffer to make it seekable
	data, err := io.ReadAll(content)
	if err != nil {
		return nil, &ValidationError{
			Code:    ErrInvalidContent,
			Message: fmt.Sprintf("Failed to read content: %v", err),
		}
	}
	// Create a seekable reader from the buffer
	reader := bytes.NewReader(data)
	metadata, err := tag.ReadFrom(reader)
	if err != nil {
		return nil, &ValidationError{
			Code:    ErrCorruptMetadata,
			Message: fmt.Sprintf("Failed to extract metadata: %v", err),
		}
	}
	return metadata, nil
}
</file>

<file path="internal/pkg/config/app_config.go">
package config
import (
	"fmt"
	"os"
	"strconv"
	"strings"
	"time"
)
// AppConfig holds all application configuration settings
type AppConfig struct {
	Server   ServerConfig   `json:"server"`
	Database DatabaseConfig `json:"database"`
	Redis    RedisConfig    `json:"redis"`
	Auth     AuthConfig     `json:"auth"`
	AI       AIConfig       `json:"ai"`
	Storage  StorageConfig  `json:"storage"`
	Session  SessionConfig  `json:"session"`
	Tracing  TracingConfig  `json:"tracing"`
	Jobs     JobsConfig     `json:"jobs"`
	Sentry   SentryConfig   `json:"sentry"`
	Queue    QueueConfig    `json:"queue"`
}
// ServerConfig holds server-related settings
type ServerConfig struct {
	Port        int    `json:"port"`
	Environment string `json:"environment"`
	LogLevel    string `json:"log_level"`
	Address     string `json:"address"`
}
// DatabaseConfig holds database connection settings
type DatabaseConfig struct {
	Host     string `json:"host"`
	Port     int    `json:"port"`
	User     string `json:"user"`
	Password string `json:"password"`
	DBName   string `json:"dbname"`
	SSLMode  string `json:"sslmode"`
}
// RedisConfig holds Redis connection settings
type RedisConfig struct {
	Enabled  bool   `json:"enabled"`
	Host     string `json:"host"`
	Port     int    `json:"port"`
	Password string `json:"password"`
	DB       int    `json:"db"`
}
// GetAddress returns the formatted Redis address
func (c *RedisConfig) GetAddress() string {
	return fmt.Sprintf("%s:%d", c.Host, c.Port)
}
// AuthConfig holds authentication settings
type AuthConfig struct {
	JWTSecret           string        `json:"jwt_secret"`
	AccessTokenTTL      time.Duration `json:"access_token_ttl"`
	RefreshTokenTTL     time.Duration `json:"refresh_token_ttl"`
	APIKeyLength        int           `json:"api_key_length"`
	PasswordMinLength   int           `json:"password_min_length"`
	PasswordHashCost    int           `json:"password_hash_cost"`
	MaxLoginAttempts    int           `json:"max_login_attempts"`
	LockoutDuration     time.Duration `json:"lockout_duration"`
	SessionTimeout      time.Duration `json:"session_timeout"`
	EnableTwoFactor     bool          `json:"enable_two_factor"`
	RequireStrongPasswd bool          `json:"require_strong_password"`
}
// AIConfig holds AI service settings
type AIConfig struct {
	Provider      string           `json:"provider"`
	APIKey        string           `json:"api_key"`
	ModelName     string           `json:"model_name"`
	ModelVersion  string           `json:"model_version"`
	Temperature   float64          `json:"temperature"`
	MaxTokens     int              `json:"max_tokens"`
	BatchSize     int              `json:"batch_size"`
	MinConfidence float64          `json:"min_confidence"`
	BaseURL       string           `json:"base_url"`
	Timeout       time.Duration    `json:"timeout"`
	Experiment    ExperimentConfig `json:"experiment"`
}
// ExperimentConfig holds A/B testing configuration
type ExperimentConfig struct {
	TrafficPercent float64 `json:"traffic_percent"`
	MinConfidence  float64 `json:"min_confidence"`
	EnableFallback bool    `json:"enable_fallback"`
}
// SessionConfig holds session management settings
type SessionConfig struct {
	CookieName         string        `json:"cookie_name"`
	CookieDomain       string        `json:"cookie_domain"`
	CookiePath         string        `json:"cookie_path"`
	CookieSecure       bool          `json:"cookie_secure"`
	CookieHTTPOnly     bool          `json:"cookie_http_only"`
	CookieSameSite     string        `json:"cookie_same_site"`
	SessionDuration    time.Duration `json:"session_duration"`
	CleanupInterval    time.Duration `json:"cleanup_interval"`
	MaxSessionsPerUser int           `json:"max_sessions_per_user"`
}
// TracingConfig holds tracing configuration settings
type TracingConfig struct {
	Enabled     bool    `json:"enabled"`
	ServiceName string  `json:"service_name"`
	Endpoint    string  `json:"endpoint"`
	SampleRate  float64 `json:"sample_rate"`
}
// JobsConfig holds background job settings
type JobsConfig struct {
	NumWorkers        int           `json:"num_workers"`
	MaxConcurrent     int           `json:"max_concurrent"`
	PollInterval      time.Duration `json:"poll_interval"`
	ShutdownWait      time.Duration `json:"shutdown_wait"`
	DefaultMaxRetries int           `json:"default_max_retries"`
	DefaultTTL        time.Duration `json:"default_ttl"`
	MaxPayloadSize    int64         `json:"max_payload_size"`
	QueuePrefix       string        `json:"queue_prefix"`
	RetryDelay        time.Duration `json:"retry_delay"`
	MaxRetryDelay     time.Duration `json:"max_retry_delay"`
	RetryMultiplier   float64       `json:"retry_multiplier"`
	CleanupInterval   time.Duration `json:"cleanup_interval"`
	MaxJobAge         time.Duration `json:"max_job_age"`
}
// StorageConfig holds storage service settings
type StorageConfig struct {
	Provider         string        `json:"provider"`
	Region           string        `json:"region"`
	Bucket           string        `json:"bucket"`
	AccessKey        string        `json:"access_key"`
	SecretKey        string        `json:"secret_key"`
	Endpoint         string        `json:"endpoint"`
	UseSSL           bool          `json:"use_ssl"`
	UploadPartSize   int64         `json:"upload_part_size"`
	MaxUploadRetries int           `json:"max_upload_retries"`
	MaxFileSize      int64         `json:"max_file_size"`
	AllowedFileTypes []string      `json:"allowed_file_types"`
	UserQuota        int64         `json:"user_quota"`
	TotalQuota       int64         `json:"total_quota"`
	QuotaWarningPct  int           `json:"quota_warning_pct"`
	TempFileExpiry   time.Duration `json:"temp_file_expiry"`
	CleanupInterval  time.Duration `json:"cleanup_interval"`
	UploadBufferSize int64         `json:"upload_buffer_size"`
	DownloadTimeout  time.Duration `json:"download_timeout"`
	UploadTimeout    time.Duration `json:"upload_timeout"`
}
// SentryConfig holds Sentry error tracking configuration
type SentryConfig struct {
	DSN              string  `json:"dsn" env:"SENTRY_DSN"`
	Environment      string  `json:"environment" env:"SENTRY_ENVIRONMENT" envDefault:"development"`
	Debug            bool    `json:"debug" env:"SENTRY_DEBUG" envDefault:"false"`
	SampleRate       float64 `json:"sample_rate" env:"SENTRY_SAMPLE_RATE" envDefault:"1.0"`
	TracesSampleRate float64 `json:"traces_sample_rate" env:"SENTRY_TRACES_SAMPLE_RATE" envDefault:"0.2"`
}
// QueueConfig holds queue service configuration
type QueueConfig struct {
	ProjectID          string        `json:"project_id" env:"PUBSUB_PROJECT_ID,required"`
	HighPriorityTopic  string        `json:"high_priority_topic" env:"PUBSUB_HIGH_PRIORITY_TOPIC" envDefault:"high-priority"`
	LowPriorityTopic   string        `json:"low_priority_topic" env:"PUBSUB_LOW_PRIORITY_TOPIC" envDefault:"low-priority"`
	DeadLetterTopic    string        `json:"dead_letter_topic" env:"PUBSUB_DEAD_LETTER_TOPIC" envDefault:"dead-letter"`
	SubscriptionPrefix string        `json:"subscription_prefix" env:"PUBSUB_SUBSCRIPTION_PREFIX" envDefault:"sub"`
	MaxRetries         int           `json:"max_retries" env:"PUBSUB_MAX_RETRIES" envDefault:"3"`
	AckDeadline        time.Duration `json:"ack_deadline" env:"PUBSUB_ACK_DEADLINE" envDefault:"30s"`
	RetentionDuration  time.Duration `json:"retention_duration" env:"PUBSUB_RETENTION" envDefault:"168h"`
}
// Load loads configuration from environment variables
func Load() (*AppConfig, error) {
	cfg := &AppConfig{
		Server: ServerConfig{
			Port:        getEnvAsInt("SERVER_PORT", 8080),
			Environment: getEnvOrDefault("ENVIRONMENT", "development"),
			LogLevel:    getEnvOrDefault("LOG_LEVEL", "info"),
			Address:     getEnvOrDefault("SERVER_ADDRESS", ""),
		},
		Database: DatabaseConfig{
			Host:     getEnvOrDefault("DB_HOST", "localhost"),
			Port:     getEnvAsInt("DB_PORT", 5432),
			User:     getEnvOrDefault("DB_USER", "postgres"),
			Password: getEnvOrDefault("DB_PASSWORD", ""),
			DBName:   getEnvOrDefault("DB_NAME", "metadatatool"),
			SSLMode:  getEnvOrDefault("DB_SSLMODE", "disable"),
		},
		Redis: RedisConfig{
			Enabled:  getEnvAsBool("REDIS_ENABLED", false),
			Host:     getEnvOrDefault("REDIS_HOST", "localhost"),
			Port:     getEnvAsInt("REDIS_PORT", 6379),
			Password: getEnvOrDefault("REDIS_PASSWORD", ""),
			DB:       getEnvAsInt("REDIS_DB", 0),
		},
		Auth: AuthConfig{
			JWTSecret:           getEnvOrDefault("JWT_SECRET", "your-secret-key"),
			AccessTokenTTL:      getEnvAsDuration("ACCESS_TOKEN_TTL", 15*time.Minute),
			RefreshTokenTTL:     getEnvAsDuration("REFRESH_TOKEN_TTL", 7*24*time.Hour),
			APIKeyLength:        getEnvAsInt("API_KEY_LENGTH", 32),
			PasswordMinLength:   getEnvAsInt("PASSWORD_MIN_LENGTH", 8),
			PasswordHashCost:    getEnvAsInt("PASSWORD_HASH_COST", 10),
			MaxLoginAttempts:    getEnvAsInt("MAX_LOGIN_ATTEMPTS", 5),
			LockoutDuration:     getEnvAsDuration("LOCKOUT_DURATION", 15*time.Minute),
			SessionTimeout:      getEnvAsDuration("SESSION_TIMEOUT", 24*time.Hour),
			EnableTwoFactor:     getEnvAsBool("ENABLE_TWO_FACTOR", false),
			RequireStrongPasswd: getEnvAsBool("REQUIRE_STRONG_PASSWORD", true),
		},
		AI: AIConfig{
			Provider:      getEnvOrDefault("AI_PROVIDER", "openai"),
			ModelName:     getEnvOrDefault("AI_MODEL_NAME", "gpt-4"),
			ModelVersion:  getEnvOrDefault("AI_MODEL_VERSION", "latest"),
			Temperature:   getEnvAsFloat("AI_TEMPERATURE", 0.7),
			MaxTokens:     getEnvAsInt("AI_MAX_TOKENS", 2048),
			BatchSize:     getEnvAsInt("AI_BATCH_SIZE", 10),
			MinConfidence: getEnvAsFloat("AI_MIN_CONFIDENCE", 0.85),
			APIKey:        getEnvOrDefault("AI_API_KEY", ""),
			BaseURL:       getEnvOrDefault("AI_BASE_URL", "https://api.openai.com/v1"),
			Timeout:       getEnvAsDuration("AI_TIMEOUT", 30*time.Second),
			Experiment: ExperimentConfig{
				TrafficPercent: getEnvAsFloat("AI_EXPERIMENT_TRAFFIC_PERCENT", 0.1),
				MinConfidence:  getEnvAsFloat("AI_MIN_CONFIDENCE_THRESHOLD", 0.8),
				EnableFallback: getEnvAsBool("AI_ENABLE_AUTO_FALLBACK", true),
			},
		},
		Session: SessionConfig{
			CookieName:         getEnvOrDefault("SESSION_COOKIE_NAME", "session"),
			CookieDomain:       getEnvOrDefault("SESSION_COOKIE_DOMAIN", ""),
			CookiePath:         getEnvOrDefault("SESSION_COOKIE_PATH", "/"),
			CookieSecure:       getEnvAsBool("SESSION_COOKIE_SECURE", true),
			CookieHTTPOnly:     getEnvAsBool("SESSION_COOKIE_HTTP_ONLY", true),
			CookieSameSite:     getEnvOrDefault("SESSION_COOKIE_SAME_SITE", "lax"),
			SessionDuration:    getEnvAsDuration("SESSION_DURATION", 24*time.Hour),
			CleanupInterval:    getEnvAsDuration("SESSION_CLEANUP_INTERVAL", time.Hour),
			MaxSessionsPerUser: getEnvAsInt("SESSION_MAX_PER_USER", 5),
		},
		Jobs: JobsConfig{
			NumWorkers:        getEnvAsInt("JOB_NUM_WORKERS", 5),
			MaxConcurrent:     getEnvAsInt("JOB_MAX_CONCURRENT", 10),
			PollInterval:      getEnvAsDuration("JOB_POLL_INTERVAL", time.Second),
			ShutdownWait:      getEnvAsDuration("JOB_SHUTDOWN_WAIT", 30*time.Second),
			DefaultMaxRetries: getEnvAsInt("JOB_DEFAULT_MAX_RETRIES", 3),
			DefaultTTL:        getEnvAsDuration("JOB_DEFAULT_TTL", 24*time.Hour),
			MaxPayloadSize:    getEnvAsInt64("JOB_MAX_PAYLOAD_SIZE", 1024*1024),
			QueuePrefix:       getEnvOrDefault("JOB_QUEUE_PREFIX", "jobs:"),
			RetryDelay:        getEnvAsDuration("JOB_RETRY_DELAY", 5*time.Second),
			MaxRetryDelay:     getEnvAsDuration("JOB_MAX_RETRY_DELAY", time.Hour),
			RetryMultiplier:   getEnvAsFloat("JOB_RETRY_MULTIPLIER", 2.0),
			CleanupInterval:   getEnvAsDuration("JOB_CLEANUP_INTERVAL", time.Hour),
			MaxJobAge:         getEnvAsDuration("JOB_MAX_AGE", 7*24*time.Hour),
		},
		Storage: StorageConfig{
			Provider:         getEnvOrDefault("STORAGE_PROVIDER", "s3"),
			Region:           getEnvOrDefault("STORAGE_REGION", "us-east-1"),
			Bucket:           getEnvOrDefault("STORAGE_BUCKET", "metadatatool"),
			AccessKey:        getEnvOrDefault("STORAGE_ACCESS_KEY", ""),
			SecretKey:        getEnvOrDefault("STORAGE_SECRET_KEY", ""),
			Endpoint:         getEnvOrDefault("STORAGE_ENDPOINT", ""),
			UseSSL:           getEnvAsBool("STORAGE_USE_SSL", true),
			UploadPartSize:   getEnvAsInt64("STORAGE_UPLOAD_PART_SIZE", 5*1024*1024),
			MaxUploadRetries: getEnvAsInt("STORAGE_MAX_UPLOAD_RETRIES", 3),
			MaxFileSize:      getEnvAsInt64("STORAGE_MAX_FILE_SIZE", 100*1024*1024),
			AllowedFileTypes: strings.Split(getEnvOrDefault("STORAGE_ALLOWED_FILE_TYPES", ".mp3,.wav,.flac"), ","),
			UserQuota:        getEnvAsInt64("STORAGE_USER_QUOTA", 1024*1024*1024),
			TotalQuota:       getEnvAsInt64("STORAGE_TOTAL_QUOTA", 1024*1024*1024*1024),
			QuotaWarningPct:  getEnvAsInt("STORAGE_QUOTA_WARNING_PCT", 90),
			TempFileExpiry:   getEnvAsDuration("STORAGE_TEMP_FILE_EXPIRY", 24*time.Hour),
			CleanupInterval:  getEnvAsDuration("STORAGE_CLEANUP_INTERVAL", time.Hour),
			UploadBufferSize: getEnvAsInt64("STORAGE_UPLOAD_BUFFER_SIZE", 5*1024*1024),
			DownloadTimeout:  getEnvAsDuration("STORAGE_DOWNLOAD_TIMEOUT", 5*time.Minute),
			UploadTimeout:    getEnvAsDuration("STORAGE_UPLOAD_TIMEOUT", 10*time.Minute),
		},
		Tracing: TracingConfig{
			Enabled:     getEnvAsBool("TRACING_ENABLED", true),
			ServiceName: getEnvOrDefault("TRACING_SERVICE_NAME", "metadatatool"),
			Endpoint:    getEnvOrDefault("TRACING_ENDPOINT", "localhost:4317"),
			SampleRate:  getEnvAsFloat("TRACING_SAMPLE_RATE", 0.1),
		},
		Sentry: SentryConfig{
			DSN:              getEnvOrDefault("SENTRY_DSN", ""),
			Environment:      getEnvOrDefault("SENTRY_ENVIRONMENT", "development"),
			Debug:            getEnvAsBool("SENTRY_DEBUG", false),
			SampleRate:       getEnvAsFloat("SENTRY_SAMPLE_RATE", 1.0),
			TracesSampleRate: getEnvAsFloat("SENTRY_TRACES_SAMPLE_RATE", 0.2),
		},
		Queue: QueueConfig{
			ProjectID:          getEnvOrDefault("PUBSUB_PROJECT_ID", ""),
			HighPriorityTopic:  getEnvOrDefault("PUBSUB_HIGH_PRIORITY_TOPIC", "high-priority"),
			LowPriorityTopic:   getEnvOrDefault("PUBSUB_LOW_PRIORITY_TOPIC", "low-priority"),
			DeadLetterTopic:    getEnvOrDefault("PUBSUB_DEAD_LETTER_TOPIC", "dead-letter"),
			SubscriptionPrefix: getEnvOrDefault("PUBSUB_SUBSCRIPTION_PREFIX", "sub"),
			MaxRetries:         getEnvAsInt("PUBSUB_MAX_RETRIES", 3),
			AckDeadline:        getEnvAsDuration("PUBSUB_ACK_DEADLINE", 30*time.Second),
			RetentionDuration:  getEnvAsDuration("PUBSUB_RETENTION", 168*time.Hour),
		},
	}
	return cfg, nil
}
// Helper functions to get environment variables with defaults
func getEnvOrDefault(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}
func getEnvAsInt(key string, defaultValue int) int {
	if value := os.Getenv(key); value != "" {
		if intValue, err := strconv.Atoi(value); err == nil {
			return intValue
		}
	}
	return defaultValue
}
func getEnvAsInt64(key string, defaultValue int64) int64 {
	if value := os.Getenv(key); value != "" {
		if intValue, err := strconv.ParseInt(value, 10, 64); err == nil {
			return intValue
		}
	}
	return defaultValue
}
func getEnvAsFloat(key string, defaultValue float64) float64 {
	if value := os.Getenv(key); value != "" {
		if floatValue, err := strconv.ParseFloat(value, 64); err == nil {
			return floatValue
		}
	}
	return defaultValue
}
func getEnvAsBool(key string, defaultValue bool) bool {
	if value := os.Getenv(key); value != "" {
		if boolValue, err := strconv.ParseBool(value); err == nil {
			return boolValue
		}
	}
	return defaultValue
}
func getEnvAsDuration(key string, defaultValue time.Duration) time.Duration {
	if value := os.Getenv(key); value != "" {
		if duration, err := time.ParseDuration(value); err == nil {
			return duration
		}
	}
	return defaultValue
}
</file>

<file path="internal/pkg/database/postgres.go">
package database
import (
	"fmt"
	"os"
	"gorm.io/driver/postgres"
	"gorm.io/gorm"
)
// ConnectPostgres establishes a connection to PostgreSQL
func ConnectPostgres() (*gorm.DB, error) {
	dsn := fmt.Sprintf("host=%s user=%s password=%s dbname=%s port=%s sslmode=disable",
		os.Getenv("DB_HOST"),
		os.Getenv("DB_USER"),
		os.Getenv("DB_PASSWORD"),
		os.Getenv("DB_NAME"),
		os.Getenv("DB_PORT"),
	)
	db, err := gorm.Open(postgres.Open(dsn), &gorm.Config{})
	if err != nil {
		return nil, fmt.Errorf("failed to connect to database: %w", err)
	}
	return db, nil
}
</file>

<file path="internal/pkg/ddex/schema_validator.go">
package ddex
import (
	"encoding/xml"
	"fmt"
	"strings"
	"github.com/beevik/etree"
)
// XMLSchemaValidator implements the SchemaValidator interface for XML schema validation
type XMLSchemaValidator struct {
	cachedDocs map[string]*etree.Document
}
// NewXMLSchemaValidator creates a new XMLSchemaValidator instance
func NewXMLSchemaValidator() *XMLSchemaValidator {
	return &XMLSchemaValidator{
		cachedDocs: make(map[string]*etree.Document),
	}
}
// ValidateAgainstSchema validates XML data against basic XML rules and structure
func (v *XMLSchemaValidator) ValidateAgainstSchema(xmlData []byte, schemaPath string) error {
	// First, validate that it's well-formed XML
	if err := xml.Unmarshal(xmlData, new(interface{})); err != nil {
		return fmt.Errorf("malformed XML: %w", err)
	}
	// Load and parse the XML document using etree
	doc := etree.NewDocument()
	if err := doc.ReadFromBytes(xmlData); err != nil {
		return fmt.Errorf("failed to parse XML document: %w", err)
	}
	// Basic structural validation
	root := doc.Root()
	if root == nil {
		return fmt.Errorf("XML document has no root element")
	}
	// Validate required elements based on DDEX ERN 4.3 schema
	if err := v.validateRequiredElements(root); err != nil {
		return fmt.Errorf("DDEX validation failed: %w", err)
	}
	return nil
}
// validateRequiredElements checks for required DDEX ERN elements
func (v *XMLSchemaValidator) validateRequiredElements(root *etree.Element) error {
	// Check root element name
	if root.Tag != "ernMessage" {
		return fmt.Errorf("root element must be 'ernMessage', got '%s'", root.Tag)
	}
	// Required elements
	required := []string{
		"messageHeader/messageId",
		"messageHeader/messageSender",
		"messageHeader/messageRecipient",
		"messageHeader/messageCreatedDateTime",
		"resourceList",
		"releaseList",
		"dealList",
	}
	for _, path := range required {
		elements := root.FindElements(path)
		if len(elements) == 0 {
			return fmt.Errorf("missing required element: %s", path)
		}
	}
	// Validate sound recordings
	soundRecordings := root.FindElements("resourceList/soundRecording")
	for i, sr := range soundRecordings {
		if err := v.validateSoundRecording(sr, i); err != nil {
			return err
		}
	}
	// Validate releases
	releases := root.FindElements("releaseList/release")
	for i, r := range releases {
		if err := v.validateRelease(r, i); err != nil {
			return err
		}
	}
	// Validate deals
	deals := root.FindElements("dealList/releaseDeal")
	for i, d := range deals {
		if err := v.validateDeal(d, i); err != nil {
			return err
		}
	}
	return nil
}
// validateSoundRecording validates a single sound recording element
func (v *XMLSchemaValidator) validateSoundRecording(sr *etree.Element, index int) error {
	required := []string{
		"isrc",
		"title/titleText",
		"duration",
		"technicalDetails/technicalResourceDetailsReference",
		"soundRecordingType",
		"resourceReference",
	}
	for _, path := range required {
		element := sr.FindElement(path)
		if element == nil {
			return fmt.Errorf("sound recording %d: missing required element: %s", index+1, path)
		}
		// Validate non-empty values
		if strings.TrimSpace(element.Text()) == "" {
			return fmt.Errorf("sound recording %d: empty value for required element: %s", index+1, path)
		}
	}
	return nil
}
// validateRelease validates a single release element
func (v *XMLSchemaValidator) validateRelease(r *etree.Element, index int) error {
	required := []string{
		"releaseId/icpn",
		"referenceTitle/titleText",
		"releaseType",
	}
	for _, path := range required {
		element := r.FindElement(path)
		if element == nil {
			return fmt.Errorf("release %d: missing required element: %s", index+1, path)
		}
		// Validate non-empty values
		if strings.TrimSpace(element.Text()) == "" {
			return fmt.Errorf("release %d: empty value for required element: %s", index+1, path)
		}
	}
	return nil
}
// validateDeal validates a single deal element
func (v *XMLSchemaValidator) validateDeal(d *etree.Element, index int) error {
	required := []string{
		"dealReleaseReference",
		"deal/territory/territoryCode",
		"deal/dealTerms/commercialModelType",
		"deal/dealTerms/usage/useType",
	}
	for _, path := range required {
		element := d.FindElement(path)
		if element == nil {
			return fmt.Errorf("deal %d: missing required element: %s", index+1, path)
		}
		// Validate non-empty values
		if strings.TrimSpace(element.Text()) == "" {
			return fmt.Errorf("deal %d: empty value for required element: %s", index+1, path)
		}
	}
	return nil
}
// ClearCache clears the document cache
func (v *XMLSchemaValidator) ClearCache() {
	v.cachedDocs = make(map[string]*etree.Document)
}
</file>

<file path="internal/pkg/domain/ai.go">
package domain
import (
	"context"
	"time"
)
// AIService defines the interface for AI-powered metadata enrichment
type AIService interface {
	// EnrichMetadata enriches a track with AI-generated metadata
	EnrichMetadata(ctx context.Context, track *Track) error
	// ValidateMetadata validates track metadata using AI
	ValidateMetadata(ctx context.Context, track *Track) (float64, error)
	// BatchProcess processes multiple tracks in batch
	BatchProcess(ctx context.Context, tracks []*Track) error
}
// AIProvider represents the type of AI service
type AIProvider string
const (
	AIProviderQwen2  AIProvider = "qwen2"
	AIProviderOpenAI AIProvider = "openai"
)
// AIServiceConfig holds configuration for all AI services
type AIServiceConfig struct {
	EnableFallback   bool
	Qwen2Config      *Qwen2Config
	OpenAIConfig     *OpenAIConfig
	ExperimentConfig *ExperimentConfig
}
// ExperimentConfig holds A/B testing configuration
type ExperimentConfig struct {
	ExperimentTrafficPercent float64 // Percentage of traffic to route to experiment (0-1)
	MinConfidenceThreshold   float64 // Minimum confidence threshold (0-1)
	EnableAutoFallback       bool    // Whether to automatically fallback on low confidence
	BigQueryProjectID        string  // GCP project ID for analytics
	BigQueryDataset          string  // BigQuery dataset for analytics
}
// Qwen2Config holds configuration for Qwen2-Audio service
type Qwen2Config struct {
	APIKey                string
	Endpoint              string
	TimeoutSeconds        int
	MinConfidence         float64
	MaxConcurrentRequests int
	RetryAttempts         int
	RetryBackoffSeconds   int
}
// OpenAIConfig holds configuration for OpenAI service
type OpenAIConfig struct {
	APIKey                string
	Endpoint              string
	TimeoutSeconds        int
	MinConfidence         float64
	MaxConcurrentRequests int
	RetryAttempts         int
	RetryBackoffSeconds   int
	RequestsPerSecond     int // Rate limit for OpenAI API requests
}
// AIMetadata holds AI-generated metadata for a track
type AIMetadata struct {
	Provider     AIProvider
	Energy       float64
	Danceability float64
	ProcessedAt  time.Time
	ProcessingMs int64
	NeedsReview  bool
	ReviewReason string
}
// AIMetrics holds metrics for an AI service
type AIMetrics struct {
	RequestCount   int64
	SuccessCount   int64
	FailureCount   int64
	LastSuccess    time.Time
	LastError      error
	AverageLatency time.Duration
}
// AIResult represents the result of an AI analysis
type AIResult struct {
	Metadata     *AIMetadata
	Error        error
	RetryCount   int
	Duration     time.Duration
	UsedFallback bool
}
// AIServiceFactory creates AI service instances
type AIServiceFactory interface {
	// CreateService creates an AI service based on the provider
	CreateService(provider AIProvider) (AIService, error)
	// GetDefaultService returns the default AI service
	GetDefaultService() AIService
}
// CompositeAIService defines the interface for managing multiple AI services
type CompositeAIService interface {
	AIService
	// SetPrimaryProvider sets the primary AI provider
	SetPrimaryProvider(provider AIProvider)
	// SetFallbackProvider sets the fallback AI provider
	SetFallbackProvider(provider AIProvider)
	// GetProviderMetrics returns metrics for each provider
	GetProviderMetrics() map[AIProvider]*AIMetrics
}
</file>

<file path="internal/pkg/domain/audio.go">
package domain
import (
	"context"
	"time"
)
// AudioFormat represents the format of an audio file
type AudioFormat string
const (
	AudioFormatMP3  AudioFormat = "mp3"
	AudioFormatWAV  AudioFormat = "wav"
	AudioFormatFLAC AudioFormat = "flac"
	AudioFormatM4A  AudioFormat = "m4a"
	AudioFormatAAC  AudioFormat = "aac"
	AudioFormatOGG  AudioFormat = "ogg"
)
// IsValid checks if the audio format is supported
func (f AudioFormat) IsValid() bool {
	switch f {
	case AudioFormatMP3, AudioFormatWAV, AudioFormatFLAC, AudioFormatM4A, AudioFormatAAC, AudioFormatOGG:
		return true
	default:
		return false
	}
}
// String returns the string representation of the audio format
func (f AudioFormat) String() string {
	return string(f)
}
// AudioMetadata represents metadata extracted from an audio file
type AudioMetadata struct {
	// Basic metadata
	Title    string `json:"title"`
	Artist   string `json:"artist"`
	Album    string `json:"album"`
	Year     int    `json:"year"`
	Duration int    `json:"duration"` // Duration in seconds
	ISRC     string `json:"isrc"`
	// Technical details
	Format     AudioFormat `json:"format"`
	Bitrate    int         `json:"bitrate"`    // Bitrate in kbps
	SampleRate int         `json:"sampleRate"` // Sample rate in Hz
	Channels   int         `json:"channels"`   // Number of audio channels
	// Musical attributes
	BPM float64 `json:"bpm"` // Beats per minute
	Key string  `json:"key"` // Musical key
	// Additional metadata
	Genre        string
	TrackNumber  int
	BitDepth     int
	FileSize     int64
	IsLossless   bool
	IsVariable   bool
	EncodingTool string
	Composer     string
	Publisher    string
	Copyright    string
	Lyrics       string
	CoverArt     []byte
	Comments     string
	Rating       int
	ReplayGain   float64
	ModifiedAt   time.Time
	EncodedAt    time.Time
	CustomTags   map[string]string
}
// AudioProcessOptions contains options for audio processing
type AudioProcessOptions struct {
	FilePath        string      // Path to the audio file
	Format          AudioFormat // Audio format
	AnalyzeAudio    bool        // Whether to perform audio analysis
	ExtractMetadata bool        // Whether to extract metadata
	StartTime       time.Time   // When the processing started
}
// AudioProcessor defines the interface for audio processing
type AudioProcessor interface {
	// Process processes an audio file with the given options
	Process(ctx context.Context, file *ProcessingAudioFile, options *AudioProcessOptions) (*AudioProcessResult, error)
}
// AudioProcessResult contains the results of audio processing
type AudioProcessResult struct {
	Metadata     *CompleteTrackMetadata // Complete track metadata
	Analysis     *AudioAnalysis         // Audio analysis results
	AnalyzerInfo string                 // Information about the analyzer used
}
// AudioAnalysis represents the results of audio analysis
type AudioAnalysis struct {
	// Temporal features
	BPM           float64
	TimeSignature string
	Key           string
	Mode          string
	Tempo         float64
	BeatsPerBar   int
	// Spectral features
	Loudness      float64
	Energy        float64
	Brightness    float64
	Timbre        float64
	SpectralFlux  float64
	SpectralRoll  float64
	SpectralSlope float64
	// Perceptual features
	Danceability float64
	Valence      float64
	Arousal      float64
	Complexity   float64
	Intensity    float64
	Mood         string
	// Segments and structure
	Segments     []AudioSegment
	Transitions  []float64
	SectionCount int
	// Analysis metadata
	AnalyzedAt   time.Time
	Duration     float64
	SampleCount  int64
	WindowSize   int
	HopSize      int
	SampleRate   int
	AnalyzerInfo string
}
// AudioSegment represents a segment in the audio file
type AudioSegment struct {
	Start      float64
	Duration   float64
	Loudness   float64
	Timbre     []float64
	Pitches    []float64
	Confidence float64
}
// AudioService handles audio file operations
type AudioService interface {
	// Process processes an audio file and returns the results
	Process(ctx context.Context, file *ProcessingAudioFile, options *AudioProcessOptions) (*AudioProcessResult, error)
	// Upload stores an audio file and returns its URL
	Upload(ctx context.Context, file *StorageFile) error
	// GetURL retrieves a pre-signed URL for an audio file
	GetURL(ctx context.Context, id string) (string, error)
	// Download downloads an audio file from storage
	Download(ctx context.Context, url string) (*StorageFile, error)
	// Delete deletes an audio file from storage
	Delete(ctx context.Context, url string) error
}
</file>

<file path="internal/pkg/domain/auth.go">
package domain
import (
	"github.com/golang-jwt/jwt/v5"
)
// Permission represents a specific action that can be performed
type Permission string
const (
	// Track-related permissions
	PermissionCreateTrack Permission = "track:create"
	PermissionReadTrack   Permission = "track:read"
	PermissionUpdateTrack Permission = "track:update"
	PermissionDeleteTrack Permission = "track:delete"
	// Metadata-related permissions
	PermissionEnrichMetadata Permission = "metadata:enrich"
	PermissionExportDDEX     Permission = "metadata:export_ddex"
	// User management permissions
	PermissionManageUsers Permission = "users:manage"
	PermissionManageRoles Permission = "roles:manage"
	// API key permissions
	PermissionManageAPIKeys Permission = "apikeys:manage"
)
// Role represents user authorization level
type Role string
const (
	RoleAdmin  Role = "admin"
	RoleUser   Role = "user"
	RoleGuest  Role = "guest"
	RoleSystem Role = "system"
)
// RolePermissions maps roles to their allowed permissions
var RolePermissions = map[Role][]Permission{
	RoleAdmin: {
		// Admin has all permissions
		PermissionCreateTrack, PermissionReadTrack, PermissionUpdateTrack, PermissionDeleteTrack,
		PermissionEnrichMetadata, PermissionExportDDEX,
		PermissionManageUsers, PermissionManageRoles,
		PermissionManageAPIKeys,
	},
	RoleUser: {
		// Regular user has basic track and metadata permissions
		PermissionCreateTrack, PermissionReadTrack, PermissionUpdateTrack,
		PermissionEnrichMetadata, PermissionExportDDEX,
	},
	RoleGuest: {
		// Guest can only read tracks
		PermissionReadTrack,
	},
	RoleSystem: {
		// System has all track and metadata permissions but no user management
		PermissionCreateTrack, PermissionReadTrack, PermissionUpdateTrack, PermissionDeleteTrack,
		PermissionEnrichMetadata, PermissionExportDDEX,
	},
}
// Claims represents JWT claims with added role information
type Claims struct {
	UserID      string       `json:"user_id"`
	Email       string       `json:"email"`
	Role        Role         `json:"role"`
	Permissions []Permission `json:"permissions"`
	jwt.RegisteredClaims
}
// TokenPair represents an access and refresh token pair
type TokenPair struct {
	AccessToken  string `json:"access_token"`
	RefreshToken string `json:"refresh_token"`
}
// AuthService handles authentication and authorization
type AuthService interface {
	// GenerateTokens creates a new pair of access and refresh tokens
	GenerateTokens(user *User) (*TokenPair, error)
	// ValidateToken validates and parses a JWT token
	ValidateToken(token string) (*Claims, error)
	// RefreshToken validates a refresh token and generates new token pair
	RefreshToken(refreshToken string) (*TokenPair, error)
	// HashPassword creates a bcrypt hash of the password
	HashPassword(password string) (string, error)
	// VerifyPassword checks if the provided password matches the hash
	VerifyPassword(hashedPassword, password string) error
	// GenerateAPIKey creates a new API key
	GenerateAPIKey() (string, error)
	// HasPermission checks if a role has a specific permission
	HasPermission(role Role, permission Permission) bool
	// GetPermissions returns all permissions for a role
	GetPermissions(role Role) []Permission
}
</file>

<file path="internal/pkg/domain/context.go">
package domain
import (
	"context"
)
type contextKey string
const (
	userContextKey contextKey = "user"
)
// WithUser adds a user to the context
func WithUser(ctx context.Context, user *User) context.Context {
	return context.WithValue(ctx, userContextKey, user)
}
// UserFromContext retrieves a user from the context
func UserFromContext(ctx context.Context) (*User, bool) {
	user, ok := ctx.Value(userContextKey).(*User)
	return user, ok
}
</file>

<file path="internal/pkg/domain/ddex.go">
package domain
import (
	"context"
	"encoding/xml"
)
// DDEX format version
const (
	DDEXERN43 = "4.3"
)
// DDEXService handles DDEX format operations
type DDEXService interface {
	// ValidateTrack validates a track's metadata against DDEX schema
	ValidateTrack(ctx context.Context, track *Track) (bool, []string)
	// ExportTrack exports a single track to DDEX format
	ExportTrack(ctx context.Context, track *Track) (string, error)
	// ExportTracks exports multiple tracks to DDEX format
	ExportTracks(ctx context.Context, tracks []*Track) (string, error)
}
// ERNMessage represents a DDEX ERN message
type ERNMessage struct {
	XMLName       xml.Name      `xml:"ernMessage"`
	MessageHeader MessageHeader `xml:"messageHeader"`
	ResourceList  ResourceList  `xml:"resourceList"`
	ReleaseList   ReleaseList   `xml:"releaseList"`
	DealList      DealList      `xml:"dealList"`
}
// MessageHeader represents the DDEX message header
type MessageHeader struct {
	MessageID              string `xml:"messageId"`
	MessageSender          string `xml:"messageSender"`
	MessageRecipient       string `xml:"messageRecipient"`
	MessageCreatedDateTime string `xml:"messageCreatedDateTime"`
}
// ResourceList represents a list of resources
type ResourceList struct {
	SoundRecordings []SoundRecording `xml:"soundRecording"`
}
// ReleaseList represents a list of releases
type ReleaseList struct {
	Releases []Release `xml:"release"`
}
// DealList represents a list of deals
type DealList struct {
	ReleaseDeals []ReleaseDeal `xml:"releaseDeal"`
}
// SoundRecording represents a DDEX sound recording
type SoundRecording struct {
	ISRC               string           `xml:"isrc"`
	Title              Title            `xml:"title"`
	Duration           string           `xml:"duration"`
	TechnicalDetails   TechnicalDetails `xml:"technicalDetails"`
	SoundRecordingType string           `xml:"soundRecordingType"`
	ResourceReference  string           `xml:"resourceReference"`
}
// Title represents a DDEX title
type Title struct {
	TitleText string `xml:"titleText"`
}
// TechnicalDetails represents technical details of a recording
type TechnicalDetails struct {
	TechnicalResourceDetailsReference string `xml:"technicalResourceDetailsReference"`
	Audio                             Audio  `xml:"audio"`
}
// Audio represents audio technical details
type Audio struct {
	Format     string `xml:"audioFormat"`
	BitRate    int    `xml:"bitRate"`
	SampleRate int    `xml:"sampleRate"`
}
// Release represents a DDEX release
type Release struct {
	ReleaseID      ReleaseID `xml:"releaseId"`
	ReferenceTitle Title     `xml:"referenceTitle"`
	ReleaseType    string    `xml:"releaseType"`
}
// ReleaseID represents a DDEX release ID
type ReleaseID struct {
	ICPN string `xml:"icpn"`
}
// ReleaseDeal represents a DDEX release deal
type ReleaseDeal struct {
	DealReleaseReference string `xml:"dealReleaseReference"`
	Deal                 Deal   `xml:"deal"`
}
// Deal represents a DDEX deal
type Deal struct {
	Territory Territory `xml:"territory"`
	DealTerms DealTerms `xml:"dealTerms"`
}
// Territory represents a DDEX territory
type Territory struct {
	TerritoryCode string `xml:"territoryCode"`
}
// DealTerms represents DDEX deal terms
type DealTerms struct {
	CommercialModelType string `xml:"commercialModelType"`
	Usage               Usage  `xml:"usage"`
}
// Usage represents DDEX usage terms
type Usage struct {
	UseType string `xml:"useType"`
}
</file>

<file path="internal/pkg/domain/errors.go">
package domain
import "errors"
var (
	// Authentication errors
	ErrInvalidCredentials = errors.New("invalid credentials")
	ErrInvalidToken       = errors.New("invalid token")
	ErrInvalidPassword    = errors.New("invalid password")
	ErrUnauthorized       = errors.New("unauthorized")
	ErrForbidden          = errors.New("forbidden")
	// User errors
	ErrUserNotFound = errors.New("user not found")
	ErrEmailExists  = errors.New("email already exists")
	// Session errors
	ErrSessionNotFound = errors.New("session not found")
	// Validation errors
	ErrInvalidInput = errors.New("invalid input")
	// System errors
	ErrInternal = errors.New("internal error")
)
</file>

<file path="internal/pkg/domain/experiment.go">
package domain
// ExperimentMetrics holds metrics for both control and experiment groups
type ExperimentMetrics struct {
	Control    *ModelMetrics
	Experiment *ModelMetrics
}
// ModelMetrics holds metrics for a single model
type ModelMetrics struct {
	TotalRequests     int64
	AvgProcessingTime float64
	AvgConfidence     float64
	SuccessRate       float64
}
</file>

<file path="internal/pkg/domain/file_types.go">
package domain
import (
	"context"
	"io"
	"time"
)
// ProcessingAudioFile represents an audio file being processed
type ProcessingAudioFile struct {
	Path     string      // Path to the file
	Name     string      // Original filename
	Size     int64       // File size in bytes
	Format   AudioFormat // Audio format
	Reader   io.Reader   // File reader (optional)
	Content  io.Reader   // File content for processing
	Metadata *AudioMetadata
}
// StorageFile represents a file to be stored
type StorageFile struct {
	Key         string    // Unique identifier/path in storage
	Name        string    // Original filename
	Size        int64     // File size in bytes
	ContentType string    // MIME type
	Content     io.Reader // File content
	Metadata    map[string]string
	UploadedAt  time.Time
}
// FileMetadata represents metadata for a stored file
type FileMetadata struct {
	Key          string
	Name         string
	Size         int64
	ContentType  string
	UploadedAt   time.Time
	LastModified time.Time
	ETag         string
	StorageClass string
	Metadata     map[string]string
}
// StorageService defines the interface for storage operations
type StorageService interface {
	// Core file operations
	Upload(ctx context.Context, file *StorageFile) error
	Download(ctx context.Context, key string) (*StorageFile, error)
	Delete(ctx context.Context, key string) error
	GetURL(ctx context.Context, key string) (string, error)
	GetMetadata(ctx context.Context, key string) (*FileMetadata, error)
	ListFiles(ctx context.Context, prefix string) ([]*FileMetadata, error)
	// Audio-specific operations
	UploadAudio(ctx context.Context, file io.Reader, path string) error
	DeleteAudio(ctx context.Context, path string) error
	GetSignedURL(ctx context.Context, path string, expiry time.Duration) (string, error)
	// Quota and validation
	GetQuotaUsage(ctx context.Context) (int64, error)
	ValidateUpload(ctx context.Context, fileSize int64, mimeType string) error
}
// StorageClient defines the interface for low-level storage operations
type StorageClient interface {
	// Upload uploads a file to storage
	Upload(ctx context.Context, key string, content io.Reader, options map[string]string) error
	// Download downloads a file from storage
	Download(ctx context.Context, key string) (io.ReadCloser, error)
	// Delete deletes a file from storage
	Delete(ctx context.Context, key string) error
	// GetURL gets a pre-signed URL for a file
	GetURL(ctx context.Context, key string, operation SignedURLOperation) (string, error)
	// GetMetadata gets metadata for a file
	GetMetadata(ctx context.Context, key string) (*FileMetadata, error)
	// List lists files with a prefix
	List(ctx context.Context, prefix string) ([]*FileMetadata, error)
}
</file>

<file path="internal/pkg/domain/graphql_input.go">
package domain
import "io"
// CreateTrackInput represents input for creating a new track
type CreateTrackInput struct {
	Title     string
	Artist    string
	Album     *string
	Genre     *string
	Year      *int
	Label     *string
	Territory *string
	ISRC      *string
	ISWC      *string
	AudioFile *AudioFile
}
// UpdateTrackInput represents input for updating a track
type UpdateTrackInput struct {
	ID        string
	Title     *string
	Artist    *string
	Album     *string
	Genre     *string
	Year      *int
	Label     *string
	Territory *string
	ISRC      *string
	ISWC      *string
	Metadata  *MetadataInput
}
// MetadataInput represents input for updating track metadata
type MetadataInput struct {
	ISRC         *string
	ISWC         *string
	BPM          *float64
	Key          *string
	Mood         *string
	Labels       []string
	CustomFields map[string]string
}
// AudioFile represents an uploaded audio file
type AudioFile struct {
	File     io.Reader
	Filename string
	Size     int64
}
</file>

<file path="internal/pkg/domain/graphql.go">
package domain
import "time"
// TrackConnection represents a paginated connection of tracks
type TrackConnection struct {
	Edges      []*TrackEdge
	PageInfo   *PageInfo
	TotalCount int
}
// TrackEdge represents a single edge in a track connection
type TrackEdge struct {
	Node   *Track
	Cursor string
}
// PageInfo contains information about pagination
type PageInfo struct {
	HasNextPage     bool
	HasPreviousPage bool
	StartCursor     string
	EndCursor       string
}
// TrackFilter represents filter options for track queries
type TrackFilter struct {
	Title       *string
	Artist      *string
	Album       *string
	Genre       *string
	Label       *string
	ISRC        *string
	ISWC        *string
	NeedsReview *bool
	CreatedFrom *time.Time
	CreatedTo   *time.Time
}
// BatchResult represents the result of a batch operation
type BatchResult struct {
	SuccessCount int
	FailureCount int
	Errors       []*BatchError
}
// BatchError represents an error in a batch operation
type BatchError struct {
	TrackID string
	Message string
	Code    string
}
</file>

<file path="internal/pkg/domain/job.go">
package domain
import (
	"context"
	"encoding/json"
	"time"
)
// JobStatus represents the current state of a job
type JobStatus string
const (
	JobStatusPending    JobStatus = "pending"
	JobStatusProcessing JobStatus = "processing"
	JobStatusCompleted  JobStatus = "completed"
	JobStatusFailed     JobStatus = "failed"
	JobStatusCanceled   JobStatus = "canceled"
)
// JobPriority represents the priority level of a job
type JobPriority int
const (
	JobPriorityLow    JobPriority = 0
	JobPriorityNormal JobPriority = 1
	JobPriorityHigh   JobPriority = 2
)
// JobType represents the type of job
type JobType string
const (
	JobTypeAudioProcess JobType = "audio_process"
	JobTypeAIEnrich     JobType = "ai_enrich"
	JobTypeDDEXExport   JobType = "ddex_export"
	JobTypeCleanup      JobType = "cleanup"
)
// Job represents a background job
type Job struct {
	ID          string          `json:"id"`
	Type        JobType         `json:"type"`
	Priority    JobPriority     `json:"priority"`
	Status      JobStatus       `json:"status"`
	Payload     json.RawMessage `json:"payload"`
	Error       string          `json:"error,omitempty"`
	Progress    int             `json:"progress"`
	RetryCount  int             `json:"retry_count"`
	MaxRetries  int             `json:"max_retries"`
	CreatedAt   time.Time       `json:"created_at"`
	StartedAt   *time.Time      `json:"started_at,omitempty"`
	CompletedAt *time.Time      `json:"completed_at,omitempty"`
	NextRetryAt *time.Time      `json:"next_retry_at,omitempty"`
}
// JobConfig holds configuration for the job system
type JobConfig struct {
	// Worker settings
	NumWorkers    int           `env:"JOB_NUM_WORKERS" envDefault:"5"`
	MaxConcurrent int           `env:"JOB_MAX_CONCURRENT" envDefault:"10"`
	PollInterval  time.Duration `env:"JOB_POLL_INTERVAL" envDefault:"1s"`
	ShutdownWait  time.Duration `env:"JOB_SHUTDOWN_WAIT" envDefault:"30s"`
	// Job settings
	DefaultMaxRetries int           `env:"JOB_DEFAULT_MAX_RETRIES" envDefault:"3"`
	DefaultTTL        time.Duration `env:"JOB_DEFAULT_TTL" envDefault:"24h"`
	MaxPayloadSize    int64         `env:"JOB_MAX_PAYLOAD_SIZE" envDefault:"1048576"` // 1MB
	// Queue settings
	QueuePrefix     string        `env:"JOB_QUEUE_PREFIX" envDefault:"jobs:"`
	RetryDelay      time.Duration `env:"JOB_RETRY_DELAY" envDefault:"5s"`
	MaxRetryDelay   time.Duration `env:"JOB_MAX_RETRY_DELAY" envDefault:"1h"`
	RetryMultiplier float64       `env:"JOB_RETRY_MULTIPLIER" envDefault:"2.0"`
	// Cleanup settings
	CleanupInterval time.Duration `env:"JOB_CLEANUP_INTERVAL" envDefault:"1h"`
	MaxJobAge       time.Duration `env:"JOB_MAX_AGE" envDefault:"168h"` // 7 days
}
// JobQueue defines the interface for job queue operations
type JobQueue interface {
	// Enqueue adds a new job to the queue
	Enqueue(ctx context.Context, job *Job) error
	// Dequeue gets the next job to process
	Dequeue(ctx context.Context) (*Job, error)
	// Complete marks a job as completed
	Complete(ctx context.Context, jobID string) error
	// Fail marks a job as failed
	Fail(ctx context.Context, jobID string, err error) error
	// Cancel cancels a pending or running job
	Cancel(ctx context.Context, jobID string) error
	// GetStatus gets the current status of a job
	GetStatus(ctx context.Context, jobID string) (*Job, error)
	// UpdateProgress updates the progress of a running job
	UpdateProgress(ctx context.Context, jobID string, progress int) error
}
// JobHandler defines the interface for job type handlers
type JobHandler interface {
	// HandleJob processes a specific type of job
	HandleJob(ctx context.Context, job *Job) error
	// JobType returns the type of job this handler processes
	JobType() JobType
}
// JobProcessor manages the job processing system
type JobProcessor interface {
	// Start starts the job processor
	Start(ctx context.Context) error
	// Stop stops the job processor
	Stop() error
	// RegisterHandler registers a handler for a specific job type
	RegisterHandler(handler JobHandler) error
}
</file>

<file path="internal/pkg/domain/metadata.go">
package domain
import "time"
// MetadataProvider defines the interface for any type that can provide metadata
type MetadataProvider interface {
	GetBasicMetadata() BasicTrackMetadata
}
// Metadata represents the GraphQL metadata type
type Metadata struct {
	ISRC         string            `json:"isrc"`
	ISWC         string            `json:"iswc"`
	BPM          float64           `json:"bpm"`
	Key          string            `json:"key"`
	Mood         string            `json:"mood"`
	Labels       []string          `json:"labels"`
	AITags       []string          `json:"aiTags"`
	Confidence   float64           `json:"confidence"`
	ModelVersion string            `json:"modelVersion"`
	CustomFields map[string]string `json:"customFields"`
}
// BasicTrackMetadata contains the fundamental metadata fields shared across all types
type BasicTrackMetadata struct {
	Title     string    `json:"title"`
	Artist    string    `json:"artist"`
	Album     string    `json:"album"`
	Year      int       `json:"year"`
	Duration  float64   `json:"duration"`
	ISRC      string    `json:"isrc"`
	CreatedAt time.Time `json:"createdAt"`
	UpdatedAt time.Time `json:"updatedAt"`
}
// CompleteTrackMetadata represents the complete metadata for a track
type CompleteTrackMetadata struct {
	BasicTrackMetadata `json:"basic"`
	Technical          AudioTechnicalMetadata `json:"technical"`
	Musical            MusicalMetadata        `json:"musical"`
	AI                 *TrackAIMetadata       `json:"ai,omitempty"`
	Additional         AdditionalMetadata     `json:"additional"`
}
// AudioTechnicalMetadata contains audio file technical details
type AudioTechnicalMetadata struct {
	Format     AudioFormat `json:"format"`
	SampleRate int         `json:"sampleRate"` // Hz
	Bitrate    int         `json:"bitrate"`    // kbps
	Channels   int         `json:"channels"`
	FileSize   int64       `json:"fileSize"` // bytes
}
// MusicalMetadata contains musical attributes
type MusicalMetadata struct {
	BPM    float64 `json:"bpm"`
	Key    string  `json:"key"`
	Mode   string  `json:"mode"`
	Mood   string  `json:"mood"`
	Genre  string  `json:"genre"`
	Energy float64 `json:"energy"`
	Tempo  float64 `json:"tempo"`
}
// TrackAIMetadata contains AI-generated metadata and processing information
type TrackAIMetadata struct {
	Tags                  []string               `json:"tags"`
	Confidence            float64                `json:"confidence"`
	Model                 string                 `json:"model"`
	Version               string                 `json:"version"`
	ProcessedAt           time.Time              `json:"processedAt"`
	NeedsReview           bool                   `json:"needsReview"`
	ReviewReason          string                 `json:"reviewReason,omitempty"`
	Analysis              string                 `json:"analysis,omitempty"`
	ValidationIssues      []ValidationIssue      `json:"validationIssues,omitempty"`
	ValidationSuggestions []ValidationSuggestion `json:"validationSuggestions,omitempty"`
}
// AdditionalMetadata contains supplementary metadata fields
type AdditionalMetadata struct {
	Publisher    string            `json:"publisher"`
	Copyright    string            `json:"copyright"`
	Lyrics       string            `json:"lyrics"`
	CustomTags   map[string]string `json:"customTags"`
	CustomFields map[string]string `json:"customFields"`
}
// GetBasicMetadata implements MetadataProvider interface
func (t CompleteTrackMetadata) GetBasicMetadata() BasicTrackMetadata {
	return t.BasicTrackMetadata
}
</file>

<file path="internal/pkg/domain/models.go">
package domain
// This file is deprecated and will be removed.
// All types have been moved to their respective domain files:
// - Track types -> track.go
// - Metadata types -> metadata.go
// - Queue/Message types -> queue.go
</file>

<file path="internal/pkg/domain/queue.go">
package domain
import (
	"context"
	"fmt"
	"time"
)
// QueuePriority represents the priority level of a queue message
type QueuePriority int
const (
	PriorityLow QueuePriority = iota
	PriorityMedium
	PriorityHigh
)
// QueueMessageType represents the type of message
type QueueMessageType string
const (
	QueueMessageTypeAIProcess QueueMessageType = "ai_process"
	QueueMessageTypeDDEX      QueueMessageType = "ddex"
	QueueMessageTypeCleanup   QueueMessageType = "cleanup"
)
// QueueMessage represents a message in the queue
type QueueMessage struct {
	ID         string           `json:"id"`
	Type       QueueMessageType `json:"type"`
	Priority   QueuePriority    `json:"priority"`
	Payload    []byte           `json:"payload"`
	PubSubID   string           `json:"pubsub_id,omitempty"`
	CreatedAt  time.Time        `json:"created_at"`
	Error      string           `json:"error,omitempty"`
	RetryCount int              `json:"retry_count"`
}
// MessageStatus represents the current status of a message
type MessageStatus string
const (
	// MessageStatusPending indicates the message is waiting to be processed
	MessageStatusPending MessageStatus = "pending"
	// MessageStatusProcessing indicates the message is being processed
	MessageStatusProcessing MessageStatus = "processing"
	// MessageStatusCompleted indicates the message was processed successfully
	MessageStatusCompleted MessageStatus = "completed"
	// MessageStatusFailed indicates the message processing failed
	MessageStatusFailed MessageStatus = "failed"
	// MessageStatusRetrying indicates the message is being retried
	MessageStatusRetrying MessageStatus = "retrying"
	// MessageStatusDeadLetter indicates the message has been moved to dead letter queue
	MessageStatusDeadLetter MessageStatus = "dead_letter"
)
// Message represents a queue message
type Message struct {
	ID           string                 `json:"id"`
	Type         string                 `json:"type"`
	Data         map[string]interface{} `json:"data"`
	Status       MessageStatus          `json:"status"`
	RetryCount   int                    `json:"retry_count"`
	MaxRetries   int                    `json:"max_retries"`
	ErrorMessage string                 `json:"error_message,omitempty"`
	CreatedAt    time.Time              `json:"created_at"`
	UpdatedAt    time.Time              `json:"updated_at"`
	NextRetryAt  *time.Time             `json:"next_retry_at,omitempty"`
	ProcessedAt  *time.Time             `json:"processed_at,omitempty"`
	DeadLetterAt *time.Time             `json:"dead_letter_at,omitempty"`
	Priority     QueuePriority          `json:"priority"`
}
// MessageHandler is a function that processes a message
type MessageHandler func(ctx context.Context, msg *Message) error
// Publisher defines the interface for publishing messages
type Publisher interface {
	// Publish publishes a message to a topic
	Publish(ctx context.Context, topic string, data []byte) error
	// PublishWithRetry publishes a message with retry configuration
	PublishWithRetry(ctx context.Context, topic string, data []byte, maxRetries int) error
}
// Subscriber defines the interface for subscribing to messages
type Subscriber interface {
	// Subscribe subscribes to a topic with a message handler
	Subscribe(ctx context.Context, topic string, handler MessageHandler) error
	// Unsubscribe removes a subscription from a topic
	Unsubscribe(ctx context.Context, topic string) error
}
// QueueService combines Publisher and Subscriber interfaces
type QueueService interface {
	Publish(ctx context.Context, topic string, message *Message, priority QueuePriority) error
	Subscribe(ctx context.Context, topic string, handler MessageHandler) error
	HandleDeadLetter(ctx context.Context, topic string, message *Message) error
	// GetMessage retrieves a message by ID
	GetMessage(ctx context.Context, id string) (*Message, error)
	// RetryMessage marks a message for retry
	RetryMessage(ctx context.Context, id string) error
	// AckMessage acknowledges a message as processed
	AckMessage(ctx context.Context, id string) error
	// NackMessage marks a message as failed
	NackMessage(ctx context.Context, id string, err error) error
	// ListDeadLetters retrieves messages in the dead letter queue
	ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*Message, error)
	// ReplayDeadLetter moves a message from dead letter queue back to main queue
	ReplayDeadLetter(ctx context.Context, id string) error
	// PurgeDeadLetters removes all messages from dead letter queue
	PurgeDeadLetters(ctx context.Context, topic string) error
	// Close closes the queue service
	Close() error
}
// QueueConfig holds configuration for the queue service
type QueueConfig struct {
	// Connection settings
	Host     string `json:"host"`
	Port     int    `json:"port"`
	Password string `json:"password"`
	DB       int    `json:"db"`
	// Processing settings
	RetryDelays       []int         `json:"retry_delays"`
	DefaultMaxRetries int           `json:"default_max_retries"`
	DeadLetterTTL     time.Duration `json:"dead_letter_ttl"`
	ProcessingTimeout time.Duration `json:"processing_timeout"`
	BatchSize         int           `json:"batch_size"`
	PollInterval      time.Duration `json:"poll_interval"`
	CleanupInterval   time.Duration `json:"cleanup_interval"`
}
// DefaultQueueConfig returns a default configuration
func DefaultQueueConfig() QueueConfig {
	return QueueConfig{
		RetryDelays:       []int{1, 5, 15, 30, 60}, // Exponential backoff
		DefaultMaxRetries: 5,
		DeadLetterTTL:     7 * 24 * time.Hour, // 7 days
		ProcessingTimeout: 5 * time.Minute,
		BatchSize:         100,
		PollInterval:      time.Second,
		CleanupInterval:   1 * time.Hour,
	}
}
// Validate checks if the configuration is valid
func (c *QueueConfig) Validate() error {
	if len(c.RetryDelays) == 0 {
		return fmt.Errorf("retry delays must not be empty")
	}
	if c.DefaultMaxRetries <= 0 {
		return fmt.Errorf("default max retries must be positive")
	}
	if c.DeadLetterTTL <= 0 {
		return fmt.Errorf("dead letter TTL must be positive")
	}
	if c.ProcessingTimeout <= 0 {
		return fmt.Errorf("processing timeout must be positive")
	}
	if c.BatchSize <= 0 {
		return fmt.Errorf("batch size must be positive")
	}
	if c.PollInterval <= 0 {
		return fmt.Errorf("poll interval must be positive")
	}
	if c.CleanupInterval <= 0 {
		return fmt.Errorf("cleanup interval must be positive")
	}
	return nil
}
type Queue interface {
	Close() error
	Publish(ctx context.Context, topic string, data []byte) error
	PublishWithRetry(ctx context.Context, topic string, data []byte, maxRetries int) error
	Subscribe(ctx context.Context, topic string, handler MessageHandler) error
	Unsubscribe(ctx context.Context, topic string) error
	GetMessage(ctx context.Context, id string) (*Message, error)
	RetryMessage(ctx context.Context, id string) error
	AckMessage(ctx context.Context, id string) error
	NackMessage(ctx context.Context, id string, err error) error
	ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*Message, error)
	ReplayDeadLetter(ctx context.Context, id string) error
	PurgeDeadLetters(ctx context.Context, topic string) error
}
// String returns the string representation of QueuePriority
func (p QueuePriority) String() string {
	switch p {
	case PriorityLow:
		return "low"
	case PriorityMedium:
		return "medium"
	case PriorityHigh:
		return "high"
	default:
		return "unknown"
	}
}
</file>

<file path="internal/pkg/domain/session.go">
package domain
import (
	"context"
	"time"
)
// SessionConfig holds configuration for session management
type SessionConfig struct {
	// Cookie settings
	CookieName     string `json:"cookie_name"`
	CookieDomain   string `json:"cookie_domain"`
	CookiePath     string `json:"cookie_path"`
	CookieSecure   bool   `json:"cookie_secure"`
	CookieHTTPOnly bool   `json:"cookie_http_only"`
	CookieSameSite string `json:"cookie_same_site"`
	// Session settings
	SessionDuration    time.Duration `json:"session_duration"`
	CleanupInterval    time.Duration `json:"cleanup_interval"`
	MaxSessionsPerUser int           `json:"max_sessions_per_user"`
}
// Session represents a user session
type Session struct {
	ID          string       `json:"id"`
	UserID      string       `json:"user_id"`
	Token       string       `json:"token"`
	Role        Role         `json:"role"`
	Permissions []Permission `json:"permissions"`
	UserAgent   string       `json:"user_agent"`
	IP          string       `json:"ip"`
	ExpiresAt   time.Time    `json:"expires_at"`
	CreatedAt   time.Time    `json:"created_at"`
	LastSeenAt  time.Time    `json:"last_seen_at"`
}
// SessionRepository defines the interface for session persistence
type SessionRepository interface {
	Create(ctx context.Context, session *Session) error
	GetByID(ctx context.Context, id string) (*Session, error)
	GetUserSessions(ctx context.Context, userID string) ([]*Session, error)
	Delete(ctx context.Context, id string) error
	DeleteUserSessions(ctx context.Context, userID string) error
}
// SessionStore defines the interface for session management
type SessionStore interface {
	// Create creates a new session
	Create(ctx context.Context, session *Session) error
	// Get retrieves a session by ID
	Get(ctx context.Context, sessionID string) (*Session, error)
	// GetUserSessions retrieves all active sessions for a user
	GetUserSessions(ctx context.Context, userID string) ([]*Session, error)
	// Update updates an existing session
	Update(ctx context.Context, session *Session) error
	// Delete removes a session
	Delete(ctx context.Context, sessionID string) error
	// DeleteUserSessions removes all sessions for a user
	DeleteUserSessions(ctx context.Context, userID string) error
	// DeleteExpired removes all expired sessions
	DeleteExpired(ctx context.Context) error
	// Touch updates the last seen time of a session
	Touch(ctx context.Context, sessionID string) error
}
</file>

<file path="internal/pkg/domain/storage_types.go">
package domain
// SignedURLOperation represents the type of operation for a signed URL
type SignedURLOperation string
const (
	SignedURLUpload   SignedURLOperation = "upload"
	SignedURLDownload SignedURLOperation = "download"
)
// StoragePathType represents the type of storage path
type StoragePathType string
const (
	StoragePathTemp StoragePathType = "temp"
	StoragePathPerm StoragePathType = "perm"
)
// String returns the string representation of the storage path type
func (s StoragePathType) String() string {
	return string(s)
}
// StorageError represents a storage-specific error
type StorageError struct {
	Code    string
	Message string
	Op      string
	Err     error
}
// Error implements the error interface
func (e *StorageError) Error() string {
	if e.Err != nil {
		return e.Message + ": " + e.Err.Error()
	}
	return e.Message
}
// Unwrap returns the underlying error
func (e *StorageError) Unwrap() error {
	return e.Err
}
// NewStorageError creates a new storage error
func NewStorageError(code, message, op string, err error) *StorageError {
	return &StorageError{
		Code:    code,
		Message: message,
		Op:      op,
		Err:     err,
	}
}
</file>

<file path="internal/pkg/domain/track.go">
package domain
import (
	"context"
	"fmt"
	"time"
)
// Track represents a music track
type Track struct {
	// Core fields
	ID        string     `json:"id"`
	CreatedAt time.Time  `json:"createdAt"`
	UpdatedAt time.Time  `json:"updatedAt"`
	DeletedAt *time.Time `json:"deletedAt,omitempty"`
	// Storage details
	StoragePath string `json:"storagePath"`
	FilePath    string `json:"filePath"` // Deprecated: use StoragePath
	FileSize    int64  `json:"fileSize"`
	AudioData   []byte `json:"-"` // In-memory audio data for processing
	// Track metadata
	Metadata CompleteTrackMetadata `json:"metadata"`
	// Relationships
	LabelID   string   `json:"labelId"`
	ArtistIDs []string `json:"artistIds"`
	ReleaseID string   `json:"releaseId"`
	// Versioning
	Version    int    `json:"version"`
	PreviousID string `json:"previousId,omitempty"`
	// Status
	Status    TrackStatus `json:"status"`
	StatusMsg string      `json:"statusMsg,omitempty"`
}
// TrackStatus represents the current status of a track
type TrackStatus string
const (
	// Track status constants
	TrackStatusDraft    TrackStatus = "draft"
	TrackStatusPending  TrackStatus = "pending"
	TrackStatusActive   TrackStatus = "active"
	TrackStatusInactive TrackStatus = "inactive"
	TrackStatusRejected TrackStatus = "rejected"
	TrackStatusDeleted  TrackStatus = "deleted"
)
// IsValid checks if the track status is valid
func (s TrackStatus) IsValid() bool {
	switch s {
	case TrackStatusDraft, TrackStatusPending, TrackStatusActive,
		TrackStatusInactive, TrackStatusRejected, TrackStatusDeleted:
		return true
	default:
		return false
	}
}
// String returns the string representation of the track status
func (s TrackStatus) String() string {
	return string(s)
}
// Helper methods to access metadata fields
func (t *Track) Title() string       { return t.Metadata.Title }
func (t *Track) Artist() string      { return t.Metadata.Artist }
func (t *Track) Album() string       { return t.Metadata.Album }
func (t *Track) Year() int           { return t.Metadata.Year }
func (t *Track) Duration() float64   { return t.Metadata.Duration }
func (t *Track) ISRC() string        { return t.Metadata.ISRC }
func (t *Track) ISWC() string        { return t.Metadata.Additional.CustomFields["iswc"] }
func (t *Track) Label() string       { return t.Metadata.Additional.CustomFields["label"] }
func (t *Track) Territory() string   { return t.Metadata.Additional.CustomFields["territory"] }
func (t *Track) Genre() string       { return t.Metadata.Musical.Genre }
func (t *Track) BPM() float64        { return t.Metadata.Musical.BPM }
func (t *Track) Key() string         { return t.Metadata.Musical.Key }
func (t *Track) Mood() string        { return t.Metadata.Musical.Mood }
func (t *Track) AudioFormat() string { return string(t.Metadata.Technical.Format) }
func (t *Track) SampleRate() int     { return t.Metadata.Technical.SampleRate }
func (t *Track) Bitrate() int        { return t.Metadata.Technical.Bitrate }
func (t *Track) Channels() int       { return t.Metadata.Technical.Channels }
func (t *Track) Publisher() string   { return t.Metadata.Additional.Publisher }
func (t *Track) Copyright() string   { return t.Metadata.Additional.Copyright }
func (t *Track) Lyrics() string      { return t.Metadata.Additional.Lyrics }
// AI-related fields
func (t *Track) AITags() []string      { return t.Metadata.AI.Tags }
func (t *Track) AIConfidence() float64 { return t.Metadata.AI.Confidence }
func (t *Track) ModelVersion() string  { return t.Metadata.AI.Version }
func (t *Track) NeedsReview() bool     { return t.Metadata.AI.NeedsReview }
// Helper methods to set metadata fields
func (t *Track) SetTitle(v string)       { t.Metadata.Title = v }
func (t *Track) SetArtist(v string)      { t.Metadata.Artist = v }
func (t *Track) SetAlbum(v string)       { t.Metadata.Album = v }
func (t *Track) SetYear(v int)           { t.Metadata.Year = v }
func (t *Track) SetDuration(v float64)   { t.Metadata.Duration = v }
func (t *Track) SetISRC(v string)        { t.Metadata.ISRC = v }
func (t *Track) SetISWC(v string)        { t.Metadata.Additional.CustomFields["iswc"] = v }
func (t *Track) SetLabel(v string)       { t.Metadata.Additional.CustomFields["label"] = v }
func (t *Track) SetTerritory(v string)   { t.Metadata.Additional.CustomFields["territory"] = v }
func (t *Track) SetGenre(v string)       { t.Metadata.Musical.Genre = v }
func (t *Track) SetBPM(v float64)        { t.Metadata.Musical.BPM = v }
func (t *Track) SetKey(v string)         { t.Metadata.Musical.Key = v }
func (t *Track) SetMood(v string)        { t.Metadata.Musical.Mood = v }
func (t *Track) SetAudioFormat(v string) { t.Metadata.Technical.Format = AudioFormat(v) }
func (t *Track) SetSampleRate(v int)     { t.Metadata.Technical.SampleRate = v }
func (t *Track) SetBitrate(v int)        { t.Metadata.Technical.Bitrate = v }
func (t *Track) SetChannels(v int)       { t.Metadata.Technical.Channels = v }
func (t *Track) SetPublisher(v string)   { t.Metadata.Additional.Publisher = v }
func (t *Track) SetCopyright(v string)   { t.Metadata.Additional.Copyright = v }
func (t *Track) SetLyrics(v string)      { t.Metadata.Additional.Lyrics = v }
// Additional helper methods for relationships
func (t *Track) SetLabelID(id string) {
	t.LabelID = id
	t.UpdatedAt = time.Now()
}
func (t *Track) AddArtist(id string) {
	for _, existingID := range t.ArtistIDs {
		if existingID == id {
			return
		}
	}
	t.ArtistIDs = append(t.ArtistIDs, id)
	t.UpdatedAt = time.Now()
}
func (t *Track) RemoveArtist(id string) {
	for i, existingID := range t.ArtistIDs {
		if existingID == id {
			t.ArtistIDs = append(t.ArtistIDs[:i], t.ArtistIDs[i+1:]...)
			t.UpdatedAt = time.Now()
			return
		}
	}
}
func (t *Track) SetRelease(id string) {
	t.ReleaseID = id
	t.UpdatedAt = time.Now()
}
// Version management methods
func (t *Track) IncrementVersion() {
	t.Version++
	t.UpdatedAt = time.Now()
}
func (t *Track) SetPreviousVersion(id string) {
	t.PreviousID = id
	t.UpdatedAt = time.Now()
}
// Status management methods
func (t *Track) SetStatus(status TrackStatus, msg string) error {
	if !status.IsValid() {
		return fmt.Errorf("invalid track status: %s", status)
	}
	t.Status = status
	t.StatusMsg = msg
	t.UpdatedAt = time.Now()
	return nil
}
// TrackRepository defines the interface for track data operations
type TrackRepository interface {
	// Create creates a new track
	Create(ctx context.Context, track *Track) error
	// GetByID retrieves a track by ID
	GetByID(ctx context.Context, id string) (*Track, error)
	// Update updates an existing track
	Update(ctx context.Context, track *Track) error
	// Delete soft-deletes a track
	Delete(ctx context.Context, id string) error
	// List retrieves tracks based on filters with pagination
	List(ctx context.Context, filter map[string]interface{}, offset, limit int) ([]*Track, error)
	// SearchByMetadata searches tracks by metadata fields
	SearchByMetadata(ctx context.Context, query map[string]interface{}) ([]*Track, error)
	// GetByISRC retrieves a track by ISRC
	GetByISRC(ctx context.Context, isrc string) (*Track, error)
	// BatchUpdate updates multiple tracks in a single transaction
	BatchUpdate(ctx context.Context, tracks []*Track) error
}
</file>

<file path="internal/pkg/domain/user.go">
package domain
import (
	"context"
	"fmt"
	"time"
	"golang.org/x/crypto/bcrypt"
)
// SubscriptionPlan represents a user's subscription level
type SubscriptionPlan string
const (
	PlanFree     SubscriptionPlan = "free"
	PlanBasic    SubscriptionPlan = "basic"
	PlanPro      SubscriptionPlan = "pro"
	PlanBusiness SubscriptionPlan = "business"
)
// PlanLimits defines the quota limits for each subscription plan
var PlanLimits = map[SubscriptionPlan]int{
	PlanFree:     100,
	PlanBasic:    1000,
	PlanPro:      10000,
	PlanBusiness: 100000,
}
// User represents a system user
type User struct {
	ID             string           `json:"id"`
	Email          string           `json:"email"`
	Password       string           `json:"-"` // Never expose password in JSON
	Name           string           `json:"name"`
	Role           Role             `json:"role"`
	Permissions    []Permission     `json:"permissions"`
	Company        string           `json:"company,omitempty"`
	APIKey         string           `json:"api_key,omitempty"`
	Plan           SubscriptionPlan `json:"plan"`
	TrackQuota     int              `json:"track_quota"`
	TracksUsed     int              `json:"tracks_used"`
	QuotaResetDate time.Time        `json:"quota_reset_date"`
	LastLoginAt    time.Time        `json:"last_login_at"`
	CreatedAt      time.Time        `json:"created_at"`
	UpdatedAt      time.Time        `json:"updated_at"`
}
// NewUser creates a new user with default values
func NewUser(email, name string, role Role) *User {
	now := time.Now()
	return &User{
		Email:          email,
		Name:           name,
		Role:           role,
		Permissions:    RolePermissions[role],
		Plan:           PlanFree,
		TrackQuota:     PlanLimits[PlanFree],
		QuotaResetDate: now.AddDate(0, 1, 0), // Reset quota in 1 month
		CreatedAt:      now,
		UpdatedAt:      now,
	}
}
// SetPassword safely hashes and sets the user's password
func (u *User) SetPassword(password string) error {
	if len(password) < 8 {
		return fmt.Errorf("password must be at least 8 characters long")
	}
	hashedPassword, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)
	if err != nil {
		return fmt.Errorf("failed to hash password: %w", err)
	}
	u.Password = string(hashedPassword)
	return nil
}
// CheckPassword verifies if the provided password matches the stored hash
func (u *User) CheckPassword(password string) bool {
	err := bcrypt.CompareHashAndPassword([]byte(u.Password), []byte(password))
	return err == nil
}
// HasPermission checks if the user has a specific permission
func (u *User) HasPermission(perm Permission) bool {
	for _, p := range u.Permissions {
		if p == perm {
			return true
		}
	}
	return false
}
// UpdatePlan updates the user's subscription plan and associated limits
func (u *User) UpdatePlan(plan SubscriptionPlan) {
	u.Plan = plan
	u.TrackQuota = PlanLimits[plan]
	u.UpdatedAt = time.Now()
}
// HasQuotaAvailable checks if the user has remaining quota
func (u *User) HasQuotaAvailable() bool {
	if time.Now().After(u.QuotaResetDate) {
		u.TracksUsed = 0
		u.QuotaResetDate = time.Now().AddDate(0, 1, 0)
	}
	return u.TracksUsed < u.TrackQuota
}
// IncrementTracksUsed increments the tracks used count
func (u *User) IncrementTracksUsed() {
	u.TracksUsed++
	u.UpdatedAt = time.Now()
}
// Validate performs validation on user data
func (u *User) Validate() error {
	if u.Email == "" {
		return fmt.Errorf("email is required")
	}
	if u.Name == "" {
		return fmt.Errorf("name is required")
	}
	if u.Role == "" {
		return fmt.Errorf("role is required")
	}
	if u.Plan == "" {
		return fmt.Errorf("subscription plan is required")
	}
	return nil
}
// UserRepository defines the interface for user data access
type UserRepository interface {
	Create(ctx context.Context, user *User) error
	GetByID(ctx context.Context, id string) (*User, error)
	GetByEmail(ctx context.Context, email string) (*User, error)
	GetByAPIKey(ctx context.Context, apiKey string) (*User, error)
	Update(ctx context.Context, user *User) error
	Delete(ctx context.Context, id string) error
	UpdateAPIKey(ctx context.Context, userID string, apiKey string) error
	List(ctx context.Context, offset, limit int) ([]*User, error)
}
</file>

<file path="internal/pkg/domain/validation.go">
package domain
import (
	"unicode"
)
// ValidationResult represents the result of a validation check
type ValidationResult struct {
	IsValid bool              `json:"is_valid"`
	Errors  []ValidationError `json:"errors,omitempty"`
}
// ValidationError represents a single validation error
type ValidationError struct {
	Field   string `json:"field"`
	Message string `json:"message"`
}
// Validator defines the interface for track validation
type Validator interface {
	// Validate validates a track and returns the validation result
	Validate(track *Track) ValidationResult
}
// ValidationIssue represents a validation issue found during validation
type ValidationIssue struct {
	Field       string `json:"field"`
	Severity    string `json:"severity"`
	Description string `json:"description"`
}
// ValidationSuggestion represents a metadata improvement suggestion
type ValidationSuggestion struct {
	Field          string `json:"field"`
	CurrentValue   string `json:"current_value"`
	SuggestedValue string `json:"suggested_value"`
	Reason         string `json:"reason"`
}
// ValidationResponse represents the complete validation response from an AI service
type ValidationResponse struct {
	ConfidenceScores struct {
		TitleArtistMatch     float64 `json:"title_artist_match"`
		GenreAccuracy        float64 `json:"genre_accuracy"`
		MusicalConsistency   float64 `json:"musical_consistency"`
		MetadataCompleteness float64 `json:"metadata_completeness"`
		Overall              float64 `json:"overall"`
	} `json:"confidence_scores"`
	Issues      []ValidationIssue      `json:"issues"`
	Suggestions []ValidationSuggestion `json:"suggestions"`
	Analysis    string                 `json:"analysis"`
}
// TrackValidator implements the Validator interface
type TrackValidator struct{}
// NewTrackValidator creates a new track validator
func NewTrackValidator() *TrackValidator {
	return &TrackValidator{}
}
// Validate validates a track and returns the validation result
func (v *TrackValidator) Validate(track *Track) ValidationResult {
	var errors []ValidationError
	// Required fields validation
	if track.Title() == "" {
		errors = append(errors, ValidationError{
			Field:   "title",
			Message: "Title is required",
		})
	}
	if track.Artist() == "" {
		errors = append(errors, ValidationError{
			Field:   "artist",
			Message: "Artist is required",
		})
	}
	if track.ISRC() != "" && !isValidISRC(track.ISRC()) {
		errors = append(errors, ValidationError{
			Field:   "isrc",
			Message: "Invalid ISRC format",
		})
	}
	return ValidationResult{
		IsValid: len(errors) == 0,
		Errors:  errors,
	}
}
// Helper validation functions
func isValidISRC(isrc string) bool {
	// ISRC format: CC-XXX-YY-NNNNN
	// CC: Country Code (2 chars)
	// XXX: Registrant Code (3 chars)
	// YY: Year (2 digits)
	// NNNNN: Designation Code (5 digits)
	if len(isrc) != 12 {
		return false
	}
	// Check country code (first two characters must be letters)
	if !isAlpha(isrc[0:2]) {
		return false
	}
	// Check registrant code (next three characters must be alphanumeric)
	if !isAlphanumeric(isrc[2:5]) {
		return false
	}
	// Check year code (next two characters must be digits)
	if !isNumeric(isrc[5:7]) {
		return false
	}
	// Check designation code (last five characters must be digits)
	if !isNumeric(isrc[7:]) {
		return false
	}
	return true
}
func isValidAudioFormat(format string) bool {
	validFormats := map[string]bool{
		string(AudioFormatMP3):  true,
		string(AudioFormatWAV):  true,
		string(AudioFormatFLAC): true,
		string(AudioFormatM4A):  true,
		string(AudioFormatAAC):  true,
		string(AudioFormatOGG):  true,
	}
	return validFormats[format]
}
func isValidBitrate(bitrate int) bool {
	return bitrate >= 32 && bitrate <= 1411
}
func isValidSampleRate(sampleRate int) bool {
	validRates := map[int]bool{
		8000:   true,
		11025:  true,
		16000:  true,
		22050:  true,
		32000:  true,
		44100:  true,
		48000:  true,
		88200:  true,
		96000:  true,
		192000: true,
	}
	return validRates[sampleRate]
}
func isValidBPM(bpm float64) bool {
	return bpm >= 20 && bpm <= 400
}
// String helper functions
func isAlpha(s string) bool {
	for _, r := range s {
		if !unicode.IsLetter(r) {
			return false
		}
	}
	return true
}
func isNumeric(s string) bool {
	for _, r := range s {
		if !unicode.IsDigit(r) {
			return false
		}
	}
	return true
}
func isAlphanumeric(s string) bool {
	for _, r := range s {
		if !unicode.IsLetter(r) && !unicode.IsDigit(r) {
			return false
		}
	}
	return true
}
</file>

<file path="internal/pkg/errors/errors.go">
package errors
import (
	"fmt"
	"net/http"
)
// ErrorType represents the type of error
type ErrorType string
const (
	// Error types
	ErrorTypeValidation   ErrorType = "VALIDATION_ERROR"
	ErrorTypeNotFound     ErrorType = "NOT_FOUND"
	ErrorTypeDatabase     ErrorType = "DATABASE_ERROR"
	ErrorTypeStorage      ErrorType = "STORAGE_ERROR"
	ErrorTypeAI           ErrorType = "AI_ERROR"
	ErrorTypeUnauthorized ErrorType = "UNAUTHORIZED"
	ErrorTypeForbidden    ErrorType = "FORBIDDEN"
	ErrorTypeInternal     ErrorType = "INTERNAL_ERROR"
)
// AppError represents an application error
type AppError struct {
	Type       ErrorType `json:"type"`
	Message    string    `json:"message"`
	Details    string    `json:"details,omitempty"`
	StatusCode int       `json:"-"`
	Err        error     `json:"-"`
}
// Error implements the error interface
func (e *AppError) Error() string {
	if e.Err != nil {
		return fmt.Sprintf("%s: %s (%s)", e.Type, e.Message, e.Err.Error())
	}
	return fmt.Sprintf("%s: %s", e.Type, e.Message)
}
// Unwrap returns the wrapped error
func (e *AppError) Unwrap() error {
	return e.Err
}
// NewValidationError creates a new validation error
func NewValidationError(message string, details string) *AppError {
	return &AppError{
		Type:       ErrorTypeValidation,
		Message:    message,
		Details:    details,
		StatusCode: http.StatusBadRequest,
	}
}
// NewNotFoundError creates a new not found error
func NewNotFoundError(message string) *AppError {
	return &AppError{
		Type:       ErrorTypeNotFound,
		Message:    message,
		StatusCode: http.StatusNotFound,
	}
}
// NewDatabaseError creates a new database error
func NewDatabaseError(message string, err error) *AppError {
	return &AppError{
		Type:       ErrorTypeDatabase,
		Message:    message,
		StatusCode: http.StatusInternalServerError,
		Err:        err,
	}
}
// NewStorageError creates a new storage error
func NewStorageError(message string, err error) *AppError {
	return &AppError{
		Type:       ErrorTypeStorage,
		Message:    message,
		StatusCode: http.StatusInternalServerError,
		Err:        err,
	}
}
// NewAIError creates a new AI service error
func NewAIError(message string, err error) *AppError {
	return &AppError{
		Type:       ErrorTypeAI,
		Message:    message,
		StatusCode: http.StatusInternalServerError,
		Err:        err,
	}
}
// NewUnauthorizedError creates a new unauthorized error
func NewUnauthorizedError(message string) *AppError {
	return &AppError{
		Type:       ErrorTypeUnauthorized,
		Message:    message,
		StatusCode: http.StatusUnauthorized,
	}
}
// NewForbiddenError creates a new forbidden error
func NewForbiddenError(message string) *AppError {
	return &AppError{
		Type:       ErrorTypeForbidden,
		Message:    message,
		StatusCode: http.StatusForbidden,
	}
}
// NewInternalError creates a new internal error
func NewInternalError(message string, err error) *AppError {
	return &AppError{
		Type:       ErrorTypeInternal,
		Message:    message,
		StatusCode: http.StatusInternalServerError,
		Err:        err,
	}
}
// IsNotFound checks if the error is a not found error
func IsNotFound(err error) bool {
	if err == nil {
		return false
	}
	if e, ok := err.(*AppError); ok {
		return e.Type == ErrorTypeNotFound
	}
	return false
}
// IsValidation checks if the error is a validation error
func IsValidation(err error) bool {
	if err == nil {
		return false
	}
	if e, ok := err.(*AppError); ok {
		return e.Type == ErrorTypeValidation
	}
	return false
}
// IsUnauthorized checks if the error is an unauthorized error
func IsUnauthorized(err error) bool {
	if err == nil {
		return false
	}
	if e, ok := err.(*AppError); ok {
		return e.Type == ErrorTypeUnauthorized
	}
	return false
}
// IsForbidden checks if the error is a forbidden error
func IsForbidden(err error) bool {
	if err == nil {
		return false
	}
	if e, ok := err.(*AppError); ok {
		return e.Type == ErrorTypeForbidden
	}
	return false
}
</file>

<file path="internal/pkg/errortracking/sentry.go">
package errortracking
import (
	"fmt"
	"os"
	"github.com/getsentry/sentry-go"
)
// ErrorTracker handles error reporting to Sentry
type ErrorTracker struct {
	initialized bool
}
// NewErrorTracker creates a new error tracker
func NewErrorTracker() *ErrorTracker {
	dsn := os.Getenv("SENTRY_DSN")
	if dsn == "" {
		return &ErrorTracker{initialized: false}
	}
	err := sentry.Init(sentry.ClientOptions{
		Dsn:              dsn,
		Environment:      os.Getenv("APP_ENV"),
		TracesSampleRate: 1.0,
	})
	if err != nil {
		fmt.Printf("Sentry initialization failed: %v\n", err)
		return &ErrorTracker{initialized: false}
	}
	return &ErrorTracker{initialized: true}
}
// CaptureError reports an error to Sentry
func (t *ErrorTracker) CaptureError(err error, tags map[string]string) {
	if !t.initialized || err == nil {
		return
	}
	sentry.WithScope(func(scope *sentry.Scope) {
		for key, value := range tags {
			scope.SetTag(key, value)
		}
		sentry.CaptureException(err)
	})
}
// Close flushes any pending events
func (t *ErrorTracker) Close() {
	if t.initialized {
		sentry.Flush(2)
	}
}
</file>

<file path="internal/pkg/logger/logger.go">
package logger
import (
	"os"
	"github.com/sirupsen/logrus"
)
// Logger wraps logrus.Logger
type Logger struct {
	*logrus.Logger
}
// NewLogger creates a new logger instance
func NewLogger() *Logger {
	log := logrus.New()
	log.SetOutput(os.Stdout)
	// Set log level based on environment
	level := os.Getenv("LOG_LEVEL")
	if level == "" {
		level = "info"
	}
	logLevel, err := logrus.ParseLevel(level)
	if err != nil {
		logLevel = logrus.InfoLevel
	}
	log.SetLevel(logLevel)
	// Use JSON formatter in production
	if os.Getenv("APP_ENV") == "production" {
		log.SetFormatter(&logrus.JSONFormatter{})
	} else {
		log.SetFormatter(&logrus.TextFormatter{
			FullTimestamp: true,
		})
	}
	return &Logger{log}
}
// Fields type for structured logging
type Fields logrus.Fields
// WithFields adds fields to the logging context
func (l *Logger) WithFields(fields Fields) *logrus.Entry {
	return l.Logger.WithFields(logrus.Fields(fields))
}
</file>

<file path="internal/pkg/metrics/auth.go">
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
var (
	// Authentication metrics
	AuthAttempts = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "auth_attempts_total",
		Help: "Total number of authentication attempts",
	}, []string{"method", "status"}) // method: login, refresh, api_key; status: success, failure
	ActiveSessions = promauto.NewGauge(prometheus.GaugeOpts{
		Name: "active_sessions_total",
		Help: "Total number of active sessions",
	})
	SessionOperations = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "session_operations_total",
		Help: "Total number of session operations",
	}, []string{"operation", "status"}) // operation: create, delete, refresh; status: success, failure
	TokenOperations = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "token_operations_total",
		Help: "Total number of token operations",
	}, []string{"operation", "status"}) // operation: generate, validate, refresh; status: success, failure
	APIKeyOperations = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "api_key_operations_total",
		Help: "Total number of API key operations",
	}, []string{"operation", "status"}) // operation: generate, validate; status: success, failure
	AuthLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "auth_operation_duration_seconds",
		Help:    "Duration of authentication operations",
		Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
	}, []string{"operation"}) // operation: login, refresh, validate, session_create, etc.
	// Role and permission metrics
	PermissionChecks = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "permission_checks_total",
		Help: "Total number of permission checks",
	}, []string{"permission", "status"}) // status: allowed, denied
	RoleChecks = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "role_checks_total",
		Help: "Total number of role checks",
	}, []string{"role", "status"}) // status: allowed, denied
)
</file>

<file path="internal/pkg/metrics/jobs.go">
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
var (
	// JobsProcessed tracks the total number of processed jobs
	JobsProcessed = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "jobs_processed_total",
			Help: "The total number of processed jobs",
		},
		[]string{"type", "status"},
	)
	// JobsInQueue tracks the current number of jobs in queue
	JobsInQueue = promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "jobs_in_queue",
			Help: "The current number of jobs in queue",
		},
		[]string{"type", "priority"},
	)
	// JobProcessingDuration tracks job processing duration
	JobProcessingDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "job_processing_duration_seconds",
			Help:    "Time spent processing jobs",
			Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300},
		},
		[]string{"type"},
	)
	// JobRetries tracks the number of job retries
	JobRetries = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "job_retries_total",
			Help: "The total number of job retries",
		},
		[]string{"type"},
	)
	// JobErrors tracks the number of job errors
	JobErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "job_errors_total",
			Help: "The total number of job errors",
		},
		[]string{"type", "error_type"},
	)
	// JobStatusTransitions tracks job status changes
	JobStatusTransitions = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "job_status_transitions_total",
			Help: "The total number of job status transitions",
		},
		[]string{"type", "from", "to"},
	)
	// JobQueueLatency tracks how long jobs wait in queue
	JobQueueLatency = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "job_queue_latency_seconds",
			Help:    "Time spent by jobs waiting in queue",
			Buckets: []float64{1, 5, 15, 30, 60, 120, 300, 600},
		},
		[]string{"type", "priority"},
	)
	// JobBatchSize tracks the size of job batches
	JobBatchSize = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "job_batch_size",
			Help:    "Size of job batches",
			Buckets: []float64{1, 5, 10, 20, 50, 100, 200, 500},
		},
		[]string{"type"},
	)
	// JobCleanupOperations tracks cleanup operations
	JobCleanupOperations = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "job_cleanup_operations_total",
			Help: "The total number of job cleanup operations",
		},
		[]string{"operation", "status"},
	)
)
</file>

<file path="internal/pkg/metrics/metrics.go">
// Package metrics provides Prometheus metrics for the application
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
var (
	// HttpRequestsTotal tracks total HTTP requests
	HttpRequestsTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Total number of HTTP requests by status code and method",
		},
		[]string{"status", "method", "path"},
	)
	// HttpRequestDuration tracks HTTP request duration
	HttpRequestDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "http_request_duration_seconds",
			Help:    "HTTP request duration in seconds",
			Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
		},
		[]string{"method", "path"},
	)
	// ActiveUsers tracks current active users
	ActiveUsers = promauto.NewGauge(
		prometheus.GaugeOpts{
			Name: "active_users",
			Help: "Number of currently active users",
		},
	)
	// DatabaseQueryDuration tracks database query duration
	DatabaseQueryDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "database_query_duration_seconds",
			Help:    "Duration of database queries",
			Buckets: prometheus.DefBuckets,
		},
		[]string{"operation"},
	)
	// CacheHits tracks cache hit count
	CacheHits = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "cache_hits_total",
			Help: "Total number of cache hits",
		},
		[]string{"cache"},
	)
	// CacheMisses tracks cache miss count
	CacheMisses = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "cache_misses_total",
			Help: "Total number of cache misses",
		},
		[]string{"cache"},
	)
	// AIRequestDuration tracks AI service request duration
	AIRequestDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "ai_request_duration_seconds",
			Help:    "AI service request duration in seconds",
			Buckets: []float64{.1, .25, .5, 1, 2.5, 5, 10, 20, 30},
		},
		[]string{"operation"},
	)
	// TracksProcessed tracks track processing status
	TracksProcessed = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "tracks_processed_total",
			Help: "Total number of tracks processed by status",
		},
		[]string{"status"},
	)
	// DatabaseConnections tracks database connection pool stats
	DatabaseConnections = promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "database_connections",
			Help: "Number of database connections by state",
		},
		[]string{"state"},
	)
	// SubscriptionsByPlan tracks user subscriptions by plan
	SubscriptionsByPlan = promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "subscriptions_by_plan",
			Help: "Number of subscriptions by plan type",
		},
		[]string{"plan"},
	)
	// DatabaseErrors tracks database errors by type
	DatabaseErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "db_errors_total",
			Help: "Total number of database errors by type",
		},
		[]string{"operation", "error_type"},
	)
	// DatabaseOperationsTotal tracks total database operations
	DatabaseOperationsTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "database_operations_total",
			Help: "Total number of database operations",
		},
		[]string{"operation", "status"},
	)
	// DatabaseRowsAffected tracks the number of rows affected by operations
	DatabaseRowsAffected = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name: "db_rows_affected",
			Help: "Number of rows affected by database operations",
			Buckets: []float64{
				1, 5, 10, 25, 50, 100, 250, 500, 1000,
			},
		},
		[]string{"operation"},
	)
	// AIRequestTotal tracks the total number of AI service requests
	AIRequestTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "ai_request_total",
			Help: "Total number of AI service requests",
		},
		[]string{"provider", "status"},
	)
	// AIBatchSize tracks the size of batch processing requests
	AIBatchSize = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name: "ai_batch_size",
			Help: "Size of AI service batch processing requests",
			Buckets: []float64{
				1, 5, 10, 25, 50, 100, 250, 500,
			},
		},
		[]string{"provider"},
	)
	// AIConfidenceScore tracks the confidence scores returned by AI services
	AIConfidenceScore = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name: "ai_confidence_score",
			Help: "Confidence scores returned by AI services",
			Buckets: []float64{
				0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99,
			},
		},
		[]string{"provider"},
	)
	// AIFallbackTotal tracks the number of times fallback was used
	AIFallbackTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "ai_fallback_total",
			Help: "Total number of times fallback was used",
		},
		[]string{"primary_provider", "fallback_provider"},
	)
	// AIErrorTotal tracks the number of errors by type
	AIErrorTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "ai_error_total",
			Help: "Total number of AI service errors by type",
		},
		[]string{"provider", "error_type"},
	)
	// AI Service metrics
	AIBatchProcessingDuration = promauto.NewHistogram(prometheus.HistogramOpts{
		Name:    "ai_batch_processing_duration_seconds",
		Help:    "Duration of AI batch processing operations",
		Buckets: prometheus.DefBuckets,
	})
	BatchProcessingTotal = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_batch_processing_total",
		Help: "Total number of batch processing operations",
	})
	TracksProcessedTotal = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_tracks_processed_total",
		Help: "Total number of tracks processed",
	})
	AIEnrichmentErrors = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_enrichment_errors_total",
		Help: "Total number of AI enrichment errors",
	})
	AIValidationErrors = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_validation_errors_total",
		Help: "Total number of AI validation errors",
	})
	AIRetryAttempts = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_retry_attempts_total",
		Help: "Total number of AI operation retry attempts",
	})
	AIBatchErrors = promauto.NewCounter(prometheus.CounterOpts{
		Name: "ai_batch_errors_total",
		Help: "Total number of errors in batch processing",
	})
	// Audio processing metrics
	AudioProcessingTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "audio_processing_total",
			Help: "Total number of audio processing attempts",
		},
		[]string{"operation", "status"},
	)
	AudioProcessingDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "audio_processing_duration_seconds",
			Help:    "Duration of audio processing operations",
			Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300},
		},
		[]string{"operation"},
	)
	AudioProcessingSuccess = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "audio_processing_success_total",
			Help: "Total number of successful audio processing operations",
		},
		[]string{"operation"},
	)
	AudioProcessingErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "audio_processing_errors_total",
			Help: "Total number of audio processing errors",
		},
		[]string{"operation", "error_type"},
	)
	// AI processing metrics
	AIProcessingTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "ai_processing_total",
			Help: "Total number of AI processing attempts",
		},
		[]string{"model", "operation"},
	)
	AIProcessingDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "ai_processing_duration_seconds",
			Help:    "Duration of AI processing operations",
			Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60},
		},
		[]string{"model", "operation"},
	)
	AIProcessingErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "ai_processing_errors_total",
			Help: "Total number of AI processing errors",
		},
		[]string{"model", "error_type"},
	)
)
// init registers all metrics with Prometheus
func init() {
	// Pre-create some common label combinations to avoid runtime initialization
	HttpRequestsTotal.WithLabelValues("200", "GET", "/health").Add(0)
	DatabaseQueryDuration.WithLabelValues("track_get").Observe(0)
	CacheHits.WithLabelValues("track").Add(0)
	CacheMisses.WithLabelValues("track").Add(0)
	AIRequestDuration.WithLabelValues("openai").Observe(0)
	TracksProcessed.WithLabelValues("success").Add(0)
	DatabaseConnections.WithLabelValues("active").Set(0)
	SubscriptionsByPlan.WithLabelValues("free").Set(0)
}
</file>

<file path="internal/pkg/metrics/operations.go">
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
var (
	// AudioOps tracks all audio operations
	AudioOps = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "audio_operations_total",
			Help: "Total number of audio operations by type and status",
		},
		[]string{"operation", "status"},
	)
	// AudioOpDurations tracks durations of audio operations
	AudioOpDurations = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "audio_operation_duration_seconds",
			Help:    "Duration of audio operations in seconds",
			Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30},
		},
		[]string{"operation"},
	)
	// AudioOpErrors tracks errors in audio operations
	AudioOpErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "audio_operation_errors_total",
			Help: "Total number of audio operation errors",
		},
		[]string{"operation", "error_type"},
	)
)
</file>

<file path="internal/pkg/metrics/queue.go">
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
// QueueMetrics holds metrics for queue operations
type QueueMetrics struct {
	// Publishing metrics
	MessagesPublished *prometheus.CounterVec
	PublishErrors     *prometheus.CounterVec
	PublishLatency    *prometheus.HistogramVec
	// Processing metrics
	MessagesProcessed  *prometheus.CounterVec
	ProcessingErrors   *prometheus.CounterVec
	ProcessingLatency  *prometheus.HistogramVec
	DeadLetters        *prometheus.CounterVec
	SubscriptionErrors *prometheus.CounterVec
}
// NewQueueMetrics creates a new QueueMetrics instance
func NewQueueMetrics() *QueueMetrics {
	return &QueueMetrics{
		MessagesPublished: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_messages_published_total",
				Help: "Total number of messages published",
			},
			[]string{"topic"},
		),
		PublishErrors: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_publish_errors_total",
				Help: "Total number of publish errors",
			},
			[]string{"topic"},
		),
		PublishLatency: promauto.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "queue_publish_latency_seconds",
				Help:    "Time taken to publish messages",
				Buckets: []float64{0.01, 0.05, 0.1, 0.5, 1, 2, 5},
			},
			[]string{"topic"},
		),
		MessagesProcessed: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_messages_processed_total",
				Help: "Total number of messages processed",
			},
			[]string{"subscription"},
		),
		ProcessingErrors: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_processing_errors_total",
				Help: "Total number of processing errors",
			},
			[]string{"subscription"},
		),
		ProcessingLatency: promauto.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "queue_processing_latency_seconds",
				Help:    "Time taken to process messages",
				Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30},
			},
			[]string{"subscription"},
		),
		DeadLetters: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_dead_letters_total",
				Help: "Total number of messages sent to dead letter queue",
			},
			[]string{"type"},
		),
		SubscriptionErrors: promauto.NewCounterVec(
			prometheus.CounterOpts{
				Name: "queue_subscription_errors_total",
				Help: "Total number of subscription errors",
			},
			[]string{"subscription"},
		),
	}
}
var (
	// Queue operation metrics
	QueueOperations = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "queue_operations_total",
		Help: "Total number of queue operations",
	}, []string{"operation", "topic", "status"}) // operation: publish, subscribe, ack, nack; status: success, failure
	// Message processing metrics
	MessageProcessingDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "message_processing_duration_seconds",
		Help:    "Duration of message processing",
		Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
	}, []string{"topic"})
	// Message status metrics
	MessageStatus = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "messages_by_status",
		Help: "Number of messages by status",
	}, []string{"topic", "status"}) // status: pending, processing, completed, failed, retrying, dead_letter
	// Retry metrics
	MessageRetries = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "message_retries_total",
		Help: "Total number of message retry attempts",
	}, []string{"topic"})
	// Dead letter metrics
	DeadLetterMessages = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "dead_letter_messages",
		Help: "Number of messages in dead letter queue",
	}, []string{"topic"})
	// Queue size metrics
	QueueSize = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "queue_size",
		Help: "Current size of the queue",
	}, []string{"topic"})
	// Processing error metrics
	ProcessingErrors = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "processing_errors_total",
		Help: "Total number of processing errors",
	}, []string{"topic", "error_type"}) // error_type: timeout, validation, processing, etc.
	// Batch processing metrics
	BatchProcessingDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "batch_processing_duration_seconds",
		Help:    "Duration of batch message processing",
		Buckets: []float64{.01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
	}, []string{"topic"})
)
</file>

<file path="internal/pkg/metrics/storage_metrics.go">
package metrics
import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)
// Storage metrics
var (
	// StorageOperationDuration tracks duration of storage operations
	StorageOperationDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "storage_operation_duration_seconds",
			Help:    "Duration of storage operations",
			Buckets: []float64{0.1, 0.5, 1, 2, 5, 10},
		},
		[]string{"operation"},
	)
	// StorageOperationErrors tracks storage operation errors
	StorageOperationErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "storage_operation_errors_total",
			Help: "Total number of storage operation errors",
		},
		[]string{"operation"},
	)
	// StorageOperationSuccess tracks successful storage operations
	StorageOperationSuccess = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "storage_operation_success_total",
			Help: "Total number of successful storage operations",
		},
		[]string{"operation"},
	)
	// StorageQuotaUsage tracks storage quota usage by user
	StorageQuotaUsage = promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "storage_quota_usage_bytes",
			Help: "Current storage quota usage in bytes",
		},
		[]string{"user_id"},
	)
	// StorageCleanupFilesDeleted tracks number of files deleted during cleanup
	StorageCleanupFilesDeleted = promauto.NewCounter(
		prometheus.CounterOpts{
			Name: "storage_cleanup_files_deleted_total",
			Help: "Total number of temporary files deleted during cleanup",
		},
	)
	// StorageCleanupBytesReclaimed tracks storage space reclaimed during cleanup
	StorageCleanupBytesReclaimed = promauto.NewCounter(
		prometheus.CounterOpts{
			Name: "storage_cleanup_bytes_reclaimed_total",
			Help: "Total bytes reclaimed from deleted temporary files",
		},
	)
	// StorageTempFileCount tracks the number of temporary files
	StorageTempFileCount = promauto.NewGauge(
		prometheus.GaugeOpts{
			Name: "storage_temp_file_count",
			Help: "Number of temporary files in storage",
		},
	)
)
</file>

<file path="internal/pkg/metrics/timer.go">
package metrics
import (
	"time"
	"github.com/prometheus/client_golang/prometheus"
)
// Timer is a utility for timing operations and recording their duration
type Timer struct {
	observer prometheus.Observer
	start    time.Time
}
// NewTimer creates a new timer that will observe the duration using the given observer
func NewTimer(observer prometheus.Observer) *Timer {
	return &Timer{
		observer: observer,
		start:    time.Now(),
	}
}
// ObserveDuration records the duration since the timer was created
func (t *Timer) ObserveDuration() {
	t.observer.Observe(time.Since(t.start).Seconds())
}
</file>

<file path="internal/pkg/middleware/auth.go">
package middleware
import (
	"metadatatool/internal/pkg/domain"
	"net/http"
	"strings"
	"github.com/gin-gonic/gin"
)
// AuthMiddleware creates a new authentication middleware
func AuthMiddleware(authService domain.AuthService) gin.HandlerFunc {
	return func(c *gin.Context) {
		// Get the Authorization header
		authHeader := c.GetHeader("Authorization")
		if authHeader == "" {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "missing authorization header",
			})
			c.Abort()
			return
		}
		// Check if it's a Bearer token
		parts := strings.Split(authHeader, " ")
		if len(parts) != 2 || parts[0] != "Bearer" {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "invalid authorization header format",
			})
			c.Abort()
			return
		}
		// Validate the token
		claims, err := authService.ValidateToken(parts[1])
		if err != nil {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "invalid token",
			})
			c.Abort()
			return
		}
		// Store claims in context for later use
		c.Set("user", claims)
		c.Next()
	}
}
// RoleGuard creates a middleware for role-based access control
func RoleGuard(roles ...domain.Role) gin.HandlerFunc {
	return func(c *gin.Context) {
		claims, exists := c.Get("user")
		if !exists {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "missing user claims",
			})
			c.Abort()
			return
		}
		userClaims, ok := claims.(*domain.Claims)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{
				"error": "invalid user claims",
			})
			c.Abort()
			return
		}
		// Check if user's role is allowed
		for _, role := range roles {
			if userClaims.Role == role {
				c.Next()
				return
			}
		}
		c.JSON(http.StatusForbidden, gin.H{
			"error": "insufficient permissions",
		})
		c.Abort()
	}
}
// APIKeyAuth creates a middleware for API key authentication
func APIKeyAuth(userRepo domain.UserRepository) gin.HandlerFunc {
	return func(c *gin.Context) {
		apiKey := c.GetHeader("X-API-Key")
		if apiKey == "" {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "missing API key",
			})
			c.Abort()
			return
		}
		user, err := userRepo.GetByAPIKey(c, apiKey)
		if err != nil || user == nil {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "invalid API key",
			})
			c.Abort()
			return
		}
		// Store user in context
		c.Set("user", &domain.Claims{
			UserID: user.ID,
			Email:  user.Email,
			Role:   user.Role,
		})
		c.Next()
	}
}
</file>

<file path="internal/pkg/middleware/logger.go">
package middleware
import (
	"fmt"
	"metadatatool/internal/pkg/logger"
	"time"
	"github.com/gofiber/fiber/v2"
)
// Logger creates a logging middleware for Fiber
func Logger() fiber.Handler {
	log := logger.NewLogger()
	return func(c *fiber.Ctx) error {
		start := time.Now()
		path := c.Path()
		method := c.Method()
		// Log request
		log.WithFields(logger.Fields{
			"method": method,
			"path":   path,
			"ip":     c.IP(),
		}).Info("Incoming request")
		// Handle request
		err := c.Next()
		// Calculate duration
		duration := time.Since(start)
		// Get response status
		status := c.Response().StatusCode()
		// Log fields
		fields := logger.Fields{
			"method":   method,
			"path":     path,
			"status":   status,
			"duration": fmt.Sprintf("%dms", duration.Milliseconds()),
			"ip":       c.IP(),
		}
		// Add error if present
		if err != nil {
			fields["error"] = err.Error()
			log.WithFields(fields).Error("Request failed")
			return err
		}
		// Log success
		log.WithFields(fields).Info("Request completed")
		return nil
	}
}
// RequestID adds a unique request ID to each request
func RequestID() fiber.Handler {
	log := logger.NewLogger()
	return func(c *fiber.Ctx) error {
		requestID := c.Get("X-Request-ID")
		if requestID == "" {
			requestID = fmt.Sprintf("%d", time.Now().UnixNano())
			c.Set("X-Request-ID", requestID)
		}
		log.WithFields(logger.Fields{
			"request_id": requestID,
		}).Info("Request ID assigned")
		return c.Next()
	}
}
</file>

<file path="internal/pkg/middleware/metrics.go">
// Package middleware provides HTTP middleware functions
package middleware
import (
	"fmt"
	"metadatatool/internal/pkg/metrics"
	"strconv"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/prometheus/client_golang/prometheus"
)
var (
	httpRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Total number of HTTP requests",
		},
		[]string{"method", "path", "status"},
	)
	httpRequestDuration = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "http_request_duration_seconds",
			Help:    "HTTP request duration in seconds",
			Buckets: prometheus.DefBuckets,
		},
		[]string{"method", "path", "status"},
	)
)
func init() {
	prometheus.MustRegister(httpRequestsTotal)
	prometheus.MustRegister(httpRequestDuration)
}
// MetricsMiddleware returns a middleware that collects HTTP metrics
func MetricsMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		// Start timer
		start := time.Now()
		// Increment active users
		metrics.ActiveUsers.Inc()
		defer metrics.ActiveUsers.Dec()
		// Process request
		c.Next()
		// Record metrics after request is processed
		duration := time.Since(start).Seconds()
		path := c.FullPath()
		if path == "" {
			path = "unknown"
		}
		method := c.Request.Method
		status := strconv.Itoa(c.Writer.Status())
		// Record request total
		httpRequestsTotal.WithLabelValues(method, path, status).Inc()
		// Record request duration
		httpRequestDuration.WithLabelValues(method, path, status).Observe(duration)
		// Record errors if any
		if len(c.Errors) > 0 {
			// Log errors for monitoring
			for _, err := range c.Errors {
				fmt.Printf("Error in request: %v\n", err)
			}
		}
	}
}
// DatabaseMetricsMiddleware wraps database operations with metrics
func DatabaseMetricsMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		start := time.Now()
		// Process request
		c.Next()
		// Record database metrics if any database operation was performed
		if dbOp, exists := c.Get("db_operation"); exists {
			duration := time.Since(start).Seconds()
			operation := dbOp.(string)
			metrics.DatabaseQueryDuration.WithLabelValues(operation).Observe(duration)
			metrics.DatabaseOperationsTotal.WithLabelValues(operation, "success").Inc()
			// Record rows affected if available
			if rowsAffected, exists := c.Get("db_rows_affected"); exists {
				metrics.DatabaseRowsAffected.WithLabelValues(operation).Observe(float64(rowsAffected.(int64)))
			}
			// Record errors if any
			if err, exists := c.Get("db_error"); exists && err != nil {
				metrics.DatabaseErrors.WithLabelValues(operation, "error").Inc()
			}
		}
	}
}
// AIMetricsMiddleware wraps AI service operations with metrics
func AIMetricsMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		start := time.Now()
		// Process request
		c.Next()
		// Record AI metrics if any AI operation was performed
		if aiOp, exists := c.Get("ai_operation"); exists {
			duration := time.Since(start).Seconds()
			operation := aiOp.(string)
			metrics.AIRequestDuration.WithLabelValues(operation).Observe(duration)
			// Record provider-specific metrics
			if provider, exists := c.Get("ai_provider"); exists {
				providerStr := provider.(string)
				metrics.AIRequestTotal.WithLabelValues(providerStr, "success").Inc()
				// Record confidence score if available
				if confidence, exists := c.Get("ai_confidence"); exists {
					metrics.AIConfidenceScore.WithLabelValues(providerStr).Observe(confidence.(float64))
				}
				// Record batch size if available
				if batchSize, exists := c.Get("ai_batch_size"); exists {
					metrics.AIBatchSize.WithLabelValues(providerStr).Observe(float64(batchSize.(int)))
				}
				// Record fallback if used
				if fallbackProvider, exists := c.Get("ai_fallback_provider"); exists {
					metrics.AIFallbackTotal.WithLabelValues(providerStr, fallbackProvider.(string)).Inc()
				}
				// Record errors if any
				if err, exists := c.Get("ai_error"); exists && err != nil {
					metrics.AIErrorTotal.WithLabelValues(providerStr, "error").Inc()
				}
			}
		}
	}
}
// CacheMetricsMiddleware wraps cache operations with metrics
func CacheMetricsMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		// Process request
		c.Next()
		// Record cache metrics if any cache operation was performed
		if cacheType, exists := c.Get("cache_type"); exists {
			cacheTypeStr := cacheType.(string)
			// Record hits/misses
			if hit, exists := c.Get("cache_hit"); exists {
				if hit.(bool) {
					metrics.CacheHits.WithLabelValues(cacheTypeStr).Inc()
				} else {
					metrics.CacheMisses.WithLabelValues(cacheTypeStr).Inc()
				}
			}
		}
	}
}
func Metrics() gin.HandlerFunc {
	return func(c *gin.Context) {
		start := time.Now()
		c.Next()
		status := strconv.Itoa(c.Writer.Status())
		duration := time.Since(start).Seconds()
		httpRequestsTotal.WithLabelValues(c.Request.Method, c.Request.URL.Path, status).Inc()
		httpRequestDuration.WithLabelValues(c.Request.Method, c.Request.URL.Path, status).Observe(duration)
	}
}
</file>

<file path="internal/pkg/middleware/ratelimit.go">
package middleware
import (
	"fmt"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"strconv"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/go-redis/redis/v8"
)
type RateLimitConfig struct {
	RequestsPerMinute int
	BurstSize         int
	RedisClient       *redis.Client
}
// RateLimit creates a rate limiting middleware using Redis
func RateLimit(cfg RateLimitConfig) gin.HandlerFunc {
	return func(c *gin.Context) {
		// Get user identifier (API key or user ID)
		identifier := getUserIdentifier(c)
		if identifier == "" {
			c.JSON(http.StatusUnauthorized, gin.H{
				"error": "missing authentication",
			})
			c.Abort()
			return
		}
		// Create Redis keys
		countKey := fmt.Sprintf("ratelimit:%s:count", identifier)
		timestampKey := fmt.Sprintf("ratelimit:%s:ts", identifier)
		// Get current count and timestamp
		pipe := cfg.RedisClient.Pipeline()
		countCmd := pipe.Get(c, countKey)
		timestampCmd := pipe.Get(c, timestampKey)
		_, err := pipe.Exec(c)
		var count int
		var lastTimestamp time.Time
		if err == redis.Nil {
			// First request
			count = 0
			lastTimestamp = time.Now()
		} else if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{
				"error": "rate limit check failed",
			})
			c.Abort()
			return
		} else {
			count, _ = strconv.Atoi(countCmd.Val())
			ts, _ := strconv.ParseInt(timestampCmd.Val(), 10, 64)
			lastTimestamp = time.Unix(ts, 0)
		}
		// Reset count if minute window has passed
		now := time.Now()
		if now.Sub(lastTimestamp) >= time.Minute {
			count = 0
			lastTimestamp = now
		}
		// Check rate limit
		if count >= cfg.RequestsPerMinute {
			c.Header("X-RateLimit-Limit", strconv.Itoa(cfg.RequestsPerMinute))
			c.Header("X-RateLimit-Remaining", "0")
			c.Header("X-RateLimit-Reset", strconv.FormatInt(lastTimestamp.Add(time.Minute).Unix(), 10))
			c.JSON(http.StatusTooManyRequests, gin.H{
				"error":       "rate limit exceeded",
				"retry_after": lastTimestamp.Add(time.Minute).Unix(),
			})
			c.Abort()
			return
		}
		// Update rate limit in Redis
		pipe = cfg.RedisClient.Pipeline()
		pipe.Set(c, countKey, count+1, time.Minute)
		pipe.Set(c, timestampKey, lastTimestamp.Unix(), time.Minute)
		_, err = pipe.Exec(c)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{
				"error": "rate limit update failed",
			})
			c.Abort()
			return
		}
		// Set rate limit headers
		c.Header("X-RateLimit-Limit", strconv.Itoa(cfg.RequestsPerMinute))
		c.Header("X-RateLimit-Remaining", strconv.Itoa(cfg.RequestsPerMinute-count-1))
		c.Header("X-RateLimit-Reset", strconv.FormatInt(lastTimestamp.Add(time.Minute).Unix(), 10))
		c.Next()
	}
}
func getUserIdentifier(c *gin.Context) string {
	// Try API key first
	if apiKey := c.GetHeader("X-API-Key"); apiKey != "" {
		return apiKey
	}
	// Then try user claims from JWT
	if claims, exists := c.Get("user"); exists {
		if userClaims, ok := claims.(*domain.Claims); ok {
			return userClaims.UserID
		}
	}
	// Finally use IP address
	return c.ClientIP()
}
</file>

<file path="internal/pkg/middleware/sentry.go">
package middleware
import (
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/errortracking"
	"runtime/debug"
	"time"
	"github.com/getsentry/sentry-go"
	"github.com/gofiber/fiber/v2"
)
// SentryMiddleware creates a middleware for Sentry error tracking
func SentryMiddleware() fiber.Handler {
	errorTracker := errortracking.NewErrorTracker()
	return func(c *fiber.Ctx) error {
		// Start a new transaction
		hub := sentry.CurrentHub().Clone()
		transaction := sentry.StartTransaction(
			c.Context(),
			fmt.Sprintf("%s %s", c.Method(), c.Route().Path),
		)
		defer transaction.Finish()
		// Set transaction on hub
		hub.ConfigureScope(func(scope *sentry.Scope) {
			scope.SetTag("http.method", c.Method())
			scope.SetTag("http.url", c.OriginalURL())
			scope.SetTag("http.route", c.Route().Path)
			scope.SetTag("http.host", c.Hostname())
			scope.SetTag("http.remote_addr", c.IP())
			// Add user context if available
			if user := c.Locals("user"); user != nil {
				if claims, ok := user.(*domain.Claims); ok {
					scope.SetUser(sentry.User{
						ID:       claims.UserID,
						Email:    claims.Email,
						Username: claims.Email,
					})
				}
			}
			// Add request data as tags since we can't use SetRequest directly
			scope.SetTag("request.host", string(c.Request().Host()))
			scope.SetTag("request.uri", string(c.Request().RequestURI()))
		})
		// Create a child span for the handler execution
		span := transaction.StartChild("handler.execution")
		start := time.Now()
		// Handle panics
		defer func() {
			if err := recover(); err != nil {
				stack := debug.Stack()
				hub.ConfigureScope(func(scope *sentry.Scope) {
					scope.SetLevel(sentry.LevelFatal)
					scope.SetExtra("stacktrace", string(stack))
				})
				hub.RecoverWithContext(
					c.Context(),
					err,
				)
				// Ensure event is sent before panic continues
				hub.Flush(2 * time.Second)
				// Re-panic after sending to Sentry
				panic(err)
			}
		}()
		// Process request
		err := c.Next()
		// Finish span
		span.Finish()
		// Record request duration as a tag since SetMeasurement is not available
		duration := time.Since(start)
		hub.ConfigureScope(func(scope *sentry.Scope) {
			scope.SetTag("request.duration_ms", fmt.Sprintf("%d", duration.Milliseconds()))
		})
		// Capture any errors
		if err != nil {
			hub.ConfigureScope(func(scope *sentry.Scope) {
				scope.SetLevel(sentry.LevelError)
				scope.SetTag("http.status_code", fmt.Sprintf("%d", c.Response().StatusCode()))
				scope.SetExtra("request.params", c.AllParams())
				scope.SetExtra("request.query", c.Queries())
			})
			errorTracker.CaptureError(err, map[string]string{
				"component": "http",
				"method":    c.Method(),
				"path":      c.Route().Path,
			})
		}
		return err
	}
}
</file>

<file path="internal/pkg/middleware/tracing.go">
package middleware
import (
	"github.com/gin-gonic/gin"
	"go.opentelemetry.io/otel/trace"
)
func Tracing(tracer trace.Tracer) gin.HandlerFunc {
	return func(c *gin.Context) {
		ctx, span := tracer.Start(c.Request.Context(), c.Request.URL.Path)
		defer span.End()
		c.Request = c.Request.WithContext(ctx)
		c.Next()
	}
}
</file>

<file path="internal/pkg/monitoring/tracing/config.go">
package tracing
import (
	"fmt"
	"time"
)
// Config holds configuration for OpenTelemetry tracing
type Config struct {
	ServiceName    string        `env:"OTEL_SERVICE_NAME,required"`
	CollectorURL   string        `env:"OTEL_COLLECTOR_URL" envDefault:"http://localhost:4318"`
	SampleRate     float64       `env:"OTEL_SAMPLE_RATE" envDefault:"1.0"`
	Timeout        time.Duration `env:"OTEL_TIMEOUT" envDefault:"5s"`
	BatchSize      int           `env:"OTEL_BATCH_SIZE" envDefault:"100"`
	ExportInterval time.Duration `env:"OTEL_EXPORT_INTERVAL" envDefault:"1s"`
}
// Validate checks if the configuration is valid
func (c *Config) Validate() error {
	if c.ServiceName == "" {
		return fmt.Errorf("service name is required")
	}
	if c.SampleRate < 0 || c.SampleRate > 1 {
		return fmt.Errorf("sample rate must be between 0 and 1")
	}
	return nil
}
</file>

<file path="internal/pkg/monitoring/tracing/provider.go">
package tracing
import (
	"context"
	"fmt"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
	"go.opentelemetry.io/otel/propagation"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.24.0"
	"go.opentelemetry.io/otel/trace"
)
// Provider manages the OpenTelemetry tracer provider
type Provider struct {
	tp     *sdktrace.TracerProvider
	config *Config
}
// NewProvider creates a new OpenTelemetry tracer provider
func NewProvider(ctx context.Context, config *Config) (*Provider, error) {
	if err := config.Validate(); err != nil {
		return nil, fmt.Errorf("invalid tracing config: %w", err)
	}
	// Create OTLP exporter
	exporter, err := otlptracehttp.New(ctx,
		otlptracehttp.WithEndpoint(config.CollectorURL),
		otlptracehttp.WithTimeout(config.Timeout),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create OTLP exporter: %w", err)
	}
	// Create resource with service information
	res, err := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceName(config.ServiceName),
			semconv.ServiceVersion("1.0.0"),
		),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create resource: %w", err)
	}
	// Create TracerProvider
	tp := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter,
			sdktrace.WithMaxExportBatchSize(config.BatchSize),
			sdktrace.WithBatchTimeout(config.ExportInterval),
		),
		sdktrace.WithResource(res),
		sdktrace.WithSampler(sdktrace.TraceIDRatioBased(config.SampleRate)),
	)
	// Set global TracerProvider and propagator
	otel.SetTracerProvider(tp)
	otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
		propagation.TraceContext{},
		propagation.Baggage{},
	))
	return &Provider{
		tp:     tp,
		config: config,
	}, nil
}
// Shutdown cleanly shuts down the tracer provider
func (p *Provider) Shutdown(ctx context.Context) error {
	return p.tp.Shutdown(ctx)
}
// Tracer returns a named tracer
func (p *Provider) Tracer(name string) trace.Tracer {
	return p.tp.Tracer(name)
}
</file>

<file path="internal/pkg/monitoring/tracing/queue_middleware.go">
package tracing
import (
	"context"
	"metadatatool/internal/pkg/domain"
	"strconv"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/codes"
	"go.opentelemetry.io/otel/trace"
)
// QueueTracingMiddleware wraps a QueueService with OpenTelemetry tracing
type QueueTracingMiddleware struct {
	next   domain.QueueService
	tracer trace.Tracer
}
// NewQueueTracingMiddleware creates a new tracing middleware for queue operations
func NewQueueTracingMiddleware(next domain.QueueService, tracer trace.Tracer) *QueueTracingMiddleware {
	return &QueueTracingMiddleware{
		next:   next,
		tracer: tracer,
	}
}
// Publish traces the publish operation
func (m *QueueTracingMiddleware) Publish(ctx context.Context, topic string, message *domain.Message, priority domain.QueuePriority) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.Publish",
		trace.WithAttributes(
			attribute.String("topic", topic),
			attribute.String("message.id", message.ID),
			attribute.String("message.type", message.Type),
			attribute.String("priority", strconv.Itoa(int(priority))),
		),
	)
	defer span.End()
	err := m.next.Publish(ctx, topic, message, priority)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// Subscribe traces the subscribe operation
func (m *QueueTracingMiddleware) Subscribe(ctx context.Context, topic string, handler domain.MessageHandler) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.Subscribe",
		trace.WithAttributes(
			attribute.String("topic", topic),
		),
	)
	defer span.End()
	// Wrap the handler to add tracing
	tracedHandler := func(ctx context.Context, msg *domain.Message) error {
		handlerCtx, handlerSpan := m.tracer.Start(ctx, "QueueService.MessageHandler",
			trace.WithAttributes(
				attribute.String("message.id", msg.ID),
				attribute.String("message.type", msg.Type),
			),
		)
		defer handlerSpan.End()
		err := handler(handlerCtx, msg)
		if err != nil {
			handlerSpan.SetStatus(codes.Error, err.Error())
			handlerSpan.RecordError(err)
		}
		return err
	}
	err := m.next.Subscribe(ctx, topic, tracedHandler)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// HandleDeadLetter traces the dead letter handling operation
func (m *QueueTracingMiddleware) HandleDeadLetter(ctx context.Context, topic string, message *domain.Message) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.HandleDeadLetter",
		trace.WithAttributes(
			attribute.String("topic", topic),
			attribute.String("message.id", message.ID),
			attribute.String("message.type", message.Type),
			attribute.Int("retry_count", message.RetryCount),
		),
	)
	defer span.End()
	err := m.next.HandleDeadLetter(ctx, topic, message)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// GetMessage traces the get message operation
func (m *QueueTracingMiddleware) GetMessage(ctx context.Context, id string) (*domain.Message, error) {
	ctx, span := m.tracer.Start(ctx, "QueueService.GetMessage",
		trace.WithAttributes(
			attribute.String("message.id", id),
		),
	)
	defer span.End()
	msg, err := m.next.GetMessage(ctx, id)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return msg, err
}
// RetryMessage traces the retry message operation
func (m *QueueTracingMiddleware) RetryMessage(ctx context.Context, id string) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.RetryMessage",
		trace.WithAttributes(
			attribute.String("message.id", id),
		),
	)
	defer span.End()
	err := m.next.RetryMessage(ctx, id)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// AckMessage traces the acknowledge message operation
func (m *QueueTracingMiddleware) AckMessage(ctx context.Context, id string) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.AckMessage",
		trace.WithAttributes(
			attribute.String("message.id", id),
		),
	)
	defer span.End()
	err := m.next.AckMessage(ctx, id)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// NackMessage traces the negative acknowledge message operation
func (m *QueueTracingMiddleware) NackMessage(ctx context.Context, id string, nackErr error) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.NackMessage",
		trace.WithAttributes(
			attribute.String("message.id", id),
			attribute.String("error", nackErr.Error()),
		),
	)
	defer span.End()
	err := m.next.NackMessage(ctx, id, nackErr)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// ListDeadLetters traces the list dead letters operation
func (m *QueueTracingMiddleware) ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*domain.Message, error) {
	ctx, span := m.tracer.Start(ctx, "QueueService.ListDeadLetters",
		trace.WithAttributes(
			attribute.String("topic", topic),
			attribute.Int("offset", offset),
			attribute.Int("limit", limit),
		),
	)
	defer span.End()
	msgs, err := m.next.ListDeadLetters(ctx, topic, offset, limit)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return msgs, err
}
// ReplayDeadLetter traces the replay dead letter operation
func (m *QueueTracingMiddleware) ReplayDeadLetter(ctx context.Context, id string) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.ReplayDeadLetter",
		trace.WithAttributes(
			attribute.String("message.id", id),
		),
	)
	defer span.End()
	err := m.next.ReplayDeadLetter(ctx, id)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// PurgeDeadLetters traces the purge dead letters operation
func (m *QueueTracingMiddleware) PurgeDeadLetters(ctx context.Context, topic string) error {
	ctx, span := m.tracer.Start(ctx, "QueueService.PurgeDeadLetters",
		trace.WithAttributes(
			attribute.String("topic", topic),
		),
	)
	defer span.End()
	err := m.next.PurgeDeadLetters(ctx, topic)
	if err != nil {
		span.SetStatus(codes.Error, err.Error())
		span.RecordError(err)
	}
	return err
}
// Close traces the close operation
func (m *QueueTracingMiddleware) Close() error {
	return m.next.Close()
}
</file>

<file path="internal/pkg/queue/queue.go">
package queue
import (
	"context"
	"metadatatool/internal/pkg/domain"
	"github.com/redis/go-redis/v9"
)
type RedisQueue struct {
	client *redis.Client
	config domain.QueueConfig
}
func NewRedisQueue(client *redis.Client, config domain.QueueConfig) *RedisQueue {
	return &RedisQueue{
		client: client,
		config: config,
	}
}
func (q *RedisQueue) Close() error {
	return q.client.Close()
}
func (q *RedisQueue) Publish(ctx context.Context, topic string, data []byte) error {
	// TODO: Implement queue publishing
	return nil
}
func (q *RedisQueue) PublishWithRetry(ctx context.Context, topic string, data []byte, maxRetries int) error {
	// TODO: Implement queue publishing with retry
	return nil
}
func (q *RedisQueue) Subscribe(ctx context.Context, topic string, handler domain.MessageHandler) error {
	// TODO: Implement queue subscription
	return nil
}
func (q *RedisQueue) Unsubscribe(ctx context.Context, topic string) error {
	// TODO: Implement queue unsubscription
	return nil
}
func (q *RedisQueue) GetMessage(ctx context.Context, id string) (*domain.Message, error) {
	// TODO: Implement message retrieval
	return nil, nil
}
func (q *RedisQueue) RetryMessage(ctx context.Context, id string) error {
	// TODO: Implement message retry
	return nil
}
func (q *RedisQueue) AckMessage(ctx context.Context, id string) error {
	// TODO: Implement message acknowledgment
	return nil
}
func (q *RedisQueue) NackMessage(ctx context.Context, id string, err error) error {
	// TODO: Implement message negative acknowledgment
	return nil
}
func (q *RedisQueue) ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*domain.Message, error) {
	// TODO: Implement dead letter queue listing
	return nil, nil
}
func (q *RedisQueue) ReplayDeadLetter(ctx context.Context, id string) error {
	// TODO: Implement dead letter replay
	return nil
}
func (q *RedisQueue) PurgeDeadLetters(ctx context.Context, topic string) error {
	// TODO: Implement dead letter purge
	return nil
}
</file>

<file path="internal/pkg/retry/retry.go">
package retry
import (
	"time"
)
// Function represents a function that can be retried
type Function func() error
// Option represents a retry option
type Option func(*Config)
// Config holds retry configuration
type Config struct {
	attempts uint
	delay    time.Duration
	maxDelay time.Duration
	onRetry  func(uint, error)
}
// Attempts sets the number of retry attempts
func Attempts(attempts uint) Option {
	return func(c *Config) {
		c.attempts = attempts
	}
}
// Delay sets the delay between retries
func Delay(delay time.Duration) Option {
	return func(c *Config) {
		c.delay = delay
	}
}
// MaxDelay sets the maximum delay between retries
func MaxDelay(maxDelay time.Duration) Option {
	return func(c *Config) {
		c.maxDelay = maxDelay
	}
}
// OnRetry sets the function to call on retry
func OnRetry(onRetry func(uint, error)) Option {
	return func(c *Config) {
		c.onRetry = onRetry
	}
}
// Do executes the function with retry logic
func Do(fn Function, opts ...Option) error {
	config := &Config{
		attempts: 1,
		delay:    1 * time.Second,
		maxDelay: 30 * time.Second,
	}
	for _, opt := range opts {
		opt(config)
	}
	var err error
	for attempt := uint(0); attempt < config.attempts; attempt++ {
		err = fn()
		if err == nil {
			return nil
		}
		if attempt+1 < config.attempts {
			if config.onRetry != nil {
				config.onRetry(attempt+1, err)
			}
			delay := config.delay * time.Duration(attempt+1)
			if delay > config.maxDelay {
				delay = config.maxDelay
			}
			time.Sleep(delay)
		}
	}
	return err
}
</file>

<file path="internal/pkg/session/redis.go">
package session
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"github.com/redis/go-redis/v9"
)
type RedisSessionStore struct {
	client *redis.Client
	config domain.SessionConfig
}
func NewRedisSessionStore(client *redis.Client, config domain.SessionConfig) *RedisSessionStore {
	return &RedisSessionStore{
		client: client,
		config: config,
	}
}
func (s *RedisSessionStore) Create(ctx context.Context, session *domain.Session) error {
	data, err := json.Marshal(session)
	if err != nil {
		return fmt.Errorf("failed to marshal session: %w", err)
	}
	key := fmt.Sprintf("session:%s", session.ID)
	if err := s.client.Set(ctx, key, data, s.config.SessionDuration).Err(); err != nil {
		return fmt.Errorf("failed to create session: %w", err)
	}
	// Add to user's sessions set
	userKey := fmt.Sprintf("user:%s:sessions", session.UserID)
	if err := s.client.SAdd(ctx, userKey, session.ID).Err(); err != nil {
		return fmt.Errorf("failed to add session to user set: %w", err)
	}
	return nil
}
func (s *RedisSessionStore) Get(ctx context.Context, id string) (*domain.Session, error) {
	key := fmt.Sprintf("session:%s", id)
	data, err := s.client.Get(ctx, key).Bytes()
	if err != nil {
		if err == redis.Nil {
			return nil, domain.ErrSessionNotFound
		}
		return nil, fmt.Errorf("failed to get session: %w", err)
	}
	var session domain.Session
	if err := json.Unmarshal(data, &session); err != nil {
		return nil, fmt.Errorf("failed to unmarshal session: %w", err)
	}
	return &session, nil
}
func (s *RedisSessionStore) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	userKey := fmt.Sprintf("user:%s:sessions", userID)
	sessionIDs, err := s.client.SMembers(ctx, userKey).Result()
	if err != nil {
		return nil, fmt.Errorf("failed to get user sessions: %w", err)
	}
	sessions := make([]*domain.Session, 0, len(sessionIDs))
	for _, id := range sessionIDs {
		session, err := s.Get(ctx, id)
		if err != nil {
			if err == domain.ErrSessionNotFound {
				continue
			}
			return nil, err
		}
		sessions = append(sessions, session)
	}
	return sessions, nil
}
func (s *RedisSessionStore) Update(ctx context.Context, session *domain.Session) error {
	return s.Create(ctx, session) // Same as create since we're using Redis SET
}
func (s *RedisSessionStore) Delete(ctx context.Context, id string) error {
	session, err := s.Get(ctx, id)
	if err != nil {
		return err
	}
	key := fmt.Sprintf("session:%s", id)
	if err := s.client.Del(ctx, key).Err(); err != nil {
		return fmt.Errorf("failed to delete session: %w", err)
	}
	userKey := fmt.Sprintf("user:%s:sessions", session.UserID)
	if err := s.client.SRem(ctx, userKey, id).Err(); err != nil {
		return fmt.Errorf("failed to remove session from user set: %w", err)
	}
	return nil
}
func (s *RedisSessionStore) DeleteUserSessions(ctx context.Context, userID string) error {
	sessions, err := s.GetUserSessions(ctx, userID)
	if err != nil {
		return err
	}
	for _, session := range sessions {
		if err := s.Delete(ctx, session.ID); err != nil {
			return err
		}
	}
	return nil
}
func (s *RedisSessionStore) DeleteExpired(ctx context.Context) error {
	// Redis automatically handles expiration through TTL
	return nil
}
func (s *RedisSessionStore) Touch(ctx context.Context, id string) error {
	// First verify the session exists
	if _, err := s.Get(ctx, id); err != nil {
		return err
	}
	key := fmt.Sprintf("session:%s", id)
	if err := s.client.Expire(ctx, key, s.config.SessionDuration).Err(); err != nil {
		return fmt.Errorf("failed to touch session: %w", err)
	}
	return nil
}
</file>

<file path="internal/pkg/storage/storage.go">
package storage
import (
	"context"
	"fmt"
	"io"
	pkgconfig "metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"path/filepath"
	"strings"
	"time"
	"github.com/aws/aws-sdk-go-v2/aws"
	awsconfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)
type StorageService struct {
	client *s3.Client
	bucket string
	cfg    *pkgconfig.StorageConfig
}
// NewStorageService creates a new storage service
func NewStorageService(cfg pkgconfig.StorageConfig) (domain.StorageService, error) {
	// Create AWS config
	awsCfg, err := awsconfig.LoadDefaultConfig(context.Background(),
		awsconfig.WithRegion(cfg.Region),
		awsconfig.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(
			cfg.AccessKey,
			cfg.SecretKey,
			"",
		)),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}
	// Create S3 client
	client := s3.NewFromConfig(awsCfg)
	return &StorageService{
		client: client,
		bucket: cfg.Bucket,
		cfg:    &cfg,
	}, nil
}
// Upload uploads a file to storage
func (s *StorageService) Upload(ctx context.Context, file *domain.StorageFile) error {
	input := &s3.PutObjectInput{
		Bucket:      aws.String(s.bucket),
		Key:         aws.String(file.Key),
		Body:        file.Content,
		ContentType: aws.String(file.ContentType),
		Metadata:    file.Metadata,
	}
	_, err := s.client.PutObject(ctx, input)
	if err != nil {
		return fmt.Errorf("failed to upload file: %w", err)
	}
	return nil
}
// UploadAudio uploads an audio file to storage
func (s *StorageService) UploadAudio(ctx context.Context, file io.Reader, path string) error {
	input := &s3.PutObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(path),
		Body:   file,
	}
	_, err := s.client.PutObject(ctx, input)
	if err != nil {
		return fmt.Errorf("failed to upload audio file: %w", err)
	}
	return nil
}
// GetURL returns a pre-signed URL for the given key
func (s *StorageService) GetURL(ctx context.Context, key string) (string, error) {
	presignClient := s3.NewPresignClient(s.client)
	request, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = time.Hour * 24 // 24 hour expiry
	})
	if err != nil {
		return "", fmt.Errorf("failed to generate pre-signed URL: %w", err)
	}
	return request.URL, nil
}
// Download downloads a file from storage
func (s *StorageService) Download(ctx context.Context, key string) (*domain.StorageFile, error) {
	result, err := s.client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to download file: %w", err)
	}
	// Get object metadata
	head, err := s.client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to get file metadata: %w", err)
	}
	// Create StorageFile with metadata
	file := &domain.StorageFile{
		Key:         key,
		Name:        filepath.Base(key),
		Size:        aws.ToInt64(head.ContentLength),
		ContentType: aws.ToString(head.ContentType),
		Content:     result.Body,
		Metadata:    make(map[string]string),
	}
	// Copy metadata
	for k, v := range head.Metadata {
		file.Metadata[k] = v // v is already a string, no need for aws.ToString
	}
	return file, nil
}
// Delete deletes a file from storage
func (s *StorageService) Delete(ctx context.Context, key string) error {
	_, err := s.client.DeleteObject(ctx, &s3.DeleteObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return fmt.Errorf("failed to delete file: %w", err)
	}
	return nil
}
// DeleteAudio deletes an audio file from storage
func (s *StorageService) DeleteAudio(ctx context.Context, path string) error {
	return s.Delete(ctx, path)
}
// GetSignedURL gets a pre-signed URL for a file
func (s *StorageService) GetSignedURL(ctx context.Context, path string, expiry time.Duration) (string, error) {
	presignClient := s3.NewPresignClient(s.client)
	request, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(path),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = expiry
	})
	if err != nil {
		return "", fmt.Errorf("failed to generate pre-signed URL: %w", err)
	}
	return request.URL, nil
}
// GetMetadata gets metadata for a file
func (s *StorageService) GetMetadata(ctx context.Context, key string) (*domain.FileMetadata, error) {
	result, err := s.client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to get file metadata: %w", err)
	}
	return &domain.FileMetadata{
		Key:          key,
		Size:         aws.ToInt64(result.ContentLength),
		ContentType:  aws.ToString(result.ContentType),
		LastModified: *result.LastModified,
		ETag:         aws.ToString(result.ETag),
		StorageClass: string(result.StorageClass),
	}, nil
}
// ListFiles lists files with the given prefix
func (s *StorageService) ListFiles(ctx context.Context, prefix string) ([]*domain.FileMetadata, error) {
	var files []*domain.FileMetadata
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
		Prefix: aws.String(prefix),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to list files: %w", err)
		}
		for _, obj := range page.Contents {
			files = append(files, &domain.FileMetadata{
				Key:          aws.ToString(obj.Key),
				Size:         aws.ToInt64(obj.Size),
				LastModified: *obj.LastModified,
				ETag:         aws.ToString(obj.ETag),
				StorageClass: string(obj.StorageClass),
			})
		}
	}
	return files, nil
}
// GetQuotaUsage gets the total storage usage
func (s *StorageService) GetQuotaUsage(ctx context.Context) (int64, error) {
	var totalSize int64
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			return 0, fmt.Errorf("failed to get quota usage: %w", err)
		}
		for _, obj := range page.Contents {
			totalSize += aws.ToInt64(obj.Size)
		}
	}
	return totalSize, nil
}
// ValidateUpload validates a file upload request
func (s *StorageService) ValidateUpload(ctx context.Context, fileSize int64, mimeType string) error {
	// Check file size
	if fileSize > s.cfg.MaxFileSize {
		return &domain.StorageError{
			Code:    "FILE_TOO_LARGE",
			Message: fmt.Sprintf("File size %d exceeds maximum allowed size %d", fileSize, s.cfg.MaxFileSize),
		}
	}
	// Check file type
	if !s.isAllowedFileType(mimeType) {
		return &domain.StorageError{
			Code:    "INVALID_FILE_TYPE",
			Message: fmt.Sprintf("File type %s is not allowed", mimeType),
		}
	}
	// Check quota
	quotaUsage, err := s.GetQuotaUsage(ctx)
	if err != nil {
		return fmt.Errorf("failed to check quota: %w", err)
	}
	if quotaUsage+fileSize > s.cfg.TotalQuota {
		return &domain.StorageError{
			Code:    "QUOTA_EXCEEDED",
			Message: "Storage quota exceeded",
		}
	}
	return nil
}
// isAllowedFileType checks if a file type is allowed
func (s *StorageService) isAllowedFileType(mimeType string) bool {
	for _, allowed := range s.cfg.AllowedFileTypes {
		if strings.EqualFold(mimeType, allowed) {
			return true
		}
	}
	return false
}
// CleanupTempFiles removes expired temporary files
func (s *StorageService) CleanupTempFiles(ctx context.Context) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("cleanup"))
	defer timer.ObserveDuration()
	var tempFiles int
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
		Prefix: aws.String(domain.StoragePathTemp.String()),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			metrics.StorageOperationErrors.WithLabelValues("cleanup").Inc()
			return fmt.Errorf("failed to list temp files: %w", err)
		}
		for _, obj := range page.Contents {
			if time.Since(*obj.LastModified) > s.cfg.TempFileExpiry {
				err := s.Delete(ctx, aws.ToString(obj.Key))
				if err != nil {
					metrics.StorageOperationErrors.WithLabelValues("cleanup").Inc()
					continue
				}
				tempFiles++
			}
		}
	}
	metrics.StorageTempFileCount.Set(float64(tempFiles))
	metrics.StorageOperationSuccess.WithLabelValues("cleanup").Inc()
	return nil
}
</file>

<file path="internal/pkg/tracing/provider.go">
package tracing
import (
	"context"
	"fmt"
	"time"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.4.0"
)
// Config holds configuration for OpenTelemetry tracing
type Config struct {
	Enabled     bool          `json:"enabled"`
	ServiceName string        `json:"service_name"`
	Endpoint    string        `json:"endpoint"`
	SampleRate  float64       `json:"sample_rate"`
	BatchSize   int           `json:"batch_size"`
	Timeout     time.Duration `json:"timeout"`
}
// Provider manages the OpenTelemetry tracer provider
type Provider struct {
	tp *sdktrace.TracerProvider
}
// NewProvider creates a new OpenTelemetry tracer provider
func NewProvider(ctx context.Context, cfg *Config) (*Provider, error) {
	if !cfg.Enabled {
		return &Provider{}, nil
	}
	exporter, err := otlptracegrpc.New(ctx,
		otlptracegrpc.WithEndpoint(cfg.Endpoint),
		otlptracegrpc.WithInsecure(),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create exporter: %w", err)
	}
	res, err := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceNameKey.String(cfg.ServiceName),
		),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create resource: %w", err)
	}
	tp := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter),
		sdktrace.WithResource(res),
		sdktrace.WithSampler(sdktrace.TraceIDRatioBased(cfg.SampleRate)),
	)
	otel.SetTracerProvider(tp)
	return &Provider{tp: tp}, nil
}
// Shutdown cleanly shuts down the tracer provider
func (p *Provider) Shutdown(ctx context.Context) error {
	if p.tp == nil {
		return nil
	}
	return p.tp.Shutdown(ctx)
}
// Tracer returns a named tracer
func (p *Provider) Tracer(name string) *sdktrace.TracerProvider {
	return p.tp
}
</file>

<file path="internal/pkg/utils/audio.go">
package utils
import "path/filepath"
// IsValidAudioFormat checks if the file extension is a supported audio format
func IsValidAudioFormat(filename string) bool {
	ext := filepath.Ext(filename)
	switch ext {
	case ".mp3", ".wav", ".flac", ".m4a", ".aac":
		return true
	default:
		return false
	}
}
// GetAudioFormat returns the audio format from the file extension
func GetAudioFormat(filename string) string {
	return filepath.Ext(filename)[1:] // Remove the dot
}
</file>

<file path="internal/pkg/validator/validator.go">
package validator
import (
	"metadatatool/internal/pkg/domain"
	"github.com/go-playground/validator/v10"
)
type Validator struct {
	validate *validator.Validate
}
func NewValidator() *Validator {
	return &Validator{
		validate: validator.New(),
	}
}
func (v *Validator) Validate(track *domain.Track) domain.ValidationResult {
	err := v.validate.Struct(track)
	if err == nil {
		return domain.ValidationResult{IsValid: true}
	}
	var errors []domain.ValidationError
	for _, err := range err.(validator.ValidationErrors) {
		errors = append(errors, domain.ValidationError{
			Field:   err.Field(),
			Message: err.Error(),
		})
	}
	return domain.ValidationResult{
		IsValid: false,
		Errors:  errors,
	}
}
func (v *Validator) Struct(s interface{}) error {
	return v.validate.Struct(s)
}
func (v *Validator) Var(field interface{}, tag string) error {
	return v.validate.Var(field, tag)
}
func (v *Validator) RegisterValidation(tag string, fn validator.Func) error {
	return v.validate.RegisterValidation(tag, fn)
}
</file>

<file path="internal/repository/ai/composite_service.go">
// Package ai provides AI services for metadata enrichment and analysis.
//
// This package implements a composite AI service that can use multiple AI providers
// and includes features like fallback handling, retries, and analytics tracking.
//
// Key features:
//   - Multiple AI provider support (OpenAI, Qwen2, etc.)
//   - Automatic fallback to backup providers
//   - Retry mechanism with exponential backoff
//   - Analytics tracking for experiments
//   - Concurrent request limiting
//
// Usage example:
//
//	config := &ai.Config{
//	    EnableFallback:        true,
//	    APIKey:               "your-api-key",
//	    Endpoint:             "https://api.openai.com/v1",
//	    TimeoutSeconds:       30,
//	    MinConfidence:        0.85,
//	    MaxConcurrentRequests: 10,
//	    RetryAttempts:        3,
//	    RetryBackoffSeconds:  2,
//	}
//
//	service, err := ai.NewCompositeAIService(config, analyticsService)
//	if err != nil {
//	    log.Fatal(err)
//	}
//
//	metadata, err := service.EnrichMetadata(ctx, track)
//	if err != nil {
//	    log.Printf("Failed to enrich metadata: %v", err)
//	}
package ai
import (
	"context"
	"fmt"
	"math/rand"
	"metadatatool/internal/pkg/analytics"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"sync"
	"time"
	"github.com/sirupsen/logrus"
)
// Config holds configuration for the AI service.
// It includes settings for timeouts, retries, and rate limiting.
type Config struct {
	// EnableFallback determines if the service should try backup providers
	EnableFallback bool
	// TimeoutSeconds is the maximum time to wait for an AI response
	TimeoutSeconds int
	// MinConfidence is the minimum confidence score required (0-1)
	MinConfidence float64
	// MaxConcurrentRequests limits the number of concurrent AI calls
	MaxConcurrentRequests int
	// RetryAttempts is the maximum number of retry attempts
	RetryAttempts int
	// RetryBackoffSeconds is the base delay between retries
	RetryBackoffSeconds int
	// OpenAIConfig contains OpenAI-specific configuration
	OpenAIConfig *OpenAIConfig
	// Qwen2Config contains Qwen2-specific configuration
	Qwen2Config *Qwen2Config
}
// OpenAIConfig contains configuration specific to the OpenAI provider.
type OpenAIConfig struct {
	// APIKey is the OpenAI API key
	APIKey string
	// Endpoint is the OpenAI API endpoint
	Endpoint string
	// Model is the specific model to use (e.g., "gpt-4")
	Model string
	// MaxTokens is the maximum number of tokens to generate
	MaxTokens int
}
// Qwen2Config contains configuration specific to the Qwen2 provider.
type Qwen2Config struct {
	// APIKey is the Qwen2 API key
	APIKey string
	// Endpoint is the Qwen2 API endpoint
	Endpoint string
	// Model is the specific model to use
	Model string
	// MaxTokens is the maximum number of tokens to generate
	MaxTokens int
}
// CompositeAIService orchestrates multiple AI providers and handles
// fallback, retries, and analytics tracking.
type CompositeAIService struct {
	config           *Config
	qwen2Service     domain.AIService
	openAIService    domain.AIService
	primaryProvider  domain.AIProvider
	fallbackProvider domain.AIProvider
	metrics          map[domain.AIProvider]*domain.AIMetrics
	analytics        *analytics.BigQueryService
	mu               sync.RWMutex
	experimentGroup  string
	semaphore        chan struct{}
}
// Provider defines the interface that all AI providers must implement.
// This allows the composite service to work with different AI backends.
type Provider interface {
	// EnrichMetadata enriches track metadata using AI
	EnrichMetadata(ctx context.Context, track *domain.Track) (*domain.Metadata, error)
	// ValidateMetadata validates track metadata using AI
	ValidateMetadata(ctx context.Context, track *domain.Track) (*domain.ValidationResult, error)
}
// NewCompositeAIService creates a new composite AI service
func NewCompositeAIService(config *Config, analytics *analytics.BigQueryService) (*CompositeAIService, error) {
	if config == nil {
		return nil, fmt.Errorf("AI service config is required")
	}
	if analytics == nil {
		return nil, fmt.Errorf("analytics service is required")
	}
	// Create Qwen2-Audio service
	qwen2Config := &domain.Qwen2Config{
		APIKey:                config.Qwen2Config.APIKey,
		Endpoint:              config.Qwen2Config.Endpoint,
		TimeoutSeconds:        config.TimeoutSeconds,
		MinConfidence:         config.MinConfidence,
		MaxConcurrentRequests: config.MaxConcurrentRequests,
		RetryAttempts:         config.RetryAttempts,
		RetryBackoffSeconds:   config.RetryBackoffSeconds,
	}
	qwen2Service, err := NewQwen2Service(qwen2Config)
	if err != nil {
		return nil, fmt.Errorf("failed to create Qwen2 service: %w", err)
	}
	// Create OpenAI service
	openAIConfig := &domain.OpenAIConfig{
		APIKey:                config.OpenAIConfig.APIKey,
		Endpoint:              config.OpenAIConfig.Endpoint,
		TimeoutSeconds:        config.TimeoutSeconds,
		MinConfidence:         config.MinConfidence,
		MaxConcurrentRequests: config.MaxConcurrentRequests,
		RetryAttempts:         config.RetryAttempts,
		RetryBackoffSeconds:   config.RetryBackoffSeconds,
	}
	openAIService, err := NewOpenAIService(openAIConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create OpenAI service: %w", err)
	}
	return &CompositeAIService{
		config:           config,
		qwen2Service:     qwen2Service,
		openAIService:    openAIService,
		primaryProvider:  domain.AIProviderQwen2,
		fallbackProvider: domain.AIProviderOpenAI,
		metrics:          make(map[domain.AIProvider]*domain.AIMetrics),
		analytics:        analytics,
		experimentGroup:  "control",
		semaphore:        make(chan struct{}, config.MaxConcurrentRequests),
	}, nil
}
// EnrichMetadata enriches a track with AI-generated metadata
func (s *CompositeAIService) EnrichMetadata(ctx context.Context, track *domain.Track) error {
	start := time.Now()
	// Determine if this request should be part of the experiment (10% of traffic)
	isExperiment := rand.Float64() < 0.1
	var service domain.AIService
	var provider domain.AIProvider
	var experimentGroup string
	if isExperiment {
		service = s.openAIService
		provider = domain.AIProviderOpenAI
		experimentGroup = "experiment"
	} else {
		service = s.qwen2Service
		provider = domain.AIProviderQwen2
		experimentGroup = "control"
	}
	// Try selected service
	err := service.EnrichMetadata(ctx, track)
	duration := time.Since(start)
	// Record experiment data
	record := &analytics.AIExperimentRecord{
		Timestamp:       time.Now(),
		TrackID:         track.ID,
		ModelProvider:   string(provider),
		ModelVersion:    track.ModelVersion(),
		ProcessingTime:  duration.Seconds(),
		Confidence:      track.AIConfidence(),
		Success:         err == nil,
		ErrorMessage:    "",
		ExperimentGroup: experimentGroup,
	}
	if err != nil {
		record.ErrorMessage = err.Error()
		s.recordFailure(provider, err)
		metrics.AIRequestDuration.WithLabelValues(string(provider) + "_failed").Observe(duration.Seconds())
		// Try fallback if confidence is low or there was an error
		if s.config.EnableFallback {
			fallbackStart := time.Now()
			err = s.getFallbackService().EnrichMetadata(ctx, track)
			fallbackDuration := time.Since(fallbackStart)
			// Record fallback attempt
			fallbackRecord := &analytics.AIExperimentRecord{
				Timestamp:       time.Now(),
				TrackID:         track.ID,
				ModelProvider:   string(s.fallbackProvider),
				ModelVersion:    track.ModelVersion(),
				ProcessingTime:  fallbackDuration.Seconds(),
				Confidence:      track.AIConfidence(),
				Success:         err == nil,
				ErrorMessage:    "",
				ExperimentGroup: experimentGroup + "_fallback",
			}
			if err != nil {
				fallbackRecord.ErrorMessage = err.Error()
				s.recordFailure(s.fallbackProvider, err)
				metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_failed").Observe(fallbackDuration.Seconds())
				return fmt.Errorf("both primary and fallback services failed: %w", err)
			}
			s.recordSuccess(s.fallbackProvider, fallbackDuration)
			metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_success").Observe(fallbackDuration.Seconds())
			// Record fallback metrics
			if err := s.analytics.RecordAIExperiment(ctx, fallbackRecord); err != nil {
				logrus.WithError(err).Error("Failed to record fallback experiment data")
			}
		} else {
			return fmt.Errorf("service failed and fallback is disabled: %w", err)
		}
	} else {
		// Check confidence threshold
		if track.AIConfidence() < s.config.MinConfidence {
			// Try fallback for low confidence
			fallbackStart := time.Now()
			fallbackErr := s.getFallbackService().EnrichMetadata(ctx, track)
			fallbackDuration := time.Since(fallbackStart)
			fallbackRecord := &analytics.AIExperimentRecord{
				Timestamp:       time.Now(),
				TrackID:         track.ID,
				ModelProvider:   string(s.fallbackProvider),
				ModelVersion:    track.ModelVersion(),
				ProcessingTime:  fallbackDuration.Seconds(),
				Confidence:      track.AIConfidence(),
				Success:         fallbackErr == nil,
				ErrorMessage:    "",
				ExperimentGroup: experimentGroup + "_low_confidence",
			}
			if fallbackErr == nil && track.AIConfidence() > s.config.MinConfidence {
				s.recordSuccess(s.fallbackProvider, fallbackDuration)
				metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_success").Observe(fallbackDuration.Seconds())
			} else {
				// Keep original results if fallback didn't improve confidence
				fallbackRecord.ErrorMessage = "Fallback did not improve confidence"
			}
			// Record fallback metrics
			if err := s.analytics.RecordAIExperiment(ctx, fallbackRecord); err != nil {
				logrus.WithError(err).Error("Failed to record fallback experiment data")
			}
		}
		s.recordSuccess(provider, duration)
		metrics.AIRequestDuration.WithLabelValues(string(provider) + "_success").Observe(duration.Seconds())
	}
	// Record experiment metrics
	if err := s.analytics.RecordAIExperiment(ctx, record); err != nil {
		logrus.WithError(err).Error("Failed to record experiment data")
	}
	return nil
}
// ValidateMetadata validates track metadata using AI
func (s *CompositeAIService) ValidateMetadata(ctx context.Context, track *domain.Track) (float64, error) {
	start := time.Now()
	// Try primary service first
	confidence, err := s.getPrimaryService().ValidateMetadata(ctx, track)
	if err == nil {
		s.recordSuccess(s.primaryProvider, time.Since(start))
		return confidence, nil
	}
	// Record failure and try fallback if enabled
	s.recordFailure(s.primaryProvider, err)
	metrics.AIRequestDuration.WithLabelValues(string(s.primaryProvider) + "_failed").Observe(time.Since(start).Seconds())
	if !s.config.EnableFallback {
		return 0, fmt.Errorf("primary service failed and fallback is disabled: %w", err)
	}
	// Try fallback service
	fallbackStart := time.Now()
	confidence, err = s.getFallbackService().ValidateMetadata(ctx, track)
	if err != nil {
		s.recordFailure(s.fallbackProvider, err)
		metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_failed").Observe(time.Since(fallbackStart).Seconds())
		return 0, fmt.Errorf("both primary and fallback services failed: %w", err)
	}
	s.recordSuccess(s.fallbackProvider, time.Since(fallbackStart))
	metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_success").Observe(time.Since(fallbackStart).Seconds())
	return confidence, nil
}
// BatchProcess processes multiple tracks in batch
func (s *CompositeAIService) BatchProcess(ctx context.Context, tracks []*domain.Track) error {
	start := time.Now()
	// Try primary service first
	err := s.getPrimaryService().BatchProcess(ctx, tracks)
	if err == nil {
		s.recordSuccess(s.primaryProvider, time.Since(start))
		return nil
	}
	// Record failure and try fallback if enabled
	s.recordFailure(s.primaryProvider, err)
	metrics.AIRequestDuration.WithLabelValues(string(s.primaryProvider) + "_batch_failed").Observe(time.Since(start).Seconds())
	if !s.config.EnableFallback {
		return fmt.Errorf("primary service failed and fallback is disabled: %w", err)
	}
	// Try fallback service
	fallbackStart := time.Now()
	err = s.getFallbackService().BatchProcess(ctx, tracks)
	if err != nil {
		s.recordFailure(s.fallbackProvider, err)
		metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_batch_failed").Observe(time.Since(fallbackStart).Seconds())
		return fmt.Errorf("both primary and fallback services failed: %w", err)
	}
	s.recordSuccess(s.fallbackProvider, time.Since(fallbackStart))
	metrics.AIRequestDuration.WithLabelValues(string(s.fallbackProvider) + "_batch_success").Observe(time.Since(fallbackStart).Seconds())
	return nil
}
// SetPrimaryProvider sets the primary AI provider
func (s *CompositeAIService) SetPrimaryProvider(provider domain.AIProvider) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.primaryProvider = provider
}
// SetFallbackProvider sets the fallback AI provider
func (s *CompositeAIService) SetFallbackProvider(provider domain.AIProvider) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.fallbackProvider = provider
}
// GetProviderMetrics returns metrics for each provider
func (s *CompositeAIService) GetProviderMetrics() map[domain.AIProvider]*domain.AIMetrics {
	s.mu.RLock()
	defer s.mu.RUnlock()
	// Create a copy of the metrics
	metrics := make(map[domain.AIProvider]*domain.AIMetrics)
	for provider, metric := range s.metrics {
		metricCopy := *metric
		metrics[provider] = &metricCopy
	}
	return metrics
}
// Helper methods
func (s *CompositeAIService) getPrimaryService() domain.AIService {
	s.mu.RLock()
	defer s.mu.RUnlock()
	if s.primaryProvider == domain.AIProviderQwen2 {
		return s.qwen2Service
	}
	return s.openAIService
}
func (s *CompositeAIService) getFallbackService() domain.AIService {
	s.mu.RLock()
	defer s.mu.RUnlock()
	if s.fallbackProvider == domain.AIProviderQwen2 {
		return s.qwen2Service
	}
	return s.openAIService
}
func (s *CompositeAIService) recordSuccess(provider domain.AIProvider, duration time.Duration) {
	s.mu.Lock()
	defer s.mu.Unlock()
	if _, exists := s.metrics[provider]; !exists {
		s.metrics[provider] = &domain.AIMetrics{}
	}
	s.metrics[provider].RequestCount++
	s.metrics[provider].SuccessCount++
	s.metrics[provider].LastSuccess = time.Now()
	s.metrics[provider].AverageLatency = (s.metrics[provider].AverageLatency*time.Duration(s.metrics[provider].RequestCount-1) + duration) / time.Duration(s.metrics[provider].RequestCount)
}
func (s *CompositeAIService) recordFailure(provider domain.AIProvider, err error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	if _, exists := s.metrics[provider]; !exists {
		s.metrics[provider] = &domain.AIMetrics{}
	}
	s.metrics[provider].RequestCount++
	s.metrics[provider].FailureCount++
	s.metrics[provider].LastError = err
}
// SetExperimentGroup sets the experiment group for this service instance.
// This is used for A/B testing different AI providers or configurations.
//
// Parameters:
//   - group: Experiment group ("control" or "experiment")
func (s *CompositeAIService) SetExperimentGroup(group string) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.experimentGroup = group
}
// getExperimentGroup safely retrieves the current experiment group.
func (s *CompositeAIService) getExperimentGroup() string {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.experimentGroup
}
// enrichWithRetry attempts to enrich metadata with retries and fallback.
func (s *CompositeAIService) enrichWithRetry(ctx context.Context, track *domain.Track) (*domain.Metadata, error) {
	// Implementation details...
	return nil, nil // TODO: Implement
}
// validateWithRetry attempts to validate metadata with retries and fallback.
func (s *CompositeAIService) validateWithRetry(ctx context.Context, track *domain.Track) (*domain.ValidationResult, error) {
	// Implementation details...
	return nil, nil // TODO: Implement
}
</file>

<file path="internal/repository/ai/openai_client.go">
package ai
import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
	"metadatatool/internal/pkg/domain"
)
// OpenAIClient handles communication with the OpenAI API
type OpenAIClient struct {
	config     *domain.OpenAIConfig
	httpClient *http.Client
}
// OpenAIResponse represents the response from OpenAI's audio analysis
type OpenAIResponse struct {
	Energy       float64 `json:"energy"`
	Danceability float64 `json:"danceability"`
	Confidence   float64 `json:"confidence"`
	ReviewReason string  `json:"review_reason,omitempty"`
}
// NewOpenAIClient creates a new OpenAI client
func NewOpenAIClient(config *domain.OpenAIConfig) (*OpenAIClient, error) {
	if config == nil {
		return nil, fmt.Errorf("OpenAI config is required")
	}
	if config.APIKey == "" {
		return nil, fmt.Errorf("OpenAI API key is required")
	}
	if config.Endpoint == "" {
		return nil, fmt.Errorf("OpenAI endpoint is required")
	}
	return &OpenAIClient{
		config: config,
		httpClient: &http.Client{
			Timeout: time.Duration(config.TimeoutSeconds) * time.Second,
		},
	}, nil
}
// AnalyzeAudio sends audio data to OpenAI for analysis
func (c *OpenAIClient) AnalyzeAudio(ctx context.Context, audioData []byte, format string) (*OpenAIResponse, error) {
	url := fmt.Sprintf("%s/v1/audio/analyze", c.config.Endpoint)
	// Prepare request body
	body := struct {
		AudioData []byte `json:"audio_data"`
		Format    string `json:"format"`
	}{
		AudioData: audioData,
		Format:    format,
	}
	jsonBody, err := json.Marshal(body)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request body: %w", err)
	}
	// Create request
	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonBody))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.config.APIKey))
	// Send request
	resp, err := c.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to send request: %w", err)
	}
	defer resp.Body.Close()
	// Read response body
	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}
	// Check response status
	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("OpenAI API error: %s (status code: %d)", string(respBody), resp.StatusCode)
	}
	// Parse response
	var result OpenAIResponse
	if err := json.Unmarshal(respBody, &result); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}
	return &result, nil
}
// ValidateMetadata validates track metadata using OpenAI
func (c *OpenAIClient) ValidateMetadata(ctx context.Context, track *domain.Track) (float64, error) {
	url := fmt.Sprintf("%s/v1/metadata/validate", c.config.Endpoint)
	// Convert CompleteTrackMetadata to Metadata
	metadata := &domain.Metadata{
		ISRC:         track.ISRC(),
		ISWC:         track.ISWC(),
		BPM:          track.BPM(),
		Key:          track.Key(),
		Mood:         track.Mood(),
		Labels:       []string{},
		AITags:       track.AITags(),
		Confidence:   track.AIConfidence(),
		ModelVersion: track.ModelVersion(),
		CustomFields: track.Metadata.Additional.CustomFields,
	}
	// Convert custom tags to labels
	for tag := range track.Metadata.Additional.CustomTags {
		metadata.Labels = append(metadata.Labels, tag)
	}
	// Prepare request body
	body := struct {
		Metadata *domain.Metadata `json:"metadata"`
	}{
		Metadata: metadata,
	}
	jsonBody, err := json.Marshal(body)
	if err != nil {
		return 0, fmt.Errorf("failed to marshal request body: %w", err)
	}
	// Create request
	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonBody))
	if err != nil {
		return 0, fmt.Errorf("failed to create request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.config.APIKey))
	// Send request
	resp, err := c.httpClient.Do(req)
	if err != nil {
		return 0, fmt.Errorf("failed to send request: %w", err)
	}
	defer resp.Body.Close()
	// Read response
	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return 0, fmt.Errorf("failed to read response: %w", err)
	}
	// Parse response
	var result struct {
		Confidence float64 `json:"confidence"`
	}
	if err := json.Unmarshal(respBody, &result); err != nil {
		return 0, fmt.Errorf("failed to parse response: %w", err)
	}
	return result.Confidence, nil
}
</file>

<file path="internal/repository/ai/openai_service.go">
package ai
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"strings"
	"sync"
	"time"
	"github.com/sashabaranov/go-openai"
	"golang.org/x/time/rate"
)
// OpenAIError represents a specific error from the OpenAI service
type OpenAIError struct {
	Code        string
	Message     string
	Recoverable bool
}
func (e *OpenAIError) Error() string {
	return fmt.Sprintf("OpenAI error: %s - %s", e.Code, e.Message)
}
// OpenAIService implements the AIService interface using OpenAI
type OpenAIService struct {
	config  *domain.OpenAIConfig
	client  *openai.Client
	metrics *domain.AIMetrics
	mu      sync.RWMutex // Protects metrics
	limiter *rate.Limiter
	models  map[string]ModelInfo
}
// ModelInfo tracks information about model versions
type ModelInfo struct {
	Version    string
	LastUsed   time.Time
	TotalCalls int64
	AvgLatency time.Duration
	ErrorRate  float64
	Deprecated bool
	ReplacedBy string
}
// NewOpenAIService creates a new OpenAI service instance
func NewOpenAIService(config *domain.OpenAIConfig) (*OpenAIService, error) {
	if config == nil {
		return nil, fmt.Errorf("openai config is required")
	}
	if config.APIKey == "" {
		return nil, fmt.Errorf("openai API key is required")
	}
	// Create rate limiter based on config
	// Default to 10 requests per second if not specified
	rps := float64(10)
	if config.RequestsPerSecond > 0 {
		rps = float64(config.RequestsPerSecond)
	}
	limiter := rate.NewLimiter(rate.Limit(rps), int(rps))
	// Initialize model tracking
	models := map[string]ModelInfo{
		"gpt-4": {
			Version:    "v1",
			LastUsed:   time.Now(),
			Deprecated: false,
		},
		"gpt-3.5-turbo": {
			Version:    "v1",
			LastUsed:   time.Now(),
			Deprecated: false,
		},
	}
	// Create OpenAI client
	client := openai.NewClient(config.APIKey)
	return &OpenAIService{
		config:  config,
		client:  client,
		limiter: limiter,
		models:  models,
		metrics: &domain.AIMetrics{
			RequestCount:   0,
			SuccessCount:   0,
			FailureCount:   0,
			AverageLatency: 0,
		},
	}, nil
}
// waitForRateLimit waits for rate limit with context
func (s *OpenAIService) waitForRateLimit(ctx context.Context) error {
	if err := s.limiter.Wait(ctx); err != nil {
		if err == context.DeadlineExceeded {
			return &OpenAIError{
				Code:        "rate_limit_timeout",
				Message:     "rate limit wait timeout",
				Recoverable: true,
			}
		}
		return fmt.Errorf("rate limit error: %w", err)
	}
	return nil
}
// updateModelMetrics updates the metrics for a specific model
func (s *OpenAIService) updateModelMetrics(model string, latency time.Duration, err error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	info := s.models[model]
	info.LastUsed = time.Now()
	info.TotalCalls++
	// Update average latency
	if info.AvgLatency == 0 {
		info.AvgLatency = latency
	} else {
		info.AvgLatency = (info.AvgLatency + latency) / 2
	}
	// Update error rate
	if err != nil {
		info.ErrorRate = (info.ErrorRate*float64(info.TotalCalls-1) + 1) / float64(info.TotalCalls)
	} else {
		info.ErrorRate = (info.ErrorRate * float64(info.TotalCalls-1)) / float64(info.TotalCalls)
	}
	s.models[model] = info
}
// selectModel chooses the best model based on requirements and metrics
func (s *OpenAIService) selectModel(ctx context.Context) string {
	s.mu.RLock()
	defer s.mu.RUnlock()
	// Default to GPT-4 for best quality
	if !s.models["gpt-4"].Deprecated {
		return "gpt-4"
	}
	// Fallback to GPT-3.5 if GPT-4 is deprecated or has high error rate
	if !s.models["gpt-3.5-turbo"].Deprecated {
		return "gpt-3.5-turbo"
	}
	// If both are deprecated, use the one with lower error rate
	if s.models["gpt-4"].ErrorRate < s.models["gpt-3.5-turbo"].ErrorRate {
		return "gpt-4"
	}
	return "gpt-3.5-turbo"
}
// EnrichMetadata processes an audio track and enriches it with AI-generated metadata
func (s *OpenAIService) EnrichMetadata(ctx context.Context, track *domain.Track) error {
	if track == nil {
		return fmt.Errorf("track is required")
	}
	startTime := time.Now()
	var processingErr error
	// Wait for rate limit
	if err := s.waitForRateLimit(ctx); err != nil {
		return fmt.Errorf("rate limit error: %w", err)
	}
	// Select model
	model := s.selectModel(ctx)
	// Define retry strategy
	for attempt := 0; attempt <= s.config.RetryAttempts; attempt++ {
		if attempt > 0 {
			// Exponential backoff
			backoffDuration := time.Duration(attempt) * time.Second * time.Duration(s.config.RetryBackoffSeconds)
			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-time.After(backoffDuration):
			}
			// Wait for rate limit on retries
			if err := s.waitForRateLimit(ctx); err != nil {
				return fmt.Errorf("rate limit error on retry: %w", err)
			}
		}
		// Prepare system message
		systemMsg := `You are an expert music metadata analyst. Analyze the provided track information and generate accurate metadata.
Your task is to:
1. Determine the most accurate genre based on the title, artist, and existing metadata
2. Calculate or verify the BPM (beats per minute) if provided
3. Identify the musical key and ensure it follows standard notation (e.g., "C Major", "F# Minor")
4. Analyze the mood and energy level of the track
5. Generate relevant tags that describe the track's characteristics
Return your analysis in the following JSON format:
{
    "genre": "string",
    "bpm": float,
    "key": "string",
    "mood": "string",
    "energy": float,
    "tempo": float,
    "tags": ["string"],
    "confidence_scores": {
        "genre": float,
        "bpm": float,
        "key": float,
        "mood": float,
        "overall": float
    },
    "analysis": "string"
}
Guidelines:
- Genre should be specific but widely recognized (e.g., "Progressive House" rather than just "Electronic")
- BPM should be between 20 and 400
- Key must follow standard notation (e.g., "C Major", "F# Minor")
- Mood should reflect the emotional quality (e.g., "Energetic", "Melancholic")
- Energy should be between 0 and 1
- Tempo should match the BPM but can be adjusted for human perception
- Tags should be relevant keywords (max 5) that aid in classification
- All confidence scores must be between 0 and 1
- Analysis should explain your reasoning and any notable characteristics`
		// Prepare user message with track info
		userMsg := fmt.Sprintf(`Analyze this track:
Title: %s
Artist: %s
Album: %s
Duration: %d seconds
Current Genre: %s
Current BPM: %.1f
Current Key: %s
Current Mood: %s
Additional Context:
- Release Year: %d
- ISRC: %s
- Label: %s
- Territory: %s`,
			track.Title(),
			track.Artist(),
			track.Album(),
			int(track.Duration()),
			track.Genre(),
			track.BPM(),
			track.Key(),
			track.Mood(),
			track.Year(),
			track.ISRC(),
			track.Label(),
			track.Territory(),
		)
		// Create chat completion request
		req := openai.ChatCompletionRequest{
			Model: model,
			Messages: []openai.ChatCompletionMessage{
				{
					Role:    "system",
					Content: systemMsg,
				},
				{
					Role:    "user",
					Content: userMsg,
				},
			},
			MaxTokens:   1000,
			Temperature: 0.3,
		}
		// Call OpenAI API
		resp, err := s.client.CreateChatCompletion(ctx, req)
		requestDuration := time.Since(startTime)
		s.updateModelMetrics(model, requestDuration, err)
		if err != nil {
			var openAIErr *OpenAIError
			switch {
			case isRateLimitError(err):
				openAIErr = &OpenAIError{
					Code:        "rate_limit_exceeded",
					Message:     "OpenAI rate limit exceeded",
					Recoverable: true,
				}
			case isConnectionError(err):
				openAIErr = &OpenAIError{
					Code:        "connection_error",
					Message:     "Failed to connect to OpenAI",
					Recoverable: true,
				}
			case isTimeoutError(err):
				openAIErr = &OpenAIError{
					Code:        "timeout",
					Message:     "Request to OpenAI timed out",
					Recoverable: true,
				}
			default:
				openAIErr = &OpenAIError{
					Code:        "unknown",
					Message:     err.Error(),
					Recoverable: false,
				}
			}
			processingErr = fmt.Errorf("attempt %d: %w", attempt+1, openAIErr)
			continue
		}
		if len(resp.Choices) == 0 {
			processingErr = fmt.Errorf("attempt %d: no completion choices returned", attempt+1)
			continue
		}
		// Parse response and update track metadata
		completion := resp.Choices[0].Message.Content
		metadata, confidence, err := parseOpenAIResponse(completion)
		if err != nil {
			processingErr = fmt.Errorf("attempt %d: failed to parse response: %w", attempt+1, err)
			continue
		}
		// Check confidence threshold
		if confidence < s.config.MinConfidence {
			track.Metadata.AI = &domain.TrackAIMetadata{
				Tags:         metadata.AI.Tags,
				Confidence:   confidence,
				Model:        model,
				Version:      "v1",
				ProcessedAt:  time.Now(),
				NeedsReview:  true,
				ReviewReason: fmt.Sprintf("Low confidence score: %.2f", confidence),
			}
		} else {
			track.Metadata.AI = &domain.TrackAIMetadata{
				Tags:        metadata.AI.Tags,
				Confidence:  confidence,
				Model:       model,
				Version:     "v1",
				ProcessedAt: time.Now(),
				NeedsReview: false,
			}
		}
		// Update track fields
		track.SetGenre(metadata.Musical.Genre)
		track.SetBPM(metadata.Musical.BPM)
		track.SetKey(metadata.Musical.Key)
		track.SetMood(metadata.Musical.Mood)
		// Update metrics
		duration := time.Since(startTime)
		s.recordSuccess(duration)
		metrics.AIConfidenceScore.WithLabelValues(string(domain.AIProviderOpenAI)).Observe(confidence)
		return nil // Successfully processed
	}
	// If we get here, we've exhausted all retry attempts
	s.recordFailure(processingErr)
	return fmt.Errorf("failed to enrich metadata after %d attempts: %w", s.config.RetryAttempts, processingErr)
}
// ValidateMetadata validates track metadata using OpenAI
func (s *OpenAIService) ValidateMetadata(ctx context.Context, track *domain.Track) (float64, error) {
	if track == nil {
		return 0, fmt.Errorf("track is required")
	}
	startTime := time.Now()
	var processingErr error
	// Define retry strategy
	for attempt := 0; attempt <= s.config.RetryAttempts; attempt++ {
		if attempt > 0 {
			// Exponential backoff
			backoffDuration := time.Duration(attempt) * time.Second * time.Duration(s.config.RetryBackoffSeconds)
			select {
			case <-ctx.Done():
				return 0, ctx.Err()
			case <-time.After(backoffDuration):
			}
		}
		// Prepare system message
		systemMsg := `You are an expert music metadata validator. Analyze the provided track metadata and validate its accuracy and completeness.
Return your analysis in the following JSON format:
{
    "confidence_scores": {
        "title_artist_match": float,
        "genre_accuracy": float,
        "musical_consistency": float,
        "metadata_completeness": float,
        "overall": float
    },
    "issues": [
        {
            "field": "string",
            "severity": "low|medium|high",
            "description": "string"
        }
    ],
    "suggestions": [
        {
            "field": "string",
            "current_value": "string",
            "suggested_value": "string",
            "reason": "string"
        }
    ],
    "analysis": "string"
}
All confidence scores should be between 0 and 1. The analysis field should contain your overall assessment and reasoning.`
		// Prepare user message with track metadata
		userMsg := fmt.Sprintf(`Validate this track metadata:
Title: %s
Artist: %s
Album: %s
Genre: %s
BPM: %.1f
Key: %s
Mood: %s
ISRC: %s
ISWC: %s`,
			track.Title(),
			track.Artist(),
			track.Album(),
			track.Genre(),
			track.BPM(),
			track.Key(),
			track.Mood(),
			track.ISRC(),
			track.ISWC(),
		)
		// Create chat completion request
		req := openai.ChatCompletionRequest{
			Model: openai.GPT4,
			Messages: []openai.ChatCompletionMessage{
				{
					Role:    "system",
					Content: systemMsg,
				},
				{
					Role:    "user",
					Content: userMsg,
				},
			},
			MaxTokens:   1000,
			Temperature: 0.3, // Lower temperature for more consistent results
		}
		// Call OpenAI API
		resp, err := s.client.CreateChatCompletion(ctx, req)
		if err != nil {
			processingErr = fmt.Errorf("attempt %d: failed to get completion: %w", attempt+1, err)
			continue
		}
		if len(resp.Choices) == 0 {
			processingErr = fmt.Errorf("attempt %d: no completion choices returned", attempt+1)
			continue
		}
		// Parse response to get confidence score
		completion := resp.Choices[0].Message.Content
		parsedResp, confidence, err := parseOpenAIValidationResponse(completion)
		if err != nil {
			processingErr = fmt.Errorf("attempt %d: failed to parse response: %w", attempt+1, err)
			continue
		}
		// Update track metadata with validation results
		track.Metadata.AI = &domain.TrackAIMetadata{
			Confidence:            confidence,
			Model:                 "gpt-4",
			Version:               "v1",
			ProcessedAt:           time.Now(),
			NeedsReview:           confidence < s.config.MinConfidence,
			ReviewReason:          fmt.Sprintf("Low confidence score: %.2f", confidence),
			Analysis:              parsedResp.Analysis,
			ValidationIssues:      parsedResp.Issues,
			ValidationSuggestions: parsedResp.Suggestions,
		}
		// Update metrics
		duration := time.Since(startTime)
		s.recordSuccess(duration)
		metrics.AIConfidenceScore.WithLabelValues(string(domain.AIProviderOpenAI)).Observe(confidence)
		return confidence, nil
	}
	// If we get here, we've exhausted all retry attempts
	s.recordFailure(processingErr)
	return 0, fmt.Errorf("failed to validate metadata after %d attempts: %w", s.config.RetryAttempts, processingErr)
}
// BatchProcess processes multiple tracks in parallel
func (s *OpenAIService) BatchProcess(ctx context.Context, tracks []*domain.Track) error {
	if len(tracks) == 0 {
		return nil
	}
	startTime := time.Now()
	// Create a semaphore to limit concurrent requests
	sem := make(chan struct{}, s.config.MaxConcurrentRequests)
	errChan := make(chan error, len(tracks))
	var wg sync.WaitGroup
	// Process tracks in parallel
	for i := range tracks {
		wg.Add(1)
		go func(track *domain.Track) {
			defer wg.Done()
			// Acquire semaphore
			sem <- struct{}{}
			defer func() { <-sem }()
			// Process individual track
			if err := s.EnrichMetadata(ctx, track); err != nil {
				errChan <- fmt.Errorf("failed to process track %s: %w", track.ID, err)
				return
			}
		}(tracks[i])
	}
	// Wait for all goroutines to finish
	wg.Wait()
	close(errChan)
	// Collect any errors
	var errors []error
	for err := range errChan {
		errors = append(errors, err)
	}
	// Update batch metrics
	duration := time.Since(startTime)
	metrics.AIBatchSize.WithLabelValues(string(domain.AIProviderOpenAI)).Observe(float64(len(tracks)))
	metrics.AIRequestDuration.WithLabelValues(string(domain.AIProviderOpenAI)).Observe(duration.Seconds())
	// Return combined errors if any
	if len(errors) > 0 {
		return fmt.Errorf("batch processing completed with %d errors: %v", len(errors), errors)
	}
	return nil
}
// recordSuccess updates metrics for successful requests
func (s *OpenAIService) recordSuccess(duration time.Duration) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.metrics.RequestCount++
	s.metrics.SuccessCount++
	s.metrics.LastSuccess = time.Now()
	// Update average latency
	if s.metrics.AverageLatency == 0 {
		s.metrics.AverageLatency = duration
	} else {
		s.metrics.AverageLatency = (s.metrics.AverageLatency + duration) / 2
	}
	// Update Prometheus metrics
	metrics.AIRequestTotal.WithLabelValues(string(domain.AIProviderOpenAI), "success").Inc()
	metrics.AIRequestDuration.WithLabelValues(string(domain.AIProviderOpenAI)).Observe(duration.Seconds())
}
// recordFailure updates metrics for failed requests
func (s *OpenAIService) recordFailure(err error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.metrics.RequestCount++
	s.metrics.FailureCount++
	s.metrics.LastError = err
	// Update Prometheus metrics
	metrics.AIRequestTotal.WithLabelValues(string(domain.AIProviderOpenAI), "failure").Inc()
	metrics.AIErrorTotal.WithLabelValues(string(domain.AIProviderOpenAI), err.Error()).Inc()
}
// Helper function to parse OpenAI's response for metadata enrichment
func parseOpenAIResponse(completion string) (*domain.CompleteTrackMetadata, float64, error) {
	type Response struct {
		Genre            string   `json:"genre"`
		BPM              float64  `json:"bpm"`
		Key              string   `json:"key"`
		Mood             string   `json:"mood"`
		Energy           float64  `json:"energy"`
		Tempo            float64  `json:"tempo"`
		Tags             []string `json:"tags"`
		ConfidenceScores struct {
			Genre   float64 `json:"genre"`
			BPM     float64 `json:"bpm"`
			Key     float64 `json:"key"`
			Mood    float64 `json:"mood"`
			Overall float64 `json:"overall"`
		} `json:"confidence_scores"`
		Analysis string `json:"analysis"`
	}
	var resp Response
	if err := json.Unmarshal([]byte(completion), &resp); err != nil {
		return nil, 0, fmt.Errorf("failed to parse GPT response: %w", err)
	}
	// Validate all fields
	if resp.Genre == "" {
		return nil, 0, fmt.Errorf("genre is required")
	}
	if resp.BPM < 20 || resp.BPM > 400 {
		return nil, 0, fmt.Errorf("invalid BPM: %f (must be between 20 and 400)", resp.BPM)
	}
	if resp.Key == "" {
		return nil, 0, fmt.Errorf("key is required")
	}
	if resp.Mood == "" {
		return nil, 0, fmt.Errorf("mood is required")
	}
	if resp.Energy < 0 || resp.Energy > 1 {
		return nil, 0, fmt.Errorf("invalid energy value: %f (must be between 0 and 1)", resp.Energy)
	}
	if resp.Tempo < 20 || resp.Tempo > 400 {
		return nil, 0, fmt.Errorf("invalid tempo: %f (must be between 20 and 400)", resp.Tempo)
	}
	if len(resp.Tags) == 0 {
		return nil, 0, fmt.Errorf("at least one tag is required")
	}
	if len(resp.Tags) > 5 {
		resp.Tags = resp.Tags[:5] // Limit to 5 tags
	}
	// Validate confidence scores
	validateScore := func(name string, score float64) error {
		if score < 0 || score > 1 {
			return fmt.Errorf("invalid %s confidence score: %f (must be between 0 and 1)", name, score)
		}
		return nil
	}
	if err := validateScore("genre", resp.ConfidenceScores.Genre); err != nil {
		return nil, 0, err
	}
	if err := validateScore("BPM", resp.ConfidenceScores.BPM); err != nil {
		return nil, 0, err
	}
	if err := validateScore("key", resp.ConfidenceScores.Key); err != nil {
		return nil, 0, err
	}
	if err := validateScore("mood", resp.ConfidenceScores.Mood); err != nil {
		return nil, 0, err
	}
	if err := validateScore("overall", resp.ConfidenceScores.Overall); err != nil {
		return nil, 0, err
	}
	if resp.Analysis == "" {
		return nil, 0, fmt.Errorf("analysis is required")
	}
	// Create metadata
	metadata := &domain.CompleteTrackMetadata{
		Musical: domain.MusicalMetadata{
			Genre:  resp.Genre,
			BPM:    resp.BPM,
			Key:    resp.Key,
			Mood:   resp.Mood,
			Energy: resp.Energy,
			Tempo:  resp.Tempo,
		},
		AI: &domain.TrackAIMetadata{
			Tags:        resp.Tags,
			Confidence:  resp.ConfidenceScores.Overall,
			Model:       "gpt-4",
			Version:     "v1",
			ProcessedAt: time.Now(),
			NeedsReview: resp.ConfidenceScores.Overall < 0.7,
			Analysis:    resp.Analysis,
		},
	}
	return metadata, resp.ConfidenceScores.Overall, nil
}
// Helper function to parse OpenAI's response for metadata validation
func parseOpenAIValidationResponse(completion string) (*domain.ValidationResponse, float64, error) {
	var resp domain.ValidationResponse
	if err := json.Unmarshal([]byte(completion), &resp); err != nil {
		return nil, 0, fmt.Errorf("failed to parse GPT validation response: %w", err)
	}
	// Validate confidence scores
	validateScore := func(name string, score float64) error {
		if score < 0 || score > 1 {
			return fmt.Errorf("invalid %s confidence score: %f (must be between 0 and 1)", name, score)
		}
		return nil
	}
	if err := validateScore("title_artist_match", resp.ConfidenceScores.TitleArtistMatch); err != nil {
		return nil, 0, err
	}
	if err := validateScore("genre_accuracy", resp.ConfidenceScores.GenreAccuracy); err != nil {
		return nil, 0, err
	}
	if err := validateScore("musical_consistency", resp.ConfidenceScores.MusicalConsistency); err != nil {
		return nil, 0, err
	}
	if err := validateScore("metadata_completeness", resp.ConfidenceScores.MetadataCompleteness); err != nil {
		return nil, 0, err
	}
	if err := validateScore("overall", resp.ConfidenceScores.Overall); err != nil {
		return nil, 0, err
	}
	// Validate issues and suggestions
	if len(resp.Issues) == 0 && resp.ConfidenceScores.Overall < 0.8 {
		return nil, 0, fmt.Errorf("expected issues for low confidence score")
	}
	for i, issue := range resp.Issues {
		if issue.Field == "" {
			return nil, 0, fmt.Errorf("issue %d: field is required", i)
		}
		if issue.Severity == "" {
			return nil, 0, fmt.Errorf("issue %d: severity is required", i)
		}
		if issue.Description == "" {
			return nil, 0, fmt.Errorf("issue %d: description is required", i)
		}
		if issue.Severity != "low" && issue.Severity != "medium" && issue.Severity != "high" {
			return nil, 0, fmt.Errorf("issue %d: invalid severity: %s", i, issue.Severity)
		}
	}
	for i, suggestion := range resp.Suggestions {
		if suggestion.Field == "" {
			return nil, 0, fmt.Errorf("suggestion %d: field is required", i)
		}
		if suggestion.CurrentValue == "" {
			return nil, 0, fmt.Errorf("suggestion %d: current value is required", i)
		}
		if suggestion.SuggestedValue == "" {
			return nil, 0, fmt.Errorf("suggestion %d: suggested value is required", i)
		}
		if suggestion.Reason == "" {
			return nil, 0, fmt.Errorf("suggestion %d: reason is required", i)
		}
	}
	if resp.Analysis == "" {
		return nil, 0, fmt.Errorf("analysis is required")
	}
	return &resp, resp.ConfidenceScores.Overall, nil
}
// Error check functions
func isRateLimitError(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "rate_limit") ||
		strings.Contains(err.Error(), "rate limit") ||
		strings.Contains(err.Error(), "too many requests")
}
func isConnectionError(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "connection") ||
		strings.Contains(err.Error(), "network") ||
		strings.Contains(err.Error(), "i/o timeout")
}
func isTimeoutError(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "timeout") ||
		strings.Contains(err.Error(), "deadline exceeded")
}
</file>

<file path="internal/repository/ai/qwen2_client.go">
package ai
import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"metadatatool/internal/pkg/domain"
	"net/http"
	"time"
	"github.com/sony/gobreaker"
)
// Qwen2Error represents a specific error from the Qwen2 API
type Qwen2Error struct {
	Code        string `json:"code"`
	Message     string `json:"message"`
	Recoverable bool   `json:"recoverable"`
}
func (e *Qwen2Error) Error() string {
	return fmt.Sprintf("Qwen2 error: %s - %s", e.Code, e.Message)
}
// Qwen2Client handles communication with the Qwen2 API
type Qwen2Client struct {
	config     *domain.Qwen2Config
	httpClient *http.Client
	breaker    *gobreaker.CircuitBreaker
}
// Qwen2Response represents the response from the Qwen2 API
type Qwen2Response struct {
	Metadata struct {
		Genre      string   `json:"genre"`
		Mood       string   `json:"mood"`
		BPM        float64  `json:"bpm"`
		Key        string   `json:"key"`
		Tags       []string `json:"tags"`
		Confidence float64  `json:"confidence"`
	} `json:"metadata"`
	Error struct {
		Code    string `json:"code"`
		Message string `json:"message"`
	} `json:"error,omitempty"`
}
// BatchResponse represents a batch processing response
type BatchResponse struct {
	Results     []*Qwen2Response `json:"results"`
	FailedItems []struct {
		Index int         `json:"index"`
		Error *Qwen2Error `json:"error"`
	} `json:"failed_items"`
}
// NewQwen2Client creates a new Qwen2 API client
func NewQwen2Client(config *domain.Qwen2Config) (*Qwen2Client, error) {
	if config == nil {
		return nil, fmt.Errorf("config is required")
	}
	// Configure circuit breaker
	breakerSettings := gobreaker.Settings{
		Name:        "qwen2-breaker",
		MaxRequests: 100,
		Interval:    10 * time.Second,
		Timeout:     30 * time.Second,
		ReadyToTrip: func(counts gobreaker.Counts) bool {
			failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
			return counts.Requests >= 10 && failureRatio >= 0.6
		},
	}
	return &Qwen2Client{
		config: config,
		httpClient: &http.Client{
			Timeout: time.Duration(config.TimeoutSeconds) * time.Second,
		},
		breaker: gobreaker.NewCircuitBreaker(breakerSettings),
	}, nil
}
// AnalyzeAudio sends an audio file to Qwen2 for analysis
func (c *Qwen2Client) AnalyzeAudio(ctx context.Context, audioData io.Reader, format domain.AudioFormat) (*Qwen2Response, error) {
	// Prepare request body
	var buf bytes.Buffer
	if _, err := io.Copy(&buf, audioData); err != nil {
		return nil, fmt.Errorf("failed to read audio data: %w", err)
	}
	// Create request
	req, err := http.NewRequestWithContext(ctx, "POST", c.config.Endpoint+"/analyze", &buf)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}
	// Set headers
	req.Header.Set("Content-Type", "audio/"+string(format))
	req.Header.Set("Authorization", "Bearer "+c.config.APIKey)
	// Execute request through circuit breaker
	resp, err := c.breaker.Execute(func() (interface{}, error) {
		resp, err := c.httpClient.Do(req)
		if err != nil {
			return nil, fmt.Errorf("failed to send request: %w", err)
		}
		defer resp.Body.Close()
		body, err := io.ReadAll(resp.Body)
		if err != nil {
			return nil, fmt.Errorf("failed to read response: %w", err)
		}
		if resp.StatusCode != http.StatusOK {
			var qwenErr Qwen2Error
			if err := json.Unmarshal(body, &qwenErr); err != nil {
				return nil, fmt.Errorf("API request failed with status %d: %s", resp.StatusCode, string(body))
			}
			return nil, &qwenErr
		}
		var response Qwen2Response
		if err := json.Unmarshal(body, &response); err != nil {
			return nil, fmt.Errorf("failed to parse response: %w", err)
		}
		return &response, nil
	})
	if err != nil {
		return nil, err
	}
	return resp.(*Qwen2Response), nil
}
// ValidateMetadata validates track metadata using Qwen2
func (c *Qwen2Client) ValidateMetadata(ctx context.Context, track *domain.Track) (float64, error) {
	// Prepare request body
	body, err := json.Marshal(map[string]interface{}{
		"metadata": track.Metadata,
	})
	if err != nil {
		return 0, fmt.Errorf("failed to marshal metadata: %w", err)
	}
	// Create request
	req, err := http.NewRequestWithContext(ctx, "POST", c.config.Endpoint+"/validate", bytes.NewReader(body))
	if err != nil {
		return 0, fmt.Errorf("failed to create request: %w", err)
	}
	// Set headers
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+c.config.APIKey)
	// Execute request through circuit breaker
	resp, err := c.breaker.Execute(func() (interface{}, error) {
		resp, err := c.httpClient.Do(req)
		if err != nil {
			return 0.0, fmt.Errorf("failed to send request: %w", err)
		}
		defer resp.Body.Close()
		body, err := io.ReadAll(resp.Body)
		if err != nil {
			return 0.0, fmt.Errorf("failed to read response: %w", err)
		}
		if resp.StatusCode != http.StatusOK {
			var qwenErr Qwen2Error
			if err := json.Unmarshal(body, &qwenErr); err != nil {
				return 0.0, fmt.Errorf("API request failed with status %d: %s", resp.StatusCode, string(body))
			}
			return 0.0, &qwenErr
		}
		var response struct {
			Confidence float64 `json:"confidence"`
			Error      struct {
				Code    string `json:"code"`
				Message string `json:"message"`
			} `json:"error,omitempty"`
		}
		if err := json.Unmarshal(body, &response); err != nil {
			return 0.0, fmt.Errorf("failed to parse response: %w", err)
		}
		return response.Confidence, nil
	})
	if err != nil {
		return 0, err
	}
	return resp.(float64), nil
}
// BatchAnalyzeAudio processes multiple audio files in a single request
func (c *Qwen2Client) BatchAnalyzeAudio(ctx context.Context, requests []struct {
	AudioData []byte `json:"audio_data"`
	Format    string `json:"format"`
}) (*BatchResponse, error) {
	url := fmt.Sprintf("%s/v1/audio/batch/analyze", c.config.Endpoint)
	respBody, err := c.sendRequest(ctx, "POST", url, requests)
	if err != nil {
		return nil, fmt.Errorf("failed to batch analyze audio: %w", err)
	}
	var result BatchResponse
	if err := json.Unmarshal(respBody, &result); err != nil {
		return nil, fmt.Errorf("failed to parse batch response: %w", err)
	}
	return &result, nil
}
// BatchValidateMetadata validates multiple tracks' metadata in a single request
func (c *Qwen2Client) BatchValidateMetadata(ctx context.Context, tracks []*domain.Track) ([]float64, error) {
	url := fmt.Sprintf("%s/v1/metadata/validate/batch", c.config.Endpoint)
	// Convert tracks metadata
	metadataList := make([]*domain.Metadata, len(tracks))
	for i, track := range tracks {
		metadata := &domain.Metadata{
			ISRC:         track.ISRC(),
			ISWC:         track.ISWC(),
			BPM:          track.BPM(),
			Key:          track.Key(),
			Mood:         track.Mood(),
			Labels:       []string{},
			AITags:       track.AITags(),
			Confidence:   track.AIConfidence(),
			ModelVersion: track.ModelVersion(),
			CustomFields: track.Metadata.Additional.CustomFields,
		}
		// Convert custom tags to labels
		for tag := range track.Metadata.Additional.CustomTags {
			metadata.Labels = append(metadata.Labels, tag)
		}
		metadataList[i] = metadata
	}
	// Prepare request body
	body := struct {
		Tracks []*domain.Metadata `json:"tracks"`
	}{
		Tracks: metadataList,
	}
	respBody, err := c.sendRequest(ctx, "POST", url, body)
	if err != nil {
		return nil, fmt.Errorf("failed to validate metadata batch: %w", err)
	}
	var result struct {
		Confidences []float64 `json:"confidences"`
	}
	if err := json.Unmarshal(respBody, &result); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}
	return result.Confidences, nil
}
// sendRequest handles the common logic for sending requests to Qwen2-Audio
func (c *Qwen2Client) sendRequest(ctx context.Context, method, url string, body interface{}) ([]byte, error) {
	var jsonBody []byte
	var err error
	if body != nil {
		jsonBody, err = json.Marshal(body)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal request body: %w", err)
		}
	}
	// Execute request through circuit breaker
	resp, err := c.breaker.Execute(func() (interface{}, error) {
		req, err := http.NewRequestWithContext(ctx, method, url, bytes.NewReader(jsonBody))
		if err != nil {
			return nil, fmt.Errorf("failed to create request: %w", err)
		}
		req.Header.Set("Content-Type", "application/json")
		req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.config.APIKey))
		// Send request with retries
		var resp *http.Response
		var lastErr error
		for attempt := 0; attempt <= c.config.RetryAttempts; attempt++ {
			if attempt > 0 {
				backoffDuration := time.Duration(c.config.RetryBackoffSeconds) * time.Second * time.Duration(1<<uint(attempt-1))
				select {
				case <-ctx.Done():
					return nil, ctx.Err()
				case <-time.After(backoffDuration):
				}
			}
			resp, err = c.httpClient.Do(req)
			if err == nil {
				break
			}
			lastErr = err
		}
		if err != nil {
			return nil, fmt.Errorf("failed to send request after %d attempts: %w", c.config.RetryAttempts, lastErr)
		}
		defer resp.Body.Close()
		respBody, err := io.ReadAll(resp.Body)
		if err != nil {
			return nil, fmt.Errorf("failed to read response body: %w", err)
		}
		if resp.StatusCode != http.StatusOK {
			var qwenErr Qwen2Error
			if err := json.Unmarshal(respBody, &qwenErr); err != nil {
				return nil, fmt.Errorf("Qwen2 API error: %s (status code: %d)", string(respBody), resp.StatusCode)
			}
			return nil, &qwenErr
		}
		return respBody, nil
	})
	if err != nil {
		return nil, err
	}
	return resp.([]byte), nil
}
</file>

<file path="internal/repository/ai/qwen2_service.go">
package ai
import (
	"bytes"
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"sync"
	"time"
)
// Qwen2Service implements the AIService interface using Qwen2-Audio
type Qwen2Service struct {
	config  *domain.Qwen2Config
	client  *Qwen2Client
	metrics *domain.AIMetrics
	mu      sync.RWMutex // Protects metrics
}
// NewQwen2Service creates a new Qwen2Service instance
func NewQwen2Service(config *domain.Qwen2Config) (*Qwen2Service, error) {
	if config == nil {
		return nil, fmt.Errorf("qwen2 config is required")
	}
	if config.APIKey == "" {
		return nil, fmt.Errorf("qwen2 API key is required")
	}
	if config.Endpoint == "" {
		return nil, fmt.Errorf("qwen2 endpoint is required")
	}
	// Create Qwen2 client
	client, err := NewQwen2Client(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create qwen2 client: %w", err)
	}
	return &Qwen2Service{
		config: config,
		client: client,
		metrics: &domain.AIMetrics{
			RequestCount:   0,
			SuccessCount:   0,
			FailureCount:   0,
			AverageLatency: 0,
		},
	}, nil
}
// EnrichMetadata processes an audio track and enriches it with AI-generated metadata
func (s *Qwen2Service) EnrichMetadata(ctx context.Context, track *domain.Track) error {
	if track == nil {
		return fmt.Errorf("track is required")
	}
	startTime := time.Now()
	var processingErr error
	// Define retry strategy
	for attempt := 0; attempt <= s.config.RetryAttempts; attempt++ {
		if attempt > 0 {
			// Exponential backoff
			backoffDuration := time.Duration(attempt) * time.Second * time.Duration(s.config.RetryBackoffSeconds)
			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-time.After(backoffDuration):
			}
		}
		// Convert []byte to io.Reader
		audioReader := bytes.NewReader(track.AudioData)
		format := domain.AudioFormat(track.AudioFormat())
		// Call Qwen2 API
		response, err := s.client.AnalyzeAudio(ctx, audioReader, format)
		if err != nil {
			processingErr = fmt.Errorf("attempt %d: failed to analyze audio: %w", attempt+1, err)
			continue
		}
		// Check confidence threshold
		if response.Metadata.Confidence < s.config.MinConfidence {
			track.Metadata.AI = &domain.TrackAIMetadata{
				Tags:         response.Metadata.Tags,
				Confidence:   response.Metadata.Confidence,
				Model:        "qwen2",
				Version:      "v1",
				ProcessedAt:  time.Now(),
				NeedsReview:  true,
				ReviewReason: fmt.Sprintf("Low confidence score: %.2f", response.Metadata.Confidence),
			}
		} else {
			track.Metadata.AI = &domain.TrackAIMetadata{
				Tags:        response.Metadata.Tags,
				Confidence:  response.Metadata.Confidence,
				Model:       "qwen2",
				Version:     "v1",
				ProcessedAt: time.Now(),
				NeedsReview: false,
			}
		}
		// Update track fields
		track.SetGenre(response.Metadata.Genre)
		track.SetBPM(response.Metadata.BPM)
		track.SetKey(response.Metadata.Key)
		track.SetMood(response.Metadata.Mood)
		// Update metrics
		duration := time.Since(startTime)
		s.recordSuccess(duration)
		metrics.AIConfidenceScore.WithLabelValues(string(domain.AIProviderQwen2)).Observe(response.Metadata.Confidence)
		return nil // Successfully processed
	}
	// If we get here, we've exhausted all retry attempts
	s.recordFailure(processingErr)
	return fmt.Errorf("failed to enrich metadata after %d attempts: %w", s.config.RetryAttempts, processingErr)
}
// ValidateMetadata validates track metadata using AI
func (s *Qwen2Service) ValidateMetadata(ctx context.Context, track *domain.Track) (float64, error) {
	if track == nil {
		return 0, fmt.Errorf("track is required")
	}
	startTime := time.Now()
	var processingErr error
	// Define retry strategy
	for attempt := 0; attempt <= s.config.RetryAttempts; attempt++ {
		if attempt > 0 {
			// Exponential backoff
			backoffDuration := time.Duration(attempt) * time.Second * time.Duration(s.config.RetryBackoffSeconds)
			select {
			case <-ctx.Done():
				return 0, ctx.Err()
			case <-time.After(backoffDuration):
			}
		}
		// Validate metadata
		confidence, err := s.client.ValidateMetadata(ctx, track)
		if err != nil {
			processingErr = fmt.Errorf("attempt %d: failed to validate metadata: %w", attempt+1, err)
			continue // Try again if we have attempts left
		}
		// Update metrics
		duration := time.Since(startTime)
		s.recordSuccess(duration)
		metrics.AIConfidenceScore.WithLabelValues(string(domain.AIProviderQwen2)).Observe(confidence)
		// Return early if confidence meets threshold
		if confidence >= s.config.MinConfidence {
			return confidence, nil
		}
		// If confidence is too low, try again if we have attempts left
		processingErr = fmt.Errorf("confidence score too low: %.2f", confidence)
	}
	// If we get here, we've exhausted all retry attempts
	s.recordFailure(processingErr)
	return 0, fmt.Errorf("failed to validate metadata after %d attempts: %w", s.config.RetryAttempts, processingErr)
}
// BatchProcess processes multiple tracks in parallel
func (s *Qwen2Service) BatchProcess(ctx context.Context, tracks []*domain.Track) error {
	if len(tracks) == 0 {
		return nil
	}
	startTime := time.Now()
	// Create a semaphore to limit concurrent requests
	sem := make(chan struct{}, s.config.MaxConcurrentRequests)
	errChan := make(chan error, len(tracks))
	var wg sync.WaitGroup
	// Process tracks in parallel
	for i := range tracks {
		wg.Add(1)
		go func(track *domain.Track) {
			defer wg.Done()
			// Acquire semaphore
			sem <- struct{}{}
			defer func() { <-sem }()
			// Process individual track
			if err := s.EnrichMetadata(ctx, track); err != nil {
				errChan <- fmt.Errorf("failed to process track %s: %w", track.ID, err)
				return
			}
		}(tracks[i])
	}
	// Wait for all goroutines to finish
	wg.Wait()
	close(errChan)
	// Collect any errors
	var errors []error
	for err := range errChan {
		errors = append(errors, err)
	}
	// Update batch metrics
	duration := time.Since(startTime)
	metrics.AIBatchSize.WithLabelValues(string(domain.AIProviderQwen2)).Observe(float64(len(tracks)))
	metrics.AIRequestDuration.WithLabelValues(string(domain.AIProviderQwen2)).Observe(duration.Seconds())
	// Return combined errors if any
	if len(errors) > 0 {
		return fmt.Errorf("batch processing completed with %d errors: %v", len(errors), errors)
	}
	return nil
}
// recordSuccess updates metrics for successful requests
func (s *Qwen2Service) recordSuccess(duration time.Duration) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.metrics.RequestCount++
	s.metrics.SuccessCount++
	s.metrics.LastSuccess = time.Now()
	// Update average latency
	if s.metrics.AverageLatency == 0 {
		s.metrics.AverageLatency = duration
	} else {
		s.metrics.AverageLatency = (s.metrics.AverageLatency + duration) / 2
	}
	// Update Prometheus metrics
	metrics.AIRequestTotal.WithLabelValues(string(domain.AIProviderQwen2), "success").Inc()
	metrics.AIRequestDuration.WithLabelValues(string(domain.AIProviderQwen2)).Observe(duration.Seconds())
}
// recordFailure updates metrics for failed requests
func (s *Qwen2Service) recordFailure(err error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.metrics.RequestCount++
	s.metrics.FailureCount++
	s.metrics.LastError = err
	// Update Prometheus metrics
	metrics.AIRequestTotal.WithLabelValues(string(domain.AIProviderQwen2), "failure").Inc()
	metrics.AIErrorTotal.WithLabelValues(string(domain.AIProviderQwen2), err.Error()).Inc()
}
// getMetrics returns a copy of current metrics
func (s *Qwen2Service) getMetrics() *domain.AIMetrics {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return &domain.AIMetrics{
		RequestCount:   s.metrics.RequestCount,
		SuccessCount:   s.metrics.SuccessCount,
		FailureCount:   s.metrics.FailureCount,
		LastSuccess:    s.metrics.LastSuccess,
		LastError:      s.metrics.LastError,
		AverageLatency: s.metrics.AverageLatency,
	}
}
</file>

<file path="internal/repository/analytics/bigquery.go">
package analytics
import (
	"context"
	"fmt"
	"time"
	"metadatatool/internal/pkg/domain"
	"cloud.google.com/go/bigquery"
	"google.golang.org/api/iterator"
)
// AIExperimentRecord represents a single AI processing record
type AIExperimentRecord struct {
	Timestamp       time.Time
	TrackID         string
	ModelProvider   string
	ModelVersion    string
	ProcessingTime  float64
	Confidence      float64
	Success         bool
	ErrorMessage    string
	ExperimentGroup string // "control" or "experiment"
}
// BigQueryService handles analytics data storage
type BigQueryService struct {
	client          *bigquery.Client
	projectID       string
	dataset         string
	experimentTable *bigquery.Table
}
// NewBigQueryService creates a new BigQuery service
func NewBigQueryService(projectID, dataset string) (*BigQueryService, error) {
	ctx := context.Background()
	client, err := bigquery.NewClient(ctx, projectID)
	if err != nil {
		return nil, fmt.Errorf("failed to create BigQuery client: %w", err)
	}
	ds := client.Dataset(dataset)
	table := ds.Table("ai_experiments")
	return &BigQueryService{
		client:          client,
		projectID:       projectID,
		dataset:         dataset,
		experimentTable: table,
	}, nil
}
// RecordAIExperiment records an AI processing experiment
func (s *BigQueryService) RecordAIExperiment(ctx context.Context, record *AIExperimentRecord) error {
	inserter := s.experimentTable.Inserter()
	return inserter.Put(ctx, record)
}
// GetExperimentMetrics retrieves experiment metrics for a time range
func (s *BigQueryService) GetExperimentMetrics(ctx context.Context, start, end time.Time) (*domain.ExperimentMetrics, error) {
	query := s.client.Query(`
		SELECT
			experiment_group,
			COUNT(*) as total_requests,
			AVG(processing_time) as avg_processing_time,
			AVG(confidence) as avg_confidence,
			COUNTIF(success) / COUNT(*) as success_rate
		FROM ` + "`" + s.projectID + "." + s.dataset + ".ai_experiments" + "`" + `
		WHERE timestamp BETWEEN @start AND @end
		GROUP BY experiment_group
	`)
	query.Parameters = []bigquery.QueryParameter{
		{Name: "start", Value: start},
		{Name: "end", Value: end},
	}
	it, err := query.Read(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to execute query: %w", err)
	}
	metrics := &domain.ExperimentMetrics{
		Control:    &domain.ModelMetrics{},
		Experiment: &domain.ModelMetrics{},
	}
	for {
		var row struct {
			ExperimentGroup   string
			TotalRequests     int64
			AvgProcessingTime float64
			AvgConfidence     float64
			SuccessRate       float64
		}
		err := it.Next(&row)
		if err == iterator.Done {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("failed to read row: %w", err)
		}
		if row.ExperimentGroup == "control" {
			metrics.Control.TotalRequests = row.TotalRequests
			metrics.Control.AvgProcessingTime = row.AvgProcessingTime
			metrics.Control.AvgConfidence = row.AvgConfidence
			metrics.Control.SuccessRate = row.SuccessRate
		} else {
			metrics.Experiment.TotalRequests = row.TotalRequests
			metrics.Experiment.AvgProcessingTime = row.AvgProcessingTime
			metrics.Experiment.AvgConfidence = row.AvgConfidence
			metrics.Experiment.SuccessRate = row.SuccessRate
		}
	}
	return metrics, nil
}
// Close closes the BigQuery client
func (s *BigQueryService) Close() error {
	return s.client.Close()
}
</file>

<file path="internal/repository/audio/audio_processor.go">
package audio
import (
	"bytes"
	"context"
	"fmt"
	"io"
	"time"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
)
// Processor implements the AudioProcessor interface
type Processor struct {
	// Dependencies could be added here
}
// NewProcessor creates a new audio processor
func NewProcessor() domain.AudioProcessor {
	return &Processor{}
}
// Process processes an audio file and returns its metadata
func (p *Processor) Process(ctx context.Context, file *domain.ProcessingAudioFile, options *domain.AudioProcessOptions) (*domain.AudioProcessResult, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("process"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("process", "started").Inc()
	// Read file content if needed
	data, err := io.ReadAll(file.Content)
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("process", "read_error").Inc()
		return nil, fmt.Errorf("failed to read audio file: %w", err)
	}
	// Extract metadata if requested
	var metadata *domain.CompleteTrackMetadata
	if options.ExtractMetadata {
		metadata = &domain.CompleteTrackMetadata{}
		// Extract technical metadata
		metadata.Technical = domain.AudioTechnicalMetadata{
			Format:   file.Format,
			FileSize: file.Size,
			// Add other technical details extraction here
		}
		// Extract basic metadata
		metadata.BasicTrackMetadata = domain.BasicTrackMetadata{
			Title:     file.Name, // Default to filename, can be overridden
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
		}
	}
	// Perform audio analysis if requested
	if options.AnalyzeAudio {
		analysis, err := p.analyzeAudio(ctx, bytes.NewReader(data), file.Format)
		if err != nil {
			metrics.AudioOpErrors.WithLabelValues("process", "analysis_error").Inc()
			return nil, fmt.Errorf("failed to analyze audio: %w", err)
		}
		// Update musical metadata based on analysis
		metadata.Musical = domain.MusicalMetadata{
			BPM:    analysis.BPM,
			Key:    analysis.Key,
			Mode:   analysis.Mode,
			Tempo:  analysis.Tempo,
			Energy: analysis.Energy,
		}
	}
	metrics.AudioOps.WithLabelValues("process", "completed").Inc()
	return &domain.AudioProcessResult{
		Metadata:     metadata,
		AnalyzerInfo: "audio_processor_v1", // Add version or processor info
	}, nil
}
func (p *Processor) analyzeAudio(ctx context.Context, reader io.Reader, format domain.AudioFormat) (*domain.AudioAnalysis, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("analyze"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("analyze", "started").Inc()
	// TODO: Implement actual audio analysis
	// This is a placeholder implementation
	analysis := &domain.AudioAnalysis{
		BPM:          120.0,
		Key:          "C",
		Mode:         "major",
		Tempo:        120.0,
		Energy:       0.8,
		AnalyzedAt:   time.Now(),
		SampleRate:   44100,
		WindowSize:   2048,
		HopSize:      512,
		AnalyzerInfo: "basic_analyzer_v1",
	}
	metrics.AudioOps.WithLabelValues("analyze", "completed").Inc()
	return analysis, nil
}
func (p *Processor) detectFormat(data []byte) domain.AudioFormat {
	// TODO: Implement format detection based on file magic numbers
	return domain.AudioFormatMP3
}
</file>

<file path="internal/repository/auth/jwt_service_test.go">
package auth
import (
	"testing"
	"time"
	"github.com/google/uuid"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"metadatatool/internal/pkg/domain"
)
func createTestUser() *domain.User {
	return &domain.User{
		ID:          uuid.New().String(),
		Email:       "test@example.com",
		Name:        "Test User",
		Role:        domain.RoleUser,
		Permissions: domain.RolePermissions[domain.RoleUser],
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
	}
}
func TestNewJWTService(t *testing.T) {
	t.Run("with provided key", func(t *testing.T) {
		key := []byte("test-secret-key")
		service, err := NewJWTService(key)
		require.NoError(t, err)
		assert.Equal(t, key, service.secretKey)
	})
	t.Run("with auto-generated key", func(t *testing.T) {
		service, err := NewJWTService(nil)
		require.NoError(t, err)
		assert.Len(t, service.secretKey, SecretKeyLength)
	})
}
func TestJWTService_GenerateTokens(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("successful token generation", func(t *testing.T) {
		user := createTestUser()
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		assert.NotEmpty(t, tokens.AccessToken)
		assert.NotEmpty(t, tokens.RefreshToken)
		// Validate access token
		claims, err := service.ValidateToken(tokens.AccessToken)
		require.NoError(t, err)
		assert.Equal(t, user.ID, claims.UserID)
		assert.Equal(t, user.Email, claims.Email)
		assert.Equal(t, user.Role, claims.Role)
		assert.Equal(t, user.Permissions, claims.Permissions)
		// Validate refresh token
		claims, err = service.ValidateToken(tokens.RefreshToken)
		require.NoError(t, err)
		assert.Equal(t, user.ID, claims.UserID)
	})
}
func TestJWTService_ValidateToken(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("valid token", func(t *testing.T) {
		user := createTestUser()
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		claims, err := service.ValidateToken(tokens.AccessToken)
		require.NoError(t, err)
		assert.Equal(t, user.ID, claims.UserID)
		assert.Equal(t, user.Email, claims.Email)
		assert.Equal(t, user.Role, claims.Role)
	})
	t.Run("invalid token", func(t *testing.T) {
		_, err := service.ValidateToken("invalid-token")
		assert.Error(t, err)
	})
	t.Run("expired token", func(t *testing.T) {
		// Create a service with very short token duration for testing
		shortDurationService := &JWTService{
			secretKey: []byte("test-secret-key"),
		}
		user := createTestUser()
		token, err := shortDurationService.createToken(user, time.Millisecond)
		require.NoError(t, err)
		// Wait for token to expire
		time.Sleep(time.Millisecond * 2)
		_, err = shortDurationService.ValidateToken(token)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "token is expired")
	})
}
func TestJWTService_RefreshToken(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("successful refresh", func(t *testing.T) {
		user := createTestUser()
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		newTokens, err := service.RefreshToken(tokens.RefreshToken)
		require.NoError(t, err)
		assert.NotEmpty(t, newTokens.AccessToken)
		assert.NotEmpty(t, newTokens.RefreshToken)
		assert.NotEqual(t, tokens.AccessToken, newTokens.AccessToken)
		assert.NotEqual(t, tokens.RefreshToken, newTokens.RefreshToken)
	})
	t.Run("invalid refresh token", func(t *testing.T) {
		_, err := service.RefreshToken("invalid-token")
		assert.Error(t, err)
	})
}
func TestJWTService_PasswordHashing(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("password hashing and verification", func(t *testing.T) {
		password := "test-password"
		// Hash password
		hash, err := service.HashPassword(password)
		require.NoError(t, err)
		assert.NotEqual(t, password, hash)
		// Verify correct password
		err = service.VerifyPassword(hash, password)
		assert.NoError(t, err)
		// Verify incorrect password
		err = service.VerifyPassword(hash, "wrong-password")
		assert.Error(t, err)
	})
}
func TestJWTService_GenerateAPIKey(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("api key generation", func(t *testing.T) {
		key1, err := service.GenerateAPIKey()
		require.NoError(t, err)
		assert.NotEmpty(t, key1)
		key2, err := service.GenerateAPIKey()
		require.NoError(t, err)
		assert.NotEmpty(t, key2)
		// Keys should be different
		assert.NotEqual(t, key1, key2)
	})
}
func TestJWTService_Permissions(t *testing.T) {
	service, err := NewJWTService([]byte("test-secret-key"))
	require.NoError(t, err)
	t.Run("check permissions", func(t *testing.T) {
		// Admin should have all permissions
		assert.True(t, service.HasPermission(domain.RoleAdmin, domain.PermissionManageUsers))
		assert.True(t, service.HasPermission(domain.RoleAdmin, domain.PermissionCreateTrack))
		// Regular user should have limited permissions
		assert.True(t, service.HasPermission(domain.RoleUser, domain.PermissionReadTrack))
		assert.False(t, service.HasPermission(domain.RoleUser, domain.PermissionManageUsers))
		// Guest should only have read permission
		assert.True(t, service.HasPermission(domain.RoleGuest, domain.PermissionReadTrack))
		assert.False(t, service.HasPermission(domain.RoleGuest, domain.PermissionCreateTrack))
	})
	t.Run("get permissions", func(t *testing.T) {
		adminPerms := service.GetPermissions(domain.RoleAdmin)
		assert.Contains(t, adminPerms, domain.PermissionManageUsers)
		assert.Contains(t, adminPerms, domain.PermissionCreateTrack)
		userPerms := service.GetPermissions(domain.RoleUser)
		assert.Contains(t, userPerms, domain.PermissionReadTrack)
		assert.NotContains(t, userPerms, domain.PermissionManageUsers)
		guestPerms := service.GetPermissions(domain.RoleGuest)
		assert.Contains(t, guestPerms, domain.PermissionReadTrack)
		assert.NotContains(t, guestPerms, domain.PermissionCreateTrack)
	})
}
</file>

<file path="internal/repository/auth/jwt_service.go">
package auth
import (
	"crypto/rand"
	"encoding/base64"
	"errors"
	"fmt"
	"time"
	"github.com/golang-jwt/jwt/v5"
	"github.com/google/uuid"
	"golang.org/x/crypto/bcrypt"
	"metadatatool/internal/pkg/domain"
)
const (
	// Token durations
	accessTokenDuration  = 15 * time.Minute
	refreshTokenDuration = 7 * 24 * time.Hour
	// Key lengths
	apiKeyLength    = 32
	SecretKeyLength = 32 // Exported for tests
)
// customClaims extends jwt.RegisteredClaims with our custom claims
type customClaims struct {
	jwt.RegisteredClaims
	UserID      string              `json:"uid"`
	Email       string              `json:"email"`
	Role        domain.Role         `json:"role"`
	Permissions []domain.Permission `json:"permissions"`
}
// JWTService implements the domain.AuthService interface
type JWTService struct {
	secretKey []byte
}
// NewJWTService creates a new JWT authentication service
func NewJWTService(secretKey []byte) (*JWTService, error) {
	if len(secretKey) == 0 {
		// Generate a random secret key if none provided
		key := make([]byte, SecretKeyLength) // Use exported constant
		if _, err := rand.Read(key); err != nil {
			return nil, fmt.Errorf("failed to generate secret key: %w", err)
		}
		secretKey = key
	}
	return &JWTService{
		secretKey: secretKey,
	}, nil
}
// GenerateTokens creates a new pair of access and refresh tokens
func (s *JWTService) GenerateTokens(user *domain.User) (*domain.TokenPair, error) {
	// Generate access token
	accessToken, err := s.createToken(user, accessTokenDuration)
	if err != nil {
		return nil, fmt.Errorf("failed to create access token: %w", err)
	}
	// Generate refresh token
	refreshToken, err := s.createToken(user, refreshTokenDuration)
	if err != nil {
		return nil, fmt.Errorf("failed to create refresh token: %w", err)
	}
	return &domain.TokenPair{
		AccessToken:  accessToken,
		RefreshToken: refreshToken,
	}, nil
}
// ValidateToken validates and parses a JWT token
func (s *JWTService) ValidateToken(tokenString string) (*domain.Claims, error) {
	if tokenString == "" {
		return nil, fmt.Errorf("%w: token is empty", domain.ErrInvalidToken)
	}
	token, err := jwt.ParseWithClaims(tokenString, &customClaims{}, func(token *jwt.Token) (interface{}, error) {
		if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
			return nil, fmt.Errorf("%w: invalid signing method", domain.ErrInvalidToken)
		}
		return s.secretKey, nil
	})
	if err != nil {
		if errors.Is(err, jwt.ErrTokenExpired) {
			return nil, fmt.Errorf("%w: token is expired", domain.ErrInvalidToken)
		}
		return nil, fmt.Errorf("%w: token is invalid", domain.ErrInvalidToken)
	}
	if !token.Valid {
		return nil, fmt.Errorf("%w: token is invalid", domain.ErrInvalidToken)
	}
	claims, ok := token.Claims.(*customClaims)
	if !ok {
		return nil, fmt.Errorf("%w: invalid claims format", domain.ErrInvalidToken)
	}
	return &domain.Claims{
		UserID:      claims.UserID,
		Email:       claims.Email,
		Role:        claims.Role,
		Permissions: claims.Permissions,
	}, nil
}
// RefreshToken validates a refresh token and generates new token pair
func (s *JWTService) RefreshToken(refreshToken string) (*domain.TokenPair, error) {
	claims, err := s.ValidateToken(refreshToken)
	if err != nil {
		return nil, fmt.Errorf("invalid refresh token: %w", err)
	}
	// Create a temporary user object to generate new tokens
	user := &domain.User{
		ID:          claims.UserID,
		Email:       claims.Email,
		Role:        claims.Role,
		Permissions: claims.Permissions,
	}
	return s.GenerateTokens(user)
}
// HashPassword creates a bcrypt hash of the password
func (s *JWTService) HashPassword(password string) (string, error) {
	hashedBytes, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)
	if err != nil {
		return "", fmt.Errorf("failed to hash password: %w", err)
	}
	return string(hashedBytes), nil
}
// VerifyPassword checks if the provided password matches the hash
func (s *JWTService) VerifyPassword(hashedPassword, password string) error {
	return bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(password))
}
// GenerateAPIKey creates a new API key
func (s *JWTService) GenerateAPIKey() (string, error) {
	bytes := make([]byte, apiKeyLength)
	if _, err := rand.Read(bytes); err != nil {
		return "", fmt.Errorf("failed to generate API key: %w", err)
	}
	return base64.URLEncoding.EncodeToString(bytes), nil
}
// HasPermission checks if a role has a specific permission
func (s *JWTService) HasPermission(role domain.Role, permission domain.Permission) bool {
	permissions := s.GetPermissions(role)
	for _, p := range permissions {
		if p == permission {
			return true
		}
	}
	return false
}
// GetPermissions returns all permissions for a role
func (s *JWTService) GetPermissions(role domain.Role) []domain.Permission {
	return domain.RolePermissions[role]
}
// Helper method to create a token
func (s *JWTService) createToken(user *domain.User, duration time.Duration) (string, error) {
	now := time.Now()
	claims := customClaims{
		RegisteredClaims: jwt.RegisteredClaims{
			ExpiresAt: jwt.NewNumericDate(now.Add(duration)),
			IssuedAt:  jwt.NewNumericDate(now),
			NotBefore: jwt.NewNumericDate(now),
			ID:        uuid.NewString(),
		},
		UserID:      user.ID,
		Email:       user.Email,
		Role:        user.Role,
		Permissions: user.Permissions,
	}
	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	return token.SignedString(s.secretKey)
}
</file>

<file path="internal/repository/base/memory_auth_service.go">
package base
import (
	"crypto/rand"
	"encoding/hex"
	"metadatatool/internal/pkg/domain"
	"time"
	"github.com/golang-jwt/jwt/v5"
	"golang.org/x/crypto/bcrypt"
)
type inMemoryAuthService struct {
	secretKey []byte
}
// NewInMemoryAuthService creates a new in-memory auth service
func NewInMemoryAuthService() domain.AuthService {
	return &inMemoryAuthService{
		secretKey: []byte("test-secret-key"),
	}
}
// GenerateTokens creates a new pair of access and refresh tokens
func (s *inMemoryAuthService) GenerateTokens(user *domain.User) (*domain.TokenPair, error) {
	// Create claims for access token
	claims := &domain.Claims{
		UserID:      user.ID,
		Email:       user.Email,
		Role:        user.Role,
		Permissions: domain.RolePermissions[user.Role],
		RegisteredClaims: jwt.RegisteredClaims{
			ExpiresAt: jwt.NewNumericDate(time.Now().Add(15 * time.Minute)),
			IssuedAt:  jwt.NewNumericDate(time.Now()),
		},
	}
	// Create access token
	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	tokenString, err := token.SignedString(s.secretKey)
	if err != nil {
		return nil, err
	}
	// Create refresh token claims
	refreshClaims := &domain.Claims{
		UserID:      user.ID,
		Email:       user.Email,
		Role:        user.Role,
		Permissions: domain.RolePermissions[user.Role],
		RegisteredClaims: jwt.RegisteredClaims{
			ExpiresAt: jwt.NewNumericDate(time.Now().Add(7 * 24 * time.Hour)),
			IssuedAt:  jwt.NewNumericDate(time.Now()),
		},
	}
	// Create refresh token
	refreshToken := jwt.NewWithClaims(jwt.SigningMethodHS256, refreshClaims)
	refreshTokenString, err := refreshToken.SignedString(s.secretKey)
	if err != nil {
		return nil, err
	}
	return &domain.TokenPair{
		AccessToken:  tokenString,
		RefreshToken: refreshTokenString,
	}, nil
}
// ValidateToken validates and parses a JWT token
func (s *inMemoryAuthService) ValidateToken(tokenString string) (*domain.Claims, error) {
	token, err := jwt.ParseWithClaims(tokenString, &domain.Claims{}, func(token *jwt.Token) (interface{}, error) {
		return s.secretKey, nil
	})
	if err != nil {
		return nil, err
	}
	if claims, ok := token.Claims.(*domain.Claims); ok && token.Valid {
		return claims, nil
	}
	return nil, domain.ErrInvalidToken
}
// RefreshToken validates a refresh token and generates new token pair
func (s *inMemoryAuthService) RefreshToken(refreshToken string) (*domain.TokenPair, error) {
	claims, err := s.ValidateToken(refreshToken)
	if err != nil {
		return nil, err
	}
	user := &domain.User{
		ID:    claims.UserID,
		Email: claims.Email,
		Role:  claims.Role,
	}
	return s.GenerateTokens(user)
}
// HashPassword creates a bcrypt hash of the password
func (s *inMemoryAuthService) HashPassword(password string) (string, error) {
	hashedBytes, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)
	if err != nil {
		return "", err
	}
	return string(hashedBytes), nil
}
// VerifyPassword checks if the provided password matches the hash
func (s *inMemoryAuthService) VerifyPassword(hashedPassword, password string) error {
	return bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(password))
}
// GenerateAPIKey creates a new API key
func (s *inMemoryAuthService) GenerateAPIKey() (string, error) {
	bytes := make([]byte, 32)
	if _, err := rand.Read(bytes); err != nil {
		return "", err
	}
	return hex.EncodeToString(bytes), nil
}
// HasPermission checks if a role has a specific permission
func (s *inMemoryAuthService) HasPermission(role domain.Role, permission domain.Permission) bool {
	permissions := domain.RolePermissions[role]
	for _, p := range permissions {
		if p == permission {
			return true
		}
	}
	return false
}
// GetPermissions returns all permissions for a role
func (s *inMemoryAuthService) GetPermissions(role domain.Role) []domain.Permission {
	return domain.RolePermissions[role]
}
</file>

<file path="internal/repository/base/memory_repository.go">
package base
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"sync"
	"github.com/google/uuid"
)
// InMemoryUserRepository implements domain.UserRepository for testing
type InMemoryUserRepository struct {
	users     map[string]*domain.User // ID -> User
	emailMap  map[string]string       // Email -> ID
	apiKeyMap map[string]string       // APIKey -> ID
	mu        sync.RWMutex
}
// NewInMemoryUserRepository creates a new in-memory user repository
func NewInMemoryUserRepository() domain.UserRepository {
	return &InMemoryUserRepository{
		users:     make(map[string]*domain.User),
		emailMap:  make(map[string]string),
		apiKeyMap: make(map[string]string),
	}
}
// Create stores a new user
func (r *InMemoryUserRepository) Create(ctx context.Context, user *domain.User) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	if user.ID == "" {
		user.ID = uuid.New().String()
	}
	if _, exists := r.emailMap[user.Email]; exists {
		return fmt.Errorf("email already exists")
	}
	if user.APIKey != "" {
		if _, exists := r.apiKeyMap[user.APIKey]; exists {
			return fmt.Errorf("API key already exists")
		}
		r.apiKeyMap[user.APIKey] = user.ID
	}
	r.users[user.ID] = user
	r.emailMap[user.Email] = user.ID
	return nil
}
// GetByID retrieves a user by ID
func (r *InMemoryUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	user, exists := r.users[id]
	if !exists {
		return nil, fmt.Errorf("user not found")
	}
	return user, nil
}
// GetByEmail retrieves a user by email
func (r *InMemoryUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	id, exists := r.emailMap[email]
	if !exists {
		return nil, nil
	}
	return r.users[id], nil
}
// GetByAPIKey retrieves a user by API key
func (r *InMemoryUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	id, exists := r.apiKeyMap[apiKey]
	if !exists {
		return nil, nil
	}
	return r.users[id], nil
}
// Update modifies an existing user
func (r *InMemoryUserRepository) Update(ctx context.Context, user *domain.User) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	if _, exists := r.users[user.ID]; !exists {
		return fmt.Errorf("user not found")
	}
	// Update email mapping if changed
	oldUser := r.users[user.ID]
	if oldUser.Email != user.Email {
		delete(r.emailMap, oldUser.Email)
		r.emailMap[user.Email] = user.ID
	}
	// Update API key mapping if changed
	if oldUser.APIKey != user.APIKey {
		if oldUser.APIKey != "" {
			delete(r.apiKeyMap, oldUser.APIKey)
		}
		if user.APIKey != "" {
			r.apiKeyMap[user.APIKey] = user.ID
		}
	}
	r.users[user.ID] = user
	return nil
}
// Delete removes a user
func (r *InMemoryUserRepository) Delete(ctx context.Context, id string) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	user, exists := r.users[id]
	if !exists {
		return fmt.Errorf("user not found")
	}
	delete(r.emailMap, user.Email)
	if user.APIKey != "" {
		delete(r.apiKeyMap, user.APIKey)
	}
	delete(r.users, id)
	return nil
}
// List retrieves users with pagination
func (r *InMemoryUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	users := make([]*domain.User, 0, len(r.users))
	for _, user := range r.users {
		users = append(users, user)
	}
	// Apply pagination
	if offset >= len(users) {
		return []*domain.User{}, nil
	}
	end := offset + limit
	if end > len(users) {
		end = len(users)
	}
	return users[offset:end], nil
}
// UpdateAPIKey updates the API key for a user
func (r *InMemoryUserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	user, exists := r.users[userID]
	if !exists {
		return domain.ErrUserNotFound
	}
	user.APIKey = apiKey
	r.users[userID] = user
	return nil
}
</file>

<file path="internal/repository/base/memory_session_repository.go">
package base
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"sync"
	"time"
	"github.com/google/uuid"
)
// InMemorySessionRepository implements domain.SessionStore for testing
type InMemorySessionRepository struct {
	sessions     map[string]*domain.Session // SessionID -> Session
	userSessions map[string][]string        // UserID -> []SessionID
	mu           sync.RWMutex
}
// NewInMemorySessionRepository creates a new in-memory session repository
func NewInMemorySessionRepository() domain.SessionStore {
	return &InMemorySessionRepository{
		sessions:     make(map[string]*domain.Session),
		userSessions: make(map[string][]string),
	}
}
// Create stores a new session
func (r *InMemorySessionRepository) Create(ctx context.Context, session *domain.Session) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	if session.ID == "" {
		session.ID = uuid.New().String()
	}
	// Set timestamps if not set
	now := time.Now()
	if session.CreatedAt.IsZero() {
		session.CreatedAt = now
	}
	if session.LastSeenAt.IsZero() {
		session.LastSeenAt = now
	}
	if session.ExpiresAt.IsZero() {
		session.ExpiresAt = now.Add(24 * time.Hour)
	}
	r.sessions[session.ID] = session
	r.userSessions[session.UserID] = append(r.userSessions[session.UserID], session.ID)
	return nil
}
// Get retrieves a session by ID
func (r *InMemorySessionRepository) Get(ctx context.Context, sessionID string) (*domain.Session, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	session, exists := r.sessions[sessionID]
	if !exists {
		return nil, domain.ErrSessionNotFound
	}
	if session.ExpiresAt.Before(time.Now()) {
		return nil, domain.ErrSessionNotFound
	}
	// Update last seen
	session.LastSeenAt = time.Now()
	return session, nil
}
// GetUserSessions retrieves all sessions for a user
func (r *InMemorySessionRepository) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	r.mu.RLock()
	defer r.mu.RUnlock()
	sessionIDs, exists := r.userSessions[userID]
	if !exists {
		return []*domain.Session{}, nil
	}
	var activeSessions []*domain.Session
	now := time.Now()
	for _, id := range sessionIDs {
		session := r.sessions[id]
		if session.ExpiresAt.After(now) {
			activeSessions = append(activeSessions, session)
		}
	}
	return activeSessions, nil
}
// Update updates an existing session
func (r *InMemorySessionRepository) Update(ctx context.Context, session *domain.Session) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	if _, exists := r.sessions[session.ID]; !exists {
		return fmt.Errorf("session not found")
	}
	r.sessions[session.ID] = session
	return nil
}
// Delete removes a session
func (r *InMemorySessionRepository) Delete(ctx context.Context, sessionID string) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	session, exists := r.sessions[sessionID]
	if !exists {
		return fmt.Errorf("session not found")
	}
	// Remove from userSessions
	sessions := r.userSessions[session.UserID]
	for i, sid := range sessions {
		if sid == sessionID {
			r.userSessions[session.UserID] = append(sessions[:i], sessions[i+1:]...)
			break
		}
	}
	delete(r.sessions, sessionID)
	return nil
}
// DeleteUserSessions removes all sessions for a user
func (r *InMemorySessionRepository) DeleteUserSessions(ctx context.Context, userID string) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	sessionIDs, exists := r.userSessions[userID]
	if !exists {
		return nil
	}
	for _, id := range sessionIDs {
		delete(r.sessions, id)
	}
	delete(r.userSessions, userID)
	return nil
}
// DeleteExpired removes all expired sessions
func (r *InMemorySessionRepository) DeleteExpired(ctx context.Context) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	now := time.Now()
	for id, session := range r.sessions {
		if session.ExpiresAt.Before(now) {
			// Remove from userSessions
			sessions := r.userSessions[session.UserID]
			for i, sid := range sessions {
				if sid == id {
					r.userSessions[session.UserID] = append(sessions[:i], sessions[i+1:]...)
					break
				}
			}
			delete(r.sessions, id)
		}
	}
	return nil
}
// Touch updates the last seen time of a session
func (r *InMemorySessionRepository) Touch(ctx context.Context, sessionID string) error {
	r.mu.Lock()
	defer r.mu.Unlock()
	session, exists := r.sessions[sessionID]
	if !exists {
		return fmt.Errorf("session not found")
	}
	session.LastSeenAt = time.Now()
	return r.Update(ctx, session)
}
</file>

<file path="internal/repository/base/track_repository.go">
package base
import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
)
// TrackRepository implements domain.TrackRepository with PostgreSQL
type TrackRepository struct {
	db *sql.DB
}
// NewTrackRepository creates a new track repository
func NewTrackRepository(db *sql.DB) domain.TrackRepository {
	return &TrackRepository{
		db: db,
	}
}
// Create inserts a new track
func (r *TrackRepository) Create(ctx context.Context, track *domain.Track) error {
	query := `
		INSERT INTO tracks (
			id, storage_path, file_size, metadata, created_at, updated_at
		) VALUES ($1, $2, $3, $4, $5, $6)
		RETURNING id`
	metadata, err := json.Marshal(track.Metadata)
	if err != nil {
		return fmt.Errorf("failed to marshal metadata: %w", err)
	}
	err = r.db.QueryRowContext(ctx, query,
		track.ID, track.StoragePath, track.FileSize,
		metadata, track.CreatedAt, track.UpdatedAt,
	).Scan(&track.ID)
	if err != nil {
		return fmt.Errorf("failed to create track: %w", err)
	}
	return nil
}
// GetByID retrieves a track by ID
func (r *TrackRepository) GetByID(ctx context.Context, id string) (*domain.Track, error) {
	query := `
		SELECT id, storage_path, file_size, metadata, created_at, updated_at, deleted_at
		FROM tracks
		WHERE id = $1 AND deleted_at IS NULL`
	track := &domain.Track{}
	var metadataJSON []byte
	err := r.db.QueryRowContext(ctx, query, id).Scan(
		&track.ID, &track.StoragePath, &track.FileSize,
		&metadataJSON, &track.CreatedAt, &track.UpdatedAt, &track.DeletedAt,
	)
	if err == sql.ErrNoRows {
		return nil, nil
	}
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	if len(metadataJSON) > 0 {
		if err := json.Unmarshal(metadataJSON, &track.Metadata); err != nil {
			return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
		}
	}
	return track, nil
}
// List retrieves tracks based on filters with pagination
func (r *TrackRepository) List(ctx context.Context, filters map[string]interface{}, offset, limit int) ([]*domain.Track, error) {
	query := `
		SELECT id, storage_path, file_size, metadata, created_at, updated_at, deleted_at
		FROM tracks
		WHERE deleted_at IS NULL
		ORDER BY created_at DESC
		LIMIT $1 OFFSET $2`
	rows, err := r.db.QueryContext(ctx, query, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to list tracks: %w", err)
	}
	defer rows.Close()
	var tracks []*domain.Track
	for rows.Next() {
		track := &domain.Track{}
		var metadataJSON []byte
		err := rows.Scan(
			&track.ID, &track.StoragePath, &track.FileSize,
			&metadataJSON, &track.CreatedAt, &track.UpdatedAt, &track.DeletedAt,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan track: %w", err)
		}
		if len(metadataJSON) > 0 {
			if err := json.Unmarshal(metadataJSON, &track.Metadata); err != nil {
				return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
			}
		}
		tracks = append(tracks, track)
	}
	return tracks, nil
}
// SearchByMetadata searches tracks by metadata fields
func (r *TrackRepository) SearchByMetadata(ctx context.Context, query map[string]interface{}) ([]*domain.Track, error) {
	// Build query dynamically based on search criteria
	sqlQuery := `
		SELECT id, storage_path, file_size, metadata, created_at, updated_at, deleted_at
		FROM tracks
		WHERE deleted_at IS NULL`
	var params []interface{}
	var conditions []string
	// Add search conditions
	for key, value := range query {
		params = append(params, value)
		conditions = append(conditions, fmt.Sprintf("metadata->>'%s' = $%d", key, len(params)))
	}
	if len(conditions) > 0 {
		sqlQuery += " AND " + conditions[0]
		for i := 1; i < len(conditions); i++ {
			sqlQuery += " AND " + conditions[i]
		}
	}
	sqlQuery += " ORDER BY created_at DESC"
	rows, err := r.db.QueryContext(ctx, sqlQuery, params...)
	if err != nil {
		return nil, fmt.Errorf("failed to search tracks: %w", err)
	}
	defer rows.Close()
	var tracks []*domain.Track
	for rows.Next() {
		track := &domain.Track{}
		var metadataJSON []byte
		err := rows.Scan(
			&track.ID, &track.StoragePath, &track.FileSize,
			&metadataJSON, &track.CreatedAt, &track.UpdatedAt, &track.DeletedAt,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan track: %w", err)
		}
		if len(metadataJSON) > 0 {
			if err := json.Unmarshal(metadataJSON, &track.Metadata); err != nil {
				return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
			}
		}
		tracks = append(tracks, track)
	}
	return tracks, nil
}
// Update updates an existing track
func (r *TrackRepository) Update(ctx context.Context, track *domain.Track) error {
	query := `
		UPDATE tracks
		SET storage_path = $1,
			file_size = $2,
			metadata = $3,
			updated_at = $4
		WHERE id = $5 AND deleted_at IS NULL`
	metadata, err := json.Marshal(track.Metadata)
	if err != nil {
		return fmt.Errorf("failed to marshal metadata: %w", err)
	}
	result, err := r.db.ExecContext(ctx, query,
		track.StoragePath, track.FileSize,
		metadata, time.Now(), track.ID,
	)
	if err != nil {
		return fmt.Errorf("failed to update track: %w", err)
	}
	rows, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("failed to get affected rows: %w", err)
	}
	if rows == 0 {
		return fmt.Errorf("track not found")
	}
	return nil
}
// Delete soft-deletes a track
func (r *TrackRepository) Delete(ctx context.Context, id string) error {
	query := `
		UPDATE tracks
		SET deleted_at = $1
		WHERE id = $2 AND deleted_at IS NULL`
	result, err := r.db.ExecContext(ctx, query, time.Now(), id)
	if err != nil {
		return fmt.Errorf("failed to delete track: %w", err)
	}
	rows, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("failed to get affected rows: %w", err)
	}
	if rows == 0 {
		return fmt.Errorf("track not found")
	}
	return nil
}
// GetByISRC retrieves a track by ISRC
func (r *TrackRepository) GetByISRC(ctx context.Context, isrc string) (*domain.Track, error) {
	query := `
		SELECT id, storage_path, file_size, metadata, created_at, updated_at, deleted_at
		FROM tracks
		WHERE metadata->>'isrc' = $1 AND deleted_at IS NULL`
	track := &domain.Track{}
	var metadataJSON []byte
	err := r.db.QueryRowContext(ctx, query, isrc).Scan(
		&track.ID, &track.StoragePath, &track.FileSize,
		&metadataJSON, &track.CreatedAt, &track.UpdatedAt, &track.DeletedAt,
	)
	if err == sql.ErrNoRows {
		return nil, nil
	}
	if err != nil {
		return nil, fmt.Errorf("failed to get track by ISRC: %w", err)
	}
	if len(metadataJSON) > 0 {
		if err := json.Unmarshal(metadataJSON, &track.Metadata); err != nil {
			return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
		}
	}
	return track, nil
}
// BatchUpdate updates multiple tracks in a single transaction
func (r *TrackRepository) BatchUpdate(ctx context.Context, tracks []*domain.Track) error {
	tx, err := r.db.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()
	query := `
		UPDATE tracks
		SET storage_path = $1,
			file_size = $2,
			metadata = $3,
			updated_at = $4
		WHERE id = $5 AND deleted_at IS NULL`
	stmt, err := tx.PrepareContext(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()
	for _, track := range tracks {
		metadata, err := json.Marshal(track.Metadata)
		if err != nil {
			return fmt.Errorf("failed to marshal metadata: %w", err)
		}
		result, err := stmt.ExecContext(ctx,
			track.StoragePath, track.FileSize,
			metadata, time.Now(), track.ID,
		)
		if err != nil {
			return fmt.Errorf("failed to update track %s: %w", track.ID, err)
		}
		rows, err := result.RowsAffected()
		if err != nil {
			return fmt.Errorf("failed to get affected rows: %w", err)
		}
		if rows == 0 {
			return fmt.Errorf("track %s not found", track.ID)
		}
	}
	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}
	return nil
}
</file>

<file path="internal/repository/base/user_repository.go">
package base
import (
	"context"
	"database/sql"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
)
// UserRepository implements domain.UserRepository with PostgreSQL
type UserRepository struct {
	db *sql.DB
}
// NewUserRepository creates a new user repository
func NewUserRepository(db *sql.DB) domain.UserRepository {
	return &UserRepository{
		db: db,
	}
}
// Create inserts a new user
func (r *UserRepository) Create(ctx context.Context, user *domain.User) error {
	query := `
		INSERT INTO users (
			email, password, name, role, company, api_key, plan,
			track_quota, tracks_used, quota_reset_date, last_login_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
		RETURNING id`
	err := r.db.QueryRowContext(ctx, query,
		user.Email, user.Password, user.Name, user.Role, user.Company,
		user.APIKey, user.Plan, user.TrackQuota, user.TracksUsed,
		user.QuotaResetDate, user.LastLoginAt,
	).Scan(&user.ID)
	if err != nil {
		return fmt.Errorf("failed to create user: %w", err)
	}
	return nil
}
// GetByID retrieves a user by ID
func (r *UserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	query := `
		SELECT id, email, password, name, role, company, api_key,
			plan, track_quota, tracks_used, quota_reset_date, last_login_at
		FROM users
		WHERE id = $1 AND deleted_at IS NULL`
	user := &domain.User{}
	err := r.db.QueryRowContext(ctx, query, id).Scan(
		&user.ID, &user.Email, &user.Password, &user.Name, &user.Role,
		&user.Company, &user.APIKey, &user.Plan, &user.TrackQuota,
		&user.TracksUsed, &user.QuotaResetDate, &user.LastLoginAt,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to get user: %w", err)
	}
	return user, nil
}
// GetByEmail retrieves a user by email
func (r *UserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	query := `
		SELECT id, email, password, name, role, company, api_key,
			plan, track_quota, tracks_used, quota_reset_date, last_login_at
		FROM users
		WHERE email = $1 AND deleted_at IS NULL`
	user := &domain.User{}
	err := r.db.QueryRowContext(ctx, query, email).Scan(
		&user.ID, &user.Email, &user.Password, &user.Name, &user.Role,
		&user.Company, &user.APIKey, &user.Plan, &user.TrackQuota,
		&user.TracksUsed, &user.QuotaResetDate, &user.LastLoginAt,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to get user by email: %w", err)
	}
	return user, nil
}
// GetByAPIKey retrieves a user by API key
func (r *UserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	query := `
		SELECT id, email, password, name, role, company, api_key,
			plan, track_quota, tracks_used, quota_reset_date, last_login_at
		FROM users
		WHERE api_key = $1 AND deleted_at IS NULL`
	user := &domain.User{}
	err := r.db.QueryRowContext(ctx, query, apiKey).Scan(
		&user.ID, &user.Email, &user.Password, &user.Name, &user.Role,
		&user.Company, &user.APIKey, &user.Plan, &user.TrackQuota,
		&user.TracksUsed, &user.QuotaResetDate, &user.LastLoginAt,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to get user by API key: %w", err)
	}
	return user, nil
}
// Update modifies an existing user
func (r *UserRepository) Update(ctx context.Context, user *domain.User) error {
	query := `
		UPDATE users
		SET email = $1, password = $2, name = $3, role = $4,
			company = $5, api_key = $6, plan = $7, track_quota = $8,
			tracks_used = $9, quota_reset_date = $10, last_login_at = $11
		WHERE id = $12 AND deleted_at IS NULL`
	result, err := r.db.ExecContext(ctx, query,
		user.Email, user.Password, user.Name, user.Role,
		user.Company, user.APIKey, user.Plan, user.TrackQuota,
		user.TracksUsed, user.QuotaResetDate, user.LastLoginAt,
		user.ID,
	)
	if err != nil {
		return fmt.Errorf("failed to update user: %w", err)
	}
	rows, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("failed to get rows affected: %w", err)
	}
	if rows == 0 {
		return fmt.Errorf("user not found")
	}
	return nil
}
// Delete soft deletes a user
func (r *UserRepository) Delete(ctx context.Context, id string) error {
	query := `
		UPDATE users
		SET deleted_at = $1
		WHERE id = $2 AND deleted_at IS NULL`
	result, err := r.db.ExecContext(ctx, query, time.Now(), id)
	if err != nil {
		return fmt.Errorf("failed to delete user: %w", err)
	}
	rows, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("failed to get rows affected: %w", err)
	}
	if rows == 0 {
		return fmt.Errorf("user not found")
	}
	return nil
}
// List retrieves a paginated list of users
func (r *UserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	query := `
		SELECT id, email, password, name, role, company, api_key,
			plan, track_quota, tracks_used, quota_reset_date, last_login_at
		FROM users
		WHERE deleted_at IS NULL
		ORDER BY last_login_at DESC
		LIMIT $1 OFFSET $2`
	rows, err := r.db.QueryContext(ctx, query, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to list users: %w", err)
	}
	defer rows.Close()
	var users []*domain.User
	for rows.Next() {
		user := &domain.User{}
		err := rows.Scan(
			&user.ID, &user.Email, &user.Password, &user.Name, &user.Role,
			&user.Company, &user.APIKey, &user.Plan, &user.TrackQuota,
			&user.TracksUsed, &user.QuotaResetDate, &user.LastLoginAt,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan user: %w", err)
		}
		users = append(users, user)
	}
	return users, nil
}
// UpdateAPIKey updates the API key for a user
func (r *UserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	query := `UPDATE users SET api_key = $1 WHERE id = $2`
	result, err := r.db.ExecContext(ctx, query, apiKey, userID)
	if err != nil {
		return fmt.Errorf("failed to update API key: %w", err)
	}
	rowsAffected, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("failed to get rows affected: %w", err)
	}
	if rowsAffected == 0 {
		return domain.ErrUserNotFound
	}
	return nil
}
</file>

<file path="internal/repository/cached/track_repository.go">
package cached
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"time"
	"github.com/redis/go-redis/v9"
)
const (
	trackKeyPrefix = "track:"
	trackTTL       = 24 * time.Hour
)
// CachedTrackRepository implements domain.TrackRepository with Redis caching
type CachedTrackRepository struct {
	client   *redis.Client
	delegate domain.TrackRepository
}
// NewTrackRepository creates a new cached track repository
func NewTrackRepository(client *redis.Client, delegate domain.TrackRepository) domain.TrackRepository {
	return &CachedTrackRepository{
		client:   client,
		delegate: delegate,
	}
}
// Create inserts a new track and invalidates cache
func (r *CachedTrackRepository) Create(ctx context.Context, track *domain.Track) error {
	err := r.delegate.Create(ctx, track)
	if err != nil {
		return fmt.Errorf("failed to create track: %w", err)
	}
	// Invalidate any existing cache entries
	r.invalidateCache(ctx, track)
	return nil
}
// GetByID retrieves a track by ID, using cache if available
func (r *CachedTrackRepository) GetByID(ctx context.Context, id string) (*domain.Track, error) {
	// Try cache first
	key := fmt.Sprintf("%s:id:%s", trackKeyPrefix, id)
	track, err := r.getFromCache(ctx, key)
	if err == nil && track != nil {
		metrics.CacheHits.WithLabelValues("track").Inc()
		return track, nil
	}
	// Cache miss, get from delegate
	metrics.CacheMisses.WithLabelValues("track").Inc()
	track, err = r.delegate.GetByID(ctx, id)
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	if track != nil {
		if err := r.setCache(ctx, key, track); err != nil {
			// Log error but don't fail the request
			fmt.Printf("failed to cache track: %v\n", err)
		}
	}
	return track, nil
}
// SearchByMetadata searches tracks by metadata fields
func (r *CachedTrackRepository) SearchByMetadata(ctx context.Context, query map[string]interface{}) ([]*domain.Track, error) {
	// Search operations are not cached as they can be complex and varied
	return r.delegate.SearchByMetadata(ctx, query)
}
// Update modifies an existing track and invalidates cache
func (r *CachedTrackRepository) Update(ctx context.Context, track *domain.Track) error {
	err := r.delegate.Update(ctx, track)
	if err != nil {
		return fmt.Errorf("failed to update track: %w", err)
	}
	// Invalidate cache entries
	r.invalidateCache(ctx, track)
	return nil
}
// Delete removes a track and invalidates cache
func (r *CachedTrackRepository) Delete(ctx context.Context, id string) error {
	// Get track first to invalidate cache
	track, err := r.delegate.GetByID(ctx, id)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	if track != nil {
		r.invalidateCache(ctx, track)
	}
	return r.delegate.Delete(ctx, id)
}
// List retrieves tracks based on filters with pagination
func (r *CachedTrackRepository) List(ctx context.Context, filters map[string]interface{}, offset, limit int) ([]*domain.Track, error) {
	// List operations are not cached as they can be complex and varied
	return r.delegate.List(ctx, filters, offset, limit)
}
// GetByISRC retrieves a track by ISRC
func (r *CachedTrackRepository) GetByISRC(ctx context.Context, isrc string) (*domain.Track, error) {
	key := fmt.Sprintf("%sisrc:%s", trackKeyPrefix, isrc)
	// Try to get from cache first
	track, err := r.getFromCache(ctx, key)
	if err == nil {
		metrics.CacheHits.WithLabelValues("track", "isrc").Inc()
		return track, nil
	}
	// Get from delegate
	track, err = r.delegate.GetByISRC(ctx, isrc)
	if err != nil {
		return nil, err
	}
	// Store in cache
	if err := r.setCache(ctx, key, track); err != nil {
		// Log error but don't fail the request
		fmt.Printf("failed to cache track by ISRC: %v\n", err)
	}
	metrics.CacheMisses.WithLabelValues("track", "isrc").Inc()
	return track, nil
}
// BatchUpdate updates multiple tracks in a single transaction
func (r *CachedTrackRepository) BatchUpdate(ctx context.Context, tracks []*domain.Track) error {
	err := r.delegate.BatchUpdate(ctx, tracks)
	if err != nil {
		return err
	}
	// Invalidate cache for all updated tracks
	for _, track := range tracks {
		r.invalidateCache(ctx, track)
	}
	return nil
}
// Helper functions for cache operations
func (r *CachedTrackRepository) getFromCache(ctx context.Context, key string) (*domain.Track, error) {
	data, err := r.client.Get(ctx, key).Bytes()
	if err != nil {
		if err == redis.Nil {
			return nil, nil
		}
		return nil, fmt.Errorf("failed to get from cache: %w", err)
	}
	var track domain.Track
	if err := json.Unmarshal(data, &track); err != nil {
		return nil, fmt.Errorf("failed to unmarshal track: %w", err)
	}
	return &track, nil
}
func (r *CachedTrackRepository) setCache(ctx context.Context, key string, track *domain.Track) error {
	data, err := json.Marshal(track)
	if err != nil {
		return fmt.Errorf("failed to marshal track: %w", err)
	}
	if err := r.client.Set(ctx, key, data, trackTTL).Err(); err != nil {
		return fmt.Errorf("failed to set cache: %w", err)
	}
	return nil
}
func (r *CachedTrackRepository) invalidateCache(ctx context.Context, track *domain.Track) {
	keys := []string{
		fmt.Sprintf("%s:id:%s", trackKeyPrefix, track.ID),
		// Add other cache keys that need invalidation
	}
	if len(keys) > 0 {
		r.client.Del(ctx, keys...)
	}
}
</file>

<file path="internal/repository/cached/user_repository.go">
package cached
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
	"github.com/redis/go-redis/v9"
)
const (
	userKeyPrefix = "user:"
	userTTL       = 24 * time.Hour
)
// CachedUserRepository implements domain.UserRepository with Redis caching
type CachedUserRepository struct {
	client   *redis.Client
	delegate domain.UserRepository
}
// NewUserRepository creates a new cached user repository
func NewUserRepository(client *redis.Client, delegate domain.UserRepository) domain.UserRepository {
	return &CachedUserRepository{
		client:   client,
		delegate: delegate,
	}
}
// Create inserts a new user and invalidates cache
func (r *CachedUserRepository) Create(ctx context.Context, user *domain.User) error {
	err := r.delegate.Create(ctx, user)
	if err != nil {
		return err
	}
	// Invalidate any existing cache entries
	r.invalidateCache(ctx, user)
	return nil
}
// GetByID retrieves a user by ID, using cache if available
func (r *CachedUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	// Try cache first
	key := fmt.Sprintf("%s:id:%s", userKeyPrefix, id)
	user, err := r.getFromCache(ctx, key)
	if err == nil && user != nil {
		return user, nil
	}
	// Cache miss, get from delegate
	user, err = r.delegate.GetByID(ctx, id)
	if err != nil {
		return nil, err
	}
	if user != nil {
		r.setCache(ctx, key, user)
	}
	return user, nil
}
// GetByEmail retrieves a user by email, using cache if available
func (r *CachedUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	// Try cache first
	key := fmt.Sprintf("%s:email:%s", userKeyPrefix, email)
	user, err := r.getFromCache(ctx, key)
	if err == nil && user != nil {
		return user, nil
	}
	// Cache miss, get from delegate
	user, err = r.delegate.GetByEmail(ctx, email)
	if err != nil {
		return nil, err
	}
	if user != nil {
		r.setCache(ctx, key, user)
	}
	return user, nil
}
// GetByAPIKey retrieves a user by API key, using cache if available
func (r *CachedUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	// Try cache first
	key := fmt.Sprintf("%s:apikey:%s", userKeyPrefix, apiKey)
	user, err := r.getFromCache(ctx, key)
	if err == nil && user != nil {
		return user, nil
	}
	// Cache miss, get from delegate
	user, err = r.delegate.GetByAPIKey(ctx, apiKey)
	if err != nil {
		return nil, err
	}
	if user != nil {
		r.setCache(ctx, key, user)
	}
	return user, nil
}
// Update modifies an existing user and invalidates cache
func (r *CachedUserRepository) Update(ctx context.Context, user *domain.User) error {
	err := r.delegate.Update(ctx, user)
	if err != nil {
		return err
	}
	// Invalidate cache entries
	r.invalidateCache(ctx, user)
	return nil
}
// Delete removes a user and invalidates cache
func (r *CachedUserRepository) Delete(ctx context.Context, id string) error {
	// Get user first to invalidate cache
	user, err := r.delegate.GetByID(ctx, id)
	if err != nil {
		return err
	}
	if user != nil {
		r.invalidateCache(ctx, user)
	}
	return r.delegate.Delete(ctx, id)
}
// List retrieves a paginated list of users (not cached)
func (r *CachedUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	return r.delegate.List(ctx, offset, limit)
}
// UpdateAPIKey updates the API key for a user
func (r *CachedUserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	// Update in database
	if err := r.delegate.UpdateAPIKey(ctx, userID, apiKey); err != nil {
		return err
	}
	// Get user to invalidate all cache keys
	user, err := r.delegate.GetByID(ctx, userID)
	if err != nil {
		// Log error but don't fail the operation
		fmt.Printf("failed to get user for cache invalidation: %v\n", err)
		return nil
	}
	// Invalidate cache
	if err := r.invalidateCache(ctx, user); err != nil {
		// Log error but don't fail the operation
		fmt.Printf("failed to invalidate user cache: %v\n", err)
	}
	return nil
}
// Helper functions for cache operations
func (r *CachedUserRepository) getFromCache(ctx context.Context, key string) (*domain.User, error) {
	data, err := r.client.Get(ctx, key).Bytes()
	if err != nil {
		if err == redis.Nil {
			return nil, nil
		}
		return nil, err
	}
	var user domain.User
	err = json.Unmarshal(data, &user)
	if err != nil {
		return nil, err
	}
	return &user, nil
}
func (r *CachedUserRepository) setCache(ctx context.Context, key string, user *domain.User) error {
	data, err := json.Marshal(user)
	if err != nil {
		return err
	}
	return r.client.Set(ctx, key, data, userTTL).Err()
}
func (r *CachedUserRepository) invalidateCache(ctx context.Context, user *domain.User) error {
	keys := []string{
		fmt.Sprintf("%s:id:%s", userKeyPrefix, user.ID),
		fmt.Sprintf("%s:email:%s", userKeyPrefix, user.Email),
		fmt.Sprintf("%s:apikey:%s", userKeyPrefix, user.APIKey),
	}
	return r.client.Del(ctx, keys...).Err()
}
</file>

<file path="internal/repository/jobs/audio_handler.go">
package jobs
import (
	"context"
	"encoding/json"
	"fmt"
	"time"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
)
// AudioProcessPayload represents the payload for audio processing jobs
type AudioProcessPayload struct {
	TrackID     string `json:"track_id"`
	StoragePath string `json:"storage_path"`
	Format      string `json:"format,omitempty"`
}
// AudioProcessHandler handles audio processing jobs
type AudioProcessHandler struct {
	audioProcessor domain.AudioProcessor
	trackRepo      domain.TrackRepository
	storageClient  domain.StorageClient
}
// NewAudioProcessHandler creates a new audio process handler
func NewAudioProcessHandler(
	audioProcessor domain.AudioProcessor,
	trackRepo domain.TrackRepository,
	storageClient domain.StorageClient,
) *AudioProcessHandler {
	return &AudioProcessHandler{
		audioProcessor: audioProcessor,
		trackRepo:      trackRepo,
		storageClient:  storageClient,
	}
}
// JobType returns the type of job this handler processes
func (h *AudioProcessHandler) JobType() domain.JobType {
	return domain.JobTypeAudioProcess
}
// HandleJob processes an audio processing job
func (h *AudioProcessHandler) HandleJob(ctx context.Context, job *domain.Job) error {
	start := time.Now()
	defer func() {
		metrics.JobProcessingDuration.WithLabelValues(string(job.Type)).Observe(time.Since(start).Seconds())
	}()
	// Parse payload
	var payload AudioProcessPayload
	if err := json.Unmarshal(job.Payload, &payload); err != nil {
		return fmt.Errorf("failed to unmarshal payload: %w", err)
	}
	// Validate payload
	if payload.TrackID == "" || payload.StoragePath == "" {
		return fmt.Errorf("invalid payload: missing required fields")
	}
	// Get track from repository
	track, err := h.trackRepo.GetByID(ctx, payload.TrackID)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	// Get file metadata first
	metadata, err := h.storageClient.GetMetadata(ctx, payload.StoragePath)
	if err != nil {
		return fmt.Errorf("failed to get audio file metadata: %w", err)
	}
	// Download file content
	content, err := h.storageClient.Download(ctx, payload.StoragePath)
	if err != nil {
		return fmt.Errorf("failed to download audio: %w", err)
	}
	defer content.Close()
	// Convert to ProcessingAudioFile
	processingFile := &domain.ProcessingAudioFile{
		Name:    metadata.Name,
		Path:    metadata.Key,
		Size:    metadata.Size,
		Format:  domain.AudioFormat(payload.Format),
		Content: content,
	}
	// Process audio file
	result, err := h.audioProcessor.Process(ctx, processingFile, &domain.AudioProcessOptions{
		FilePath:        processingFile.Path,
		Format:          processingFile.Format,
		AnalyzeAudio:    true,
		ExtractMetadata: true,
	})
	if err != nil {
		return fmt.Errorf("failed to process audio: %w", err)
	}
	// Update track with extracted metadata
	track.SetTitle(result.Metadata.Title)
	track.SetArtist(result.Metadata.Artist)
	track.SetAlbum(result.Metadata.Album)
	track.SetYear(result.Metadata.Year)
	track.SetDuration(result.Metadata.Duration)
	track.SetBPM(result.Metadata.Musical.BPM)
	track.SetKey(result.Metadata.Musical.Key)
	track.SetISRC(result.Metadata.ISRC)
	track.SetAudioFormat(string(result.Metadata.Technical.Format))
	track.SetSampleRate(result.Metadata.Technical.SampleRate)
	track.SetBitrate(result.Metadata.Technical.Bitrate)
	track.SetChannels(result.Metadata.Technical.Channels)
	// Save updated track
	if err := h.trackRepo.Update(ctx, track); err != nil {
		return fmt.Errorf("failed to update track: %w", err)
	}
	return nil
}
</file>

<file path="internal/repository/jobs/processor.go">
package jobs
import (
	"context"
	"fmt"
	"sync"
	"time"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
)
// Processor implements domain.JobProcessor
type Processor struct {
	queue    domain.JobQueue
	config   *domain.JobConfig
	handlers map[domain.JobType]domain.JobHandler
	workers  []*worker
	wg       sync.WaitGroup
	mu       sync.RWMutex
	ctx      context.Context
	cancel   context.CancelFunc
}
// worker represents a job processing worker
type worker struct {
	id       int
	queue    domain.JobQueue
	handlers map[domain.JobType]domain.JobHandler
	wg       *sync.WaitGroup
}
// NewProcessor creates a new job processor
func NewProcessor(queue domain.JobQueue, config *domain.JobConfig) *Processor {
	ctx, cancel := context.WithCancel(context.Background())
	return &Processor{
		queue:    queue,
		config:   config,
		handlers: make(map[domain.JobType]domain.JobHandler),
		ctx:      ctx,
		cancel:   cancel,
	}
}
// RegisterHandler registers a handler for a specific job type
func (p *Processor) RegisterHandler(handler domain.JobHandler) error {
	p.mu.Lock()
	defer p.mu.Unlock()
	jobType := handler.JobType()
	if _, exists := p.handlers[jobType]; exists {
		return fmt.Errorf("handler already registered for job type: %s", jobType)
	}
	p.handlers[jobType] = handler
	return nil
}
// Start starts the job processor
func (p *Processor) Start(ctx context.Context) error {
	p.mu.Lock()
	defer p.mu.Unlock()
	if len(p.handlers) == 0 {
		return fmt.Errorf("no job handlers registered")
	}
	// Create worker pool
	p.workers = make([]*worker, p.config.NumWorkers)
	for i := 0; i < p.config.NumWorkers; i++ {
		w := &worker{
			id:       i,
			queue:    p.queue,
			handlers: p.handlers,
			wg:       &p.wg,
		}
		p.workers[i] = w
		p.wg.Add(1)
		go w.start(p.ctx)
	}
	return nil
}
// Stop stops the job processor
func (p *Processor) Stop() error {
	p.cancel()
	done := make(chan struct{})
	go func() {
		p.wg.Wait()
		close(done)
	}()
	select {
	case <-done:
		return nil
	case <-time.After(p.config.ShutdownWait):
		return fmt.Errorf("shutdown timed out after %v", p.config.ShutdownWait)
	}
}
// start starts the worker's processing loop
func (w *worker) start(ctx context.Context) {
	defer w.wg.Done()
	for {
		select {
		case <-ctx.Done():
			return
		default:
			if err := w.processNextJob(ctx); err != nil {
				// Log error but continue processing
				metrics.JobErrors.WithLabelValues("worker", "process_error").Inc()
			}
		}
	}
}
// processNextJob processes the next available job
func (w *worker) processNextJob(ctx context.Context) error {
	// Dequeue job
	job, err := w.queue.Dequeue(ctx)
	if err != nil {
		return fmt.Errorf("failed to dequeue job: %w", err)
	}
	// No job available
	if job == nil {
		time.Sleep(100 * time.Millisecond) // Prevent tight loop
		return nil
	}
	// Get handler for job type
	handler, ok := w.handlers[job.Type]
	if !ok {
		err := fmt.Errorf("no handler registered for job type: %s", job.Type)
		if err := w.queue.Fail(ctx, job.ID, err); err != nil {
			return fmt.Errorf("failed to mark job as failed: %w", err)
		}
		return err
	}
	// Process job
	if err := handler.HandleJob(ctx, job); err != nil {
		if err := w.queue.Fail(ctx, job.ID, err); err != nil {
			return fmt.Errorf("failed to mark job as failed: %w", err)
		}
		return err
	}
	// Mark job as completed
	if err := w.queue.Complete(ctx, job.ID); err != nil {
		return fmt.Errorf("failed to mark job as completed: %w", err)
	}
	return nil
}
</file>

<file path="internal/repository/jobs/redis_queue.go">
package jobs
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"time"
	"github.com/redis/go-redis/v9"
)
const (
	// Queue keys
	queueKeyPrefix     = "jobs:"
	queueKeyPending    = "pending"
	queueKeyProcessing = "processing"
	queueKeyCompleted  = "completed"
	queueKeyFailed     = "failed"
	// Hash fields
	hashFieldJob        = "job"
	hashFieldStatus     = "status"
	hashFieldProgress   = "progress"
	hashFieldError      = "error"
	hashFieldStartedAt  = "started_at"
	hashFieldRetryCount = "retry_count"
)
// RedisQueue implements domain.JobQueue using Redis
type RedisQueue struct {
	client *redis.Client
	config *domain.JobConfig
}
// NewRedisQueue creates a new Redis-backed job queue
func NewRedisQueue(client *redis.Client, config *domain.JobConfig) *RedisQueue {
	return &RedisQueue{
		client: client,
		config: config,
	}
}
// queueKey returns the Redis key for a specific queue
func (q *RedisQueue) queueKey(queueType string) string {
	return fmt.Sprintf("%s%s", q.config.QueuePrefix, queueType)
}
// jobKey returns the Redis key for a specific job
func (q *RedisQueue) jobKey(jobID string) string {
	return fmt.Sprintf("%s:job:%s", q.config.QueuePrefix, jobID)
}
// Enqueue adds a new job to the queue
func (q *RedisQueue) Enqueue(ctx context.Context, job *domain.Job) error {
	// Start pipeline
	pipe := q.client.Pipeline()
	// Serialize job
	jobData, err := json.Marshal(job)
	if err != nil {
		return fmt.Errorf("failed to marshal job: %w", err)
	}
	// Store job data in hash
	jobKey := q.jobKey(job.ID)
	pipe.HSet(ctx, jobKey, map[string]interface{}{
		hashFieldJob:    string(jobData),
		hashFieldStatus: string(job.Status),
	})
	pipe.Expire(ctx, jobKey, q.config.DefaultTTL)
	// Add to pending queue with priority score
	score := float64(time.Now().UnixNano()) - float64(job.Priority)*1e12 // Lower score = higher priority
	pipe.ZAdd(ctx, q.queueKey(queueKeyPending), redis.Z{
		Score:  score,
		Member: job.ID,
	})
	// Execute pipeline
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to enqueue job: %w", err)
	}
	// Record metrics
	metrics.JobsInQueue.WithLabelValues(string(job.Type)).Inc()
	metrics.JobStatusTransitions.WithLabelValues(string(job.Type), "", string(domain.JobStatusPending)).Inc()
	return nil
}
// Dequeue gets the next job to process
func (q *RedisQueue) Dequeue(ctx context.Context) (*domain.Job, error) {
	// Start pipeline
	pipe := q.client.Pipeline()
	// Get highest priority job from pending queue
	pendingKey := q.queueKey(queueKeyPending)
	processingKey := q.queueKey(queueKeyProcessing)
	result := pipe.ZPopMin(ctx, pendingKey, 1)
	// Execute pipeline to get job ID
	if _, err := pipe.Exec(ctx); err != nil {
		return nil, fmt.Errorf("failed to dequeue job: %w", err)
	}
	// Check if queue is empty
	popResult := result.Val()
	if len(popResult) == 0 {
		return nil, nil
	}
	jobID := popResult[0].Member.(string)
	jobKey := q.jobKey(jobID)
	// Get job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		return nil, fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return nil, fmt.Errorf("failed to unmarshal job: %w", err)
	}
	// Update job status
	now := time.Now()
	job.Status = domain.JobStatusProcessing
	job.StartedAt = &now
	// Update job in Redis
	pipe = q.client.Pipeline()
	updatedJobData, _ := json.Marshal(job)
	pipe.HSet(ctx, jobKey, map[string]interface{}{
		hashFieldJob:       string(updatedJobData),
		hashFieldStatus:    string(job.Status),
		hashFieldStartedAt: now.Format(time.RFC3339),
	})
	pipe.ZAdd(ctx, processingKey, redis.Z{
		Score:  float64(now.UnixNano()),
		Member: jobID,
	})
	// Execute pipeline
	if _, err := pipe.Exec(ctx); err != nil {
		return nil, fmt.Errorf("failed to update job status: %w", err)
	}
	// Record metrics
	metrics.JobsInQueue.WithLabelValues(string(job.Type)).Dec()
	metrics.JobStatusTransitions.WithLabelValues(string(job.Type), string(domain.JobStatusPending), string(domain.JobStatusProcessing)).Inc()
	metrics.JobQueueLatency.WithLabelValues(string(job.Type)).Observe(time.Since(job.CreatedAt).Seconds())
	return &job, nil
}
// Complete marks a job as completed
func (q *RedisQueue) Complete(ctx context.Context, jobID string) error {
	jobKey := q.jobKey(jobID)
	processingKey := q.queueKey(queueKeyProcessing)
	completedKey := q.queueKey(queueKeyCompleted)
	// Get current job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		return fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return fmt.Errorf("failed to unmarshal job: %w", err)
	}
	// Update job status
	now := time.Now()
	prevStatus := job.Status
	job.Status = domain.JobStatusCompleted
	job.CompletedAt = &now
	// Start pipeline
	pipe := q.client.Pipeline()
	// Update job data
	updatedJobData, _ := json.Marshal(job)
	pipe.HSet(ctx, jobKey, map[string]interface{}{
		hashFieldJob:    string(updatedJobData),
		hashFieldStatus: string(job.Status),
	})
	// Move from processing to completed queue
	pipe.ZRem(ctx, processingKey, jobID)
	pipe.ZAdd(ctx, completedKey, redis.Z{
		Score:  float64(now.UnixNano()),
		Member: jobID,
	})
	// Execute pipeline
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to complete job: %w", err)
	}
	// Record metrics
	metrics.JobsProcessed.WithLabelValues(string(job.Type), string(domain.JobStatusCompleted)).Inc()
	metrics.JobStatusTransitions.WithLabelValues(string(job.Type), string(prevStatus), string(domain.JobStatusCompleted)).Inc()
	if job.StartedAt != nil {
		metrics.JobProcessingDuration.WithLabelValues(string(job.Type)).Observe(time.Since(*job.StartedAt).Seconds())
	}
	return nil
}
// Fail marks a job as failed
func (q *RedisQueue) Fail(ctx context.Context, jobID string, jobErr error) error {
	jobKey := q.jobKey(jobID)
	processingKey := q.queueKey(queueKeyProcessing)
	failedKey := q.queueKey(queueKeyFailed)
	// Get current job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		return fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return fmt.Errorf("failed to unmarshal job: %w", err)
	}
	// Update job status
	now := time.Now()
	prevStatus := job.Status
	job.Status = domain.JobStatusFailed
	job.Error = jobErr.Error()
	job.RetryCount++
	// Check if should retry
	var nextRetryAt time.Time
	if job.RetryCount < job.MaxRetries {
		// Calculate next retry time with exponential backoff
		delay := q.config.RetryDelay * time.Duration(1<<uint(job.RetryCount-1))
		if delay > q.config.MaxRetryDelay {
			delay = q.config.MaxRetryDelay
		}
		nextRetryAt = now.Add(delay)
		job.NextRetryAt = &nextRetryAt
		job.Status = domain.JobStatusPending
	}
	// Start pipeline
	pipe := q.client.Pipeline()
	// Update job data
	updatedJobData, _ := json.Marshal(job)
	pipe.HSet(ctx, jobKey, map[string]interface{}{
		hashFieldJob:        string(updatedJobData),
		hashFieldStatus:     string(job.Status),
		hashFieldError:      job.Error,
		hashFieldRetryCount: job.RetryCount,
	})
	// Move from processing queue
	pipe.ZRem(ctx, processingKey, jobID)
	// Add to appropriate queue based on retry status
	if job.Status == domain.JobStatusPending {
		// Add back to pending queue with retry time as score
		pipe.ZAdd(ctx, q.queueKey(queueKeyPending), redis.Z{
			Score:  float64(nextRetryAt.UnixNano()),
			Member: jobID,
		})
	} else {
		// Add to failed queue
		pipe.ZAdd(ctx, failedKey, redis.Z{
			Score:  float64(now.UnixNano()),
			Member: jobID,
		})
	}
	// Execute pipeline
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to fail job: %w", err)
	}
	// Record metrics
	metrics.JobsProcessed.WithLabelValues(string(job.Type), string(domain.JobStatusFailed)).Inc()
	metrics.JobStatusTransitions.WithLabelValues(string(job.Type), string(prevStatus), string(job.Status)).Inc()
	metrics.JobRetries.WithLabelValues(string(job.Type)).Add(float64(job.RetryCount))
	metrics.JobErrors.WithLabelValues(string(job.Type), job.Error).Inc()
	return nil
}
// Cancel cancels a pending or running job
func (q *RedisQueue) Cancel(ctx context.Context, jobID string) error {
	jobKey := q.jobKey(jobID)
	// Get current job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		return fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return fmt.Errorf("failed to unmarshal job: %w", err)
	}
	// Can only cancel pending or processing jobs
	if job.Status != domain.JobStatusPending && job.Status != domain.JobStatusProcessing {
		return fmt.Errorf("cannot cancel job with status %s", job.Status)
	}
	prevStatus := job.Status
	job.Status = domain.JobStatusCanceled
	// Start pipeline
	pipe := q.client.Pipeline()
	// Update job data
	updatedJobData, _ := json.Marshal(job)
	pipe.HSet(ctx, jobKey, map[string]interface{}{
		hashFieldJob:    string(updatedJobData),
		hashFieldStatus: string(job.Status),
	})
	// Remove from current queue
	if prevStatus == domain.JobStatusPending {
		pipe.ZRem(ctx, q.queueKey(queueKeyPending), jobID)
	} else {
		pipe.ZRem(ctx, q.queueKey(queueKeyProcessing), jobID)
	}
	// Execute pipeline
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to cancel job: %w", err)
	}
	// Record metrics
	metrics.JobStatusTransitions.WithLabelValues(string(job.Type), string(prevStatus), string(domain.JobStatusCanceled)).Inc()
	if prevStatus == domain.JobStatusPending {
		metrics.JobsInQueue.WithLabelValues(string(job.Type)).Dec()
	}
	return nil
}
// GetStatus gets the current status of a job
func (q *RedisQueue) GetStatus(ctx context.Context, jobID string) (*domain.Job, error) {
	jobKey := q.jobKey(jobID)
	// Get job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		if err == redis.Nil {
			return nil, fmt.Errorf("job not found: %s", jobID)
		}
		return nil, fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return nil, fmt.Errorf("failed to unmarshal job: %w", err)
	}
	return &job, nil
}
// UpdateProgress updates the progress of a running job
func (q *RedisQueue) UpdateProgress(ctx context.Context, jobID string, progress int) error {
	jobKey := q.jobKey(jobID)
	// Get current job data
	jobData, err := q.client.HGet(ctx, jobKey, hashFieldJob).Result()
	if err != nil {
		return fmt.Errorf("failed to get job data: %w", err)
	}
	// Deserialize job
	var job domain.Job
	if err := json.Unmarshal([]byte(jobData), &job); err != nil {
		return fmt.Errorf("failed to unmarshal job: %w", err)
	}
	// Can only update progress of processing jobs
	if job.Status != domain.JobStatusProcessing {
		return fmt.Errorf("cannot update progress of job with status %s", job.Status)
	}
	// Update progress
	job.Progress = progress
	// Update job data
	updatedJobData, _ := json.Marshal(job)
	if err := q.client.HSet(ctx, jobKey, hashFieldJob, string(updatedJobData), hashFieldProgress, progress).Err(); err != nil {
		return fmt.Errorf("failed to update job progress: %w", err)
	}
	return nil
}
</file>

<file path="internal/repository/postgres/track_repository.go">
package postgres
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"time"
	"github.com/google/uuid"
	"gorm.io/gorm"
)
// TrackModel represents the database model for tracks
type TrackModel struct {
	ID          string `gorm:"primaryKey;type:uuid"`
	StoragePath string
	FileSize    int64
	CreatedAt   time.Time
	UpdatedAt   time.Time
	DeletedAt   *time.Time      `gorm:"index"`
	Metadata    json.RawMessage `gorm:"type:jsonb"`
}
// PostgresTrackRepository implements domain.TrackRepository
type PostgresTrackRepository struct {
	db *gorm.DB
}
// NewPostgresTrackRepository creates a new PostgreSQL track repository
func NewPostgresTrackRepository(db *gorm.DB) *PostgresTrackRepository {
	return &PostgresTrackRepository{db: db}
}
// Create inserts a new track
func (r *PostgresTrackRepository) Create(ctx context.Context, track *domain.Track) error {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_create").Observe(time.Since(start).Seconds())
	}()
	if track.ID == "" {
		track.ID = uuid.New().String()
	}
	model, err := toModel(track)
	if err != nil {
		return fmt.Errorf("failed to convert track to model: %w", err)
	}
	if err := r.db.WithContext(ctx).Create(model).Error; err != nil {
		return fmt.Errorf("failed to create track: %w", err)
	}
	return nil
}
// GetByID retrieves a track by ID
func (r *PostgresTrackRepository) GetByID(ctx context.Context, id string) (*domain.Track, error) {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_get").Observe(time.Since(start).Seconds())
	}()
	var model TrackModel
	if err := r.db.WithContext(ctx).First(&model, "id = ?", id).Error; err != nil {
		if err == gorm.ErrRecordNotFound {
			return nil, nil
		}
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	track := &domain.Track{
		ID:          model.ID,
		StoragePath: model.StoragePath,
		FileSize:    model.FileSize,
		CreatedAt:   model.CreatedAt,
		UpdatedAt:   model.UpdatedAt,
		DeletedAt:   model.DeletedAt,
	}
	if err := json.Unmarshal(model.Metadata, &track.Metadata); err != nil {
		return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
	}
	return track, nil
}
// Update modifies an existing track
func (r *PostgresTrackRepository) Update(ctx context.Context, track *domain.Track) error {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_update").Observe(time.Since(start).Seconds())
	}()
	model, err := toModel(track)
	if err != nil {
		return fmt.Errorf("failed to convert track to model: %w", err)
	}
	result := r.db.WithContext(ctx).Where("id = ? AND deleted_at IS NULL", track.ID).Updates(model)
	if result.Error != nil {
		return fmt.Errorf("failed to update track: %w", result.Error)
	}
	if result.RowsAffected == 0 {
		return fmt.Errorf("track not found: %s", track.ID)
	}
	return nil
}
// Delete soft deletes a track
func (r *PostgresTrackRepository) Delete(ctx context.Context, id string) error {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_delete").Observe(time.Since(start).Seconds())
	}()
	result := r.db.WithContext(ctx).Model(&TrackModel{}).Where("id = ? AND deleted_at IS NULL", id).Update("deleted_at", time.Now())
	if result.Error != nil {
		return fmt.Errorf("failed to delete track: %w", result.Error)
	}
	if result.RowsAffected == 0 {
		return fmt.Errorf("track not found: %s", id)
	}
	return nil
}
// List retrieves a paginated list of tracks
func (r *PostgresTrackRepository) List(ctx context.Context, offset, limit int) ([]*domain.Track, error) {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_list").Observe(time.Since(start).Seconds())
	}()
	var models []TrackModel
	result := r.db.WithContext(ctx).
		Where("deleted_at IS NULL").
		Order("created_at DESC").
		Offset(offset).
		Limit(limit).
		Find(&models)
	if result.Error != nil {
		return nil, fmt.Errorf("failed to list tracks: %w", result.Error)
	}
	tracks := make([]*domain.Track, len(models))
	for i, model := range models {
		track, err := model.toDomain()
		if err != nil {
			return nil, fmt.Errorf("failed to convert model to track: %w", err)
		}
		tracks[i] = track
	}
	return tracks, nil
}
// SearchByMetadata searches tracks by metadata fields
func (r *PostgresTrackRepository) SearchByMetadata(ctx context.Context, query map[string]interface{}) ([]*domain.Track, error) {
	start := time.Now()
	defer func() {
		metrics.DatabaseQueryDuration.WithLabelValues("track_search").Observe(time.Since(start).Seconds())
	}()
	db := r.db.WithContext(ctx).Model(&TrackModel{}).Where("deleted_at IS NULL")
	// Build query dynamically based on metadata fields
	for field, value := range query {
		switch field {
		case "title", "artist", "album", "genre", "label", "publisher":
			db = db.Where(fmt.Sprintf("%s ILIKE ?", field), fmt.Sprintf("%%%v%%", value))
		case "year", "bpm":
			db = db.Where(fmt.Sprintf("%s = ?", field), value)
		case "isrc", "iswc":
			db = db.Where(fmt.Sprintf("%s = ?", field), value)
		case "ai_confidence":
			db = db.Where("ai_confidence >= ?", value)
		case "needs_review":
			db = db.Where("needs_review = ?", value)
		case "created_after":
			if t, ok := value.(time.Time); ok {
				db = db.Where("created_at >= ?", t)
			}
		case "created_before":
			if t, ok := value.(time.Time); ok {
				db = db.Where("created_at <= ?", t)
			}
		}
	}
	var models []TrackModel
	result := db.Find(&models)
	if result.Error != nil {
		return nil, fmt.Errorf("failed to search tracks: %w", result.Error)
	}
	tracks := make([]*domain.Track, len(models))
	for i, model := range models {
		track, err := model.toDomain()
		if err != nil {
			return nil, fmt.Errorf("failed to convert model to track: %w", err)
		}
		tracks[i] = track
	}
	return tracks, nil
}
// Helper functions
func toModel(track *domain.Track) (*TrackModel, error) {
	metadata, err := json.Marshal(track.Metadata)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal metadata: %w", err)
	}
	return &TrackModel{
		ID:          track.ID,
		StoragePath: track.StoragePath,
		FileSize:    track.FileSize,
		CreatedAt:   track.CreatedAt,
		UpdatedAt:   track.UpdatedAt,
		DeletedAt:   track.DeletedAt,
		Metadata:    metadata,
	}, nil
}
func (m *TrackModel) toDomain() (*domain.Track, error) {
	var metadata domain.CompleteTrackMetadata
	if m.Metadata != nil {
		if err := json.Unmarshal(m.Metadata, &metadata); err != nil {
			return nil, fmt.Errorf("failed to unmarshal metadata: %w", err)
		}
	}
	track := &domain.Track{
		ID:          m.ID,
		StoragePath: m.StoragePath,
		FileSize:    m.FileSize,
		CreatedAt:   m.CreatedAt,
		UpdatedAt:   m.UpdatedAt,
		DeletedAt:   m.DeletedAt,
		Metadata:    metadata,
	}
	return track, nil
}
</file>

<file path="internal/repository/postgres/user_repository.go">
package postgres
import (
	"context"
	"errors"
	"metadatatool/internal/pkg/domain"
	"time"
	"gorm.io/gorm"
)
// UserModel is the GORM model for users
type UserModel struct {
	gorm.Model
	ID             string `gorm:"primaryKey"`
	Email          string `gorm:"uniqueIndex"`
	Password       string
	Name           string
	Role           string
	Company        string
	APIKey         string `gorm:"uniqueIndex"`
	Plan           string
	TrackQuota     int
	TracksUsed     int
	QuotaResetDate int64
	LastLoginAt    int64
}
// PostgresUserRepository implements domain.UserRepository
type PostgresUserRepository struct {
	db *gorm.DB
}
// NewUserRepository creates a new PostgresUserRepository
func NewUserRepository(db *gorm.DB) *PostgresUserRepository {
	return &PostgresUserRepository{db: db}
}
// Create inserts a new user into the database
func (r *PostgresUserRepository) Create(ctx context.Context, user *domain.User) error {
	model := &UserModel{
		ID:             user.ID,
		Email:          user.Email,
		Password:       user.Password,
		Name:           user.Name,
		Role:           string(user.Role),
		Company:        user.Company,
		APIKey:         user.APIKey,
		Plan:           string(user.Plan),
		TrackQuota:     user.TrackQuota,
		TracksUsed:     user.TracksUsed,
		QuotaResetDate: user.QuotaResetDate.Unix(),
		LastLoginAt:    user.LastLoginAt.Unix(),
	}
	result := r.db.WithContext(ctx).Create(model)
	if result.Error != nil {
		return result.Error
	}
	return nil
}
// GetByID retrieves a user by their ID
func (r *PostgresUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	var model UserModel
	result := r.db.WithContext(ctx).Where("id = ? AND deleted_at IS NULL", id).First(&model)
	if result.Error != nil {
		if errors.Is(result.Error, gorm.ErrRecordNotFound) {
			return nil, nil
		}
		return nil, result.Error
	}
	return modelToDomain(&model), nil
}
// GetByEmail retrieves a user by their email
func (r *PostgresUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	var model UserModel
	result := r.db.WithContext(ctx).Where("email = ? AND deleted_at IS NULL", email).First(&model)
	if result.Error != nil {
		if errors.Is(result.Error, gorm.ErrRecordNotFound) {
			return nil, nil
		}
		return nil, result.Error
	}
	return modelToDomain(&model), nil
}
// GetByAPIKey retrieves a user by their API key
func (r *PostgresUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	var model UserModel
	result := r.db.WithContext(ctx).Where("api_key = ? AND deleted_at IS NULL", apiKey).First(&model)
	if result.Error != nil {
		if errors.Is(result.Error, gorm.ErrRecordNotFound) {
			return nil, nil
		}
		return nil, result.Error
	}
	return modelToDomain(&model), nil
}
// Update modifies an existing user
func (r *PostgresUserRepository) Update(ctx context.Context, user *domain.User) error {
	model := &UserModel{
		ID:             user.ID,
		Email:          user.Email,
		Password:       user.Password,
		Name:           user.Name,
		Role:           string(user.Role),
		Company:        user.Company,
		APIKey:         user.APIKey,
		Plan:           string(user.Plan),
		TrackQuota:     user.TrackQuota,
		TracksUsed:     user.TracksUsed,
		QuotaResetDate: user.QuotaResetDate.Unix(),
		LastLoginAt:    user.LastLoginAt.Unix(),
	}
	result := r.db.WithContext(ctx).Save(model)
	if result.Error != nil {
		return result.Error
	}
	return nil
}
// Delete soft-deletes a user
func (r *PostgresUserRepository) Delete(ctx context.Context, id string) error {
	result := r.db.WithContext(ctx).Where("id = ?", id).Delete(&UserModel{})
	if result.Error != nil {
		return result.Error
	}
	return nil
}
// List retrieves a paginated list of users
func (r *PostgresUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	var models []*UserModel
	result := r.db.WithContext(ctx).
		Where("deleted_at IS NULL").
		Offset(offset).
		Limit(limit).
		Order("created_at DESC").
		Find(&models)
	if result.Error != nil {
		return nil, result.Error
	}
	users := make([]*domain.User, len(models))
	for i, model := range models {
		users[i] = modelToDomain(model)
	}
	return users, nil
}
// Helper function to convert UserModel to domain.User
func modelToDomain(model *UserModel) *domain.User {
	return &domain.User{
		ID:             model.ID,
		Email:          model.Email,
		Password:       model.Password,
		Name:           model.Name,
		Role:           domain.Role(model.Role),
		Company:        model.Company,
		APIKey:         model.APIKey,
		Plan:           domain.SubscriptionPlan(model.Plan),
		TrackQuota:     model.TrackQuota,
		TracksUsed:     model.TracksUsed,
		QuotaResetDate: time.Unix(model.QuotaResetDate, 0),
		LastLoginAt:    time.Unix(model.LastLoginAt, 0),
	}
}
</file>

<file path="internal/repository/queue/mock_service_test.go">
package queue
import (
	"context"
	"metadatatool/internal/pkg/domain"
	"testing"
	"time"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)
func TestMockQueueService(t *testing.T) {
	ctx := context.Background()
	service := NewMockQueueService()
	// Test message
	message := &domain.Message{
		ID:        "test-message",
		Type:      string(domain.QueueMessageTypeAIProcess),
		Priority:  domain.PriorityHigh,
		Data:      map[string]interface{}{"test": "data"},
		Status:    domain.MessageStatusPending,
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
	}
	t.Run("Publish and Subscribe", func(t *testing.T) {
		// Test message handler
		messageReceived := make(chan *domain.Message, 1)
		handler := func(ctx context.Context, msg *domain.Message) error {
			messageReceived <- msg
			return nil
		}
		// Subscribe to messages
		topic := "test-topic"
		err := service.Subscribe(ctx, topic, handler)
		require.NoError(t, err)
		// Publish message
		err = service.Publish(ctx, topic, message, domain.PriorityHigh)
		require.NoError(t, err)
		// Wait for message to be received
		select {
		case received := <-messageReceived:
			assert.Equal(t, message.ID, received.ID)
			assert.Equal(t, message.Type, received.Type)
			assert.Equal(t, message.Priority, received.Priority)
			assert.Equal(t, message.Data, received.Data)
		case <-time.After(time.Second):
			t.Fatal("timeout waiting for message")
		}
		// Verify message was stored
		messages := service.GetMessages(topic)
		require.Len(t, messages, 1)
		assert.Equal(t, message.ID, messages[0].ID)
	})
	t.Run("Dead Letter Queue", func(t *testing.T) {
		// Test dead letter handling
		message.ErrorMessage = "test error"
		message.RetryCount = 3
		message.Status = domain.MessageStatusFailed
		err := service.HandleDeadLetter(ctx, "test-topic", message)
		require.NoError(t, err)
		// Verify dead letter count
		assert.Equal(t, 1, service.GetDeadLetterCount())
	})
}
</file>

<file path="internal/repository/queue/mock_service.go">
package queue
import (
	"context"
	"errors"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"sync"
	"time"
)
var (
	ErrMessageNotFound = errors.New("message not found")
)
// MockQueueService implements domain.QueueService for testing
type MockQueueService struct {
	messages    map[string][]*domain.Message
	deadLetters []*domain.Message
	handlers    map[string]domain.MessageHandler
	mu          sync.RWMutex
}
// NewMockQueueService creates a new mock queue service
func NewMockQueueService() *MockQueueService {
	return &MockQueueService{
		messages: make(map[string][]*domain.Message),
		handlers: make(map[string]domain.MessageHandler),
	}
}
// Publish adds a message to the mock queue
func (s *MockQueueService) Publish(ctx context.Context, topic string, message *domain.Message, priority domain.QueuePriority) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.messages[topic] = append(s.messages[topic], message)
	if handler, ok := s.handlers[topic]; ok {
		go handler(ctx, message)
	}
	return nil
}
// Subscribe registers a handler for a topic
func (s *MockQueueService) Subscribe(ctx context.Context, topic string, handler domain.MessageHandler) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.handlers[topic] = handler
	return nil
}
// HandleDeadLetter processes messages from the dead letter queue
func (s *MockQueueService) HandleDeadLetter(ctx context.Context, topic string, message *domain.Message) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.deadLetters = append(s.deadLetters, message)
	return nil
}
// GetMessage returns a message by ID
func (s *MockQueueService) GetMessage(ctx context.Context, id string) (*domain.Message, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	for _, messages := range s.messages {
		for _, msg := range messages {
			if msg.ID == id {
				return msg, nil
			}
		}
	}
	return nil, ErrMessageNotFound
}
// GetMessages returns all messages for a topic
func (s *MockQueueService) GetMessages(topic string) []*domain.Message {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.messages[topic]
}
// GetDeadLetterCount returns the number of dead letter messages processed
func (s *MockQueueService) GetDeadLetterCount() int {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return len(s.deadLetters)
}
func (s *MockQueueService) RetryMessage(ctx context.Context, id string) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	for _, messages := range s.messages {
		for _, msg := range messages {
			if msg.ID == id {
				msg.RetryCount++
				msg.Status = domain.MessageStatusRetrying
				return nil
			}
		}
	}
	return ErrMessageNotFound
}
func (s *MockQueueService) AckMessage(ctx context.Context, id string) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	for _, messages := range s.messages {
		for _, msg := range messages {
			if msg.ID == id {
				msg.Status = domain.MessageStatusCompleted
				msg.ProcessedAt = &time.Time{}
				*msg.ProcessedAt = time.Now()
				return nil
			}
		}
	}
	return ErrMessageNotFound
}
func (s *MockQueueService) NackMessage(ctx context.Context, id string, err error) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	for _, messages := range s.messages {
		for _, msg := range messages {
			if msg.ID == id {
				msg.Status = domain.MessageStatusFailed
				msg.ErrorMessage = err.Error()
				return nil
			}
		}
	}
	return ErrMessageNotFound
}
func (s *MockQueueService) ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*domain.Message, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	if offset < 0 || limit < 0 {
		return nil, fmt.Errorf("invalid offset or limit")
	}
	var deadLetters []*domain.Message
	for _, msg := range s.deadLetters {
		if msg.Status == domain.MessageStatusDeadLetter {
			deadLetters = append(deadLetters, msg)
		}
	}
	if offset >= len(deadLetters) {
		return []*domain.Message{}, nil
	}
	end := offset + limit
	if end > len(deadLetters) {
		end = len(deadLetters)
	}
	return deadLetters[offset:end], nil
}
func (s *MockQueueService) ReplayDeadLetter(ctx context.Context, id string) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	for i, msg := range s.deadLetters {
		if msg.ID == id {
			msg.Status = domain.MessageStatusRetrying
			msg.RetryCount = 0
			msg.ErrorMessage = ""
			// Remove from dead letters
			s.deadLetters = append(s.deadLetters[:i], s.deadLetters[i+1:]...)
			return nil
		}
	}
	return ErrMessageNotFound
}
func (s *MockQueueService) PurgeDeadLetters(ctx context.Context, topic string) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.deadLetters = nil
	return nil
}
</file>

<file path="internal/repository/queue/pubsub_service_test.go">
package queue
import (
	"context"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"testing"
	"time"
	"cloud.google.com/go/pubsub"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"google.golang.org/api/option"
)
func TestPubSubService(t *testing.T) {
	ctx := context.Background()
	// Create test configuration
	config := &PubSubConfig{
		ProjectID:          "test-project",
		HighPriorityTopic:  "test-high-priority",
		LowPriorityTopic:   "test-low-priority",
		DeadLetterTopic:    "test-dead-letter",
		SubscriptionPrefix: "test-sub",
		MaxRetries:         3,
		AckDeadline:        30 * time.Second,
		RetentionDuration:  24 * time.Hour,
	}
	// Create metrics
	metrics := metrics.NewQueueMetrics()
	// Create test client with emulator
	client, err := pubsub.NewClient(ctx, "test-project", option.WithEndpoint("localhost:8085"))
	require.NoError(t, err)
	defer client.Close()
	// Create service
	service, err := NewPubSubService(ctx, config, metrics)
	require.NoError(t, err)
	defer service.Close()
	t.Run("Publish and Subscribe", func(t *testing.T) {
		// Create test message
		message := &domain.Message{
			ID:        "test-message",
			Type:      string(domain.QueueMessageTypeAIProcess),
			Priority:  domain.PriorityHigh,
			Data:      map[string]interface{}{"test": "data"},
			Status:    domain.MessageStatusPending,
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
		}
		// Create channel for received messages
		messageReceived := make(chan *domain.Message, 1)
		handler := func(ctx context.Context, msg *domain.Message) error {
			messageReceived <- msg
			return nil
		}
		// Subscribe to messages
		topic := "test-topic"
		err := service.Subscribe(ctx, topic, handler)
		require.NoError(t, err)
		// Publish message
		err = service.Publish(ctx, topic, message, domain.PriorityHigh)
		require.NoError(t, err)
		// Wait for message to be received
		select {
		case received := <-messageReceived:
			assert.Equal(t, message.ID, received.ID)
			assert.Equal(t, message.Type, received.Type)
			assert.Equal(t, message.Priority, received.Priority)
			assert.Equal(t, message.Data, received.Data)
		case <-time.After(5 * time.Second):
			t.Fatal("timeout waiting for message")
		}
	})
	t.Run("Dead Letter Queue", func(t *testing.T) {
		// Create test message
		message := &domain.Message{
			ID:           "test-message",
			Type:         string(domain.QueueMessageTypeAIProcess),
			Priority:     domain.PriorityHigh,
			Data:         map[string]interface{}{"test": "data"},
			Status:       domain.MessageStatusFailed,
			ErrorMessage: "test error",
			RetryCount:   3,
			CreatedAt:    time.Now(),
			UpdatedAt:    time.Now(),
		}
		// Handle dead letter
		err := service.HandleDeadLetter(ctx, "test-topic", message)
		require.NoError(t, err)
	})
}
</file>

<file path="internal/repository/queue/pubsub_service.go">
package queue
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"sync"
	"time"
	"cloud.google.com/go/pubsub"
)
// PubSubConfig holds configuration for Google Pub/Sub
type PubSubConfig struct {
	ProjectID          string        `env:"PUBSUB_PROJECT_ID,required"`
	HighPriorityTopic  string        `env:"PUBSUB_HIGH_PRIORITY_TOPIC" envDefault:"high-priority"`
	LowPriorityTopic   string        `env:"PUBSUB_LOW_PRIORITY_TOPIC" envDefault:"low-priority"`
	DeadLetterTopic    string        `env:"PUBSUB_DEAD_LETTER_TOPIC" envDefault:"dead-letter"`
	SubscriptionPrefix string        `env:"PUBSUB_SUBSCRIPTION_PREFIX" envDefault:"sub"`
	MaxRetries         int           `env:"PUBSUB_MAX_RETRIES" envDefault:"3"`
	AckDeadline        time.Duration `env:"PUBSUB_ACK_DEADLINE" envDefault:"30s"`
	RetentionDuration  time.Duration `env:"PUBSUB_RETENTION" envDefault:"168h"` // 7 days
}
// PubSubService implements domain.QueueService using Google Pub/Sub
type PubSubService struct {
	client   *pubsub.Client
	config   *PubSubConfig
	topics   map[string]*pubsub.Topic
	subs     map[string]*pubsub.Subscription
	metrics  *metrics.QueueMetrics
	handlers map[string]domain.MessageHandler
	mu       sync.RWMutex // protects handlers and topics
}
// NewPubSubService creates a new Google Pub/Sub service
func NewPubSubService(ctx context.Context, config *PubSubConfig, metrics *metrics.QueueMetrics) (*PubSubService, error) {
	if config == nil {
		return nil, fmt.Errorf("pubsub config is required")
	}
	// Create Pub/Sub client
	client, err := pubsub.NewClient(ctx, config.ProjectID)
	if err != nil {
		return nil, fmt.Errorf("failed to create pubsub client: %w", err)
	}
	return &PubSubService{
		client:   client,
		config:   config,
		topics:   make(map[string]*pubsub.Topic),
		subs:     make(map[string]*pubsub.Subscription),
		metrics:  metrics,
		handlers: make(map[string]domain.MessageHandler),
	}, nil
}
// getTopic returns the appropriate topic based on priority
func (s *PubSubService) getTopic(priority domain.QueuePriority) string {
	switch priority {
	case domain.PriorityHigh:
		return s.config.HighPriorityTopic
	case domain.PriorityLow:
		return s.config.LowPriorityTopic
	default:
		return s.config.LowPriorityTopic
	}
}
// Publish publishes a message to the appropriate topic
func (s *PubSubService) Publish(ctx context.Context, topic string, message *domain.Message, priority domain.QueuePriority) error {
	// Record start time for latency tracking
	start := time.Now()
	// Marshal message to JSON
	data, err := json.Marshal(message)
	if err != nil {
		s.metrics.PublishErrors.WithLabelValues(topic).Inc()
		return fmt.Errorf("failed to marshal message: %w", err)
	}
	// Get the appropriate topic
	topicName := s.getTopic(priority)
	t, err := s.ensureTopic(ctx, topicName)
	if err != nil {
		return err
	}
	defer t.Stop()
	// Set message attributes
	attrs := map[string]string{
		"priority":   priority.String(),
		"message_id": message.ID,
		"type":       string(message.Type),
	}
	// Publish message
	result := t.Publish(ctx, &pubsub.Message{
		Data:       data,
		Attributes: attrs,
	})
	// Wait for publish result
	_, err = result.Get(ctx)
	if err != nil {
		s.metrics.PublishErrors.WithLabelValues(topicName).Inc()
		return fmt.Errorf("failed to publish message: %w", err)
	}
	// Record metrics
	s.metrics.PublishLatency.WithLabelValues(topicName).Observe(time.Since(start).Seconds())
	s.metrics.MessagesPublished.WithLabelValues(topicName).Inc()
	return nil
}
// Subscribe subscribes to a topic and processes messages
func (s *PubSubService) Subscribe(ctx context.Context, topic string, handler domain.MessageHandler) error {
	if handler == nil {
		return fmt.Errorf("message handler is required")
	}
	// Register handler
	s.mu.Lock()
	s.handlers[topic] = handler
	s.mu.Unlock()
	// Get or create subscription
	sub, err := s.ensureSubscription(ctx, topic)
	if err != nil {
		return err
	}
	// Configure subscription
	sub.ReceiveSettings.MaxOutstandingMessages = 100
	sub.ReceiveSettings.NumGoroutines = 10
	// Start receiving messages
	go func() {
		err := sub.Receive(ctx, func(ctx context.Context, msg *pubsub.Message) {
			// Record start time for latency tracking
			start := time.Now()
			// Unmarshal message
			var message domain.Message
			if err := json.Unmarshal(msg.Data, &message); err != nil {
				s.metrics.ProcessingErrors.WithLabelValues(topic).Inc()
				msg.Nack()
				return
			}
			// Get handler
			s.mu.RLock()
			handler := s.handlers[topic]
			s.mu.RUnlock()
			// Process message
			if err := handler(ctx, &message); err != nil {
				s.metrics.ProcessingErrors.WithLabelValues(topic).Inc()
				msg.Nack()
				return
			}
			// Record metrics
			s.metrics.ProcessingLatency.WithLabelValues(topic).Observe(time.Since(start).Seconds())
			s.metrics.MessagesProcessed.WithLabelValues(topic).Inc()
			msg.Ack()
		})
		if err != nil {
			// Log error and increment metric
			s.metrics.SubscriptionErrors.WithLabelValues(topic).Inc()
		}
	}()
	return nil
}
// HandleDeadLetter processes messages from the dead letter queue
func (s *PubSubService) HandleDeadLetter(ctx context.Context, topic string, message *domain.Message) error {
	// Record dead letter handling
	s.metrics.DeadLetters.WithLabelValues(string(message.Type)).Inc()
	// Here you would implement your dead letter handling logic
	// For example:
	// - Log the failure
	// - Send notifications
	// - Store for manual review
	// - Attempt special processing
	return nil
}
// Close closes the Pub/Sub client
func (s *PubSubService) Close() error {
	return s.client.Close()
}
func (s *PubSubService) ensureTopic(ctx context.Context, name string) (*pubsub.Topic, error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	if t, ok := s.topics[name]; ok {
		return t, nil
	}
	t := s.client.Topic(name)
	exists, err := t.Exists(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to check topic existence: %w", err)
	}
	if !exists {
		t, err = s.client.CreateTopic(ctx, name)
		if err != nil {
			return nil, fmt.Errorf("failed to create topic: %w", err)
		}
	}
	s.topics[name] = t
	return t, nil
}
func (s *PubSubService) ensureSubscription(ctx context.Context, topic string) (*pubsub.Subscription, error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	if sub, ok := s.subs[topic]; ok {
		return sub, nil
	}
	t, err := s.ensureTopic(ctx, topic)
	if err != nil {
		return nil, err
	}
	subName := fmt.Sprintf("%s-sub", topic)
	sub := s.client.Subscription(subName)
	exists, err := sub.Exists(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to check subscription existence: %w", err)
	}
	if !exists {
		// Create subscription with configuration
		sub, err = s.client.CreateSubscription(ctx, subName, pubsub.SubscriptionConfig{
			Topic:             t,
			AckDeadline:       s.config.AckDeadline,
			RetentionDuration: s.config.RetentionDuration,
			RetryPolicy: &pubsub.RetryPolicy{
				MaximumBackoff: time.Minute,
				MinimumBackoff: time.Second,
			},
			DeadLetterPolicy: &pubsub.DeadLetterPolicy{
				DeadLetterTopic:     s.client.Topic(s.config.DeadLetterTopic).String(),
				MaxDeliveryAttempts: s.config.MaxRetries,
			},
		})
		if err != nil {
			return nil, fmt.Errorf("failed to create subscription: %w", err)
		}
	}
	s.subs[topic] = sub
	return sub, nil
}
func (s *PubSubService) GetMessage(ctx context.Context, id string) (*domain.Message, error) {
	// Not implemented for PubSub as it doesn't support direct message retrieval
	return nil, fmt.Errorf("get message not supported for PubSub")
}
func (s *PubSubService) RetryMessage(ctx context.Context, id string) error {
	// Not implemented for PubSub as it handles retries automatically
	return fmt.Errorf("retry message not supported for PubSub")
}
func (s *PubSubService) AckMessage(ctx context.Context, id string) error {
	// Not implemented for PubSub as acks are handled in the message handler
	return fmt.Errorf("ack message not supported for PubSub")
}
func (s *PubSubService) NackMessage(ctx context.Context, id string, err error) error {
	// Not implemented for PubSub as nacks are handled in the message handler
	return fmt.Errorf("nack message not supported for PubSub")
}
func (s *PubSubService) ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*domain.Message, error) {
	// Not implemented for PubSub as it doesn't support listing dead letters
	return nil, fmt.Errorf("list dead letters not supported for PubSub")
}
func (s *PubSubService) ReplayDeadLetter(ctx context.Context, id string) error {
	// Not implemented for PubSub as it doesn't support replaying dead letters
	return fmt.Errorf("replay dead letter not supported for PubSub")
}
func (s *PubSubService) PurgeDeadLetters(ctx context.Context, topic string) error {
	// Not implemented for PubSub as it doesn't support purging dead letters
	return fmt.Errorf("purge dead letters not supported for PubSub")
}
</file>

<file path="internal/repository/queue/redis_queue_test.go">
package queue
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"testing"
	"time"
	"github.com/redis/go-redis/v9"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)
const (
	topicPrefix   = "test_topic_"
	messagePrefix = "test_msg_"
)
func setupTestRedis(t *testing.T) (*redis.Client, func()) {
	client := redis.NewClient(&redis.Options{
		Addr: "localhost:6379",
		DB:   1, // Use a different DB for testing
	})
	// Verify connection
	_, err := client.Ping(context.Background()).Result()
	require.NoError(t, err)
	// Return client and cleanup function
	return client, func() {
		client.FlushDB(context.Background())
		client.Close()
	}
}
func setupTestQueue(t *testing.T) (*RedisQueue, func()) {
	client, cleanup := setupTestRedis(t)
	config := domain.QueueConfig{
		RetryDelays:       []int{1, 5, 15, 30, 60},
		DefaultMaxRetries: 3,
		DeadLetterTTL:     24 * time.Hour,
		ProcessingTimeout: 30 * time.Second,
		BatchSize:         10,
		PollInterval:      time.Second,
	}
	queue := NewRedisQueue(client, config)
	return queue, cleanup
}
func TestRedisQueue_PublishAndSubscribe(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	messageData := map[string]interface{}{
		"payload": "test message",
	}
	// Subscribe to topic
	msgChan := make(chan *domain.Message, 1)
	err := queue.Subscribe(ctx, topic, func(ctx context.Context, msg *domain.Message) error {
		msgChan <- msg
		return nil
	})
	require.NoError(t, err)
	// Publish message
	messageBytes, err := json.Marshal(messageData)
	require.NoError(t, err)
	err = queue.Publish(ctx, topic, messageBytes)
	require.NoError(t, err)
	// Wait for message
	select {
	case msg := <-msgChan:
		assert.Equal(t, messageData, msg.Data)
		assert.Equal(t, domain.MessageStatusProcessing, msg.Status)
	case <-time.After(5 * time.Second):
		t.Fatal("timeout waiting for message")
	}
}
func TestRedisQueue_RetryAndDeadLetter(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	messageData := map[string]interface{}{
		"payload": "test message",
	}
	// Publish message
	messageBytes, err := json.Marshal(messageData)
	require.NoError(t, err)
	err = queue.Publish(ctx, topic, messageBytes)
	require.NoError(t, err)
	// Wait for retries and dead letter
	time.Sleep(2 * time.Second)
	// Check dead letter queue
	messages, err := queue.ListDeadLetters(ctx, topic, 0, 10)
	require.NoError(t, err)
	require.Len(t, messages, 1)
}
func TestRedisQueue_MessageLifecycle(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	messageData := map[string]interface{}{
		"payload": "test message",
	}
	// Publish message
	messageBytes, err := json.Marshal(messageData)
	require.NoError(t, err)
	err = queue.Publish(ctx, topic, messageBytes)
	require.NoError(t, err)
	// Get message from queue
	messages, err := queue.client.LRange(ctx, topicPrefix+topic, 0, -1).Result()
	require.NoError(t, err)
	require.Len(t, messages, 1)
	msgID := messages[0]
	msg, err := queue.GetMessage(ctx, msgID)
	require.NoError(t, err)
	assert.Equal(t, messageData, msg.Data)
	// Acknowledge message
	err = queue.AckMessage(ctx, msgID)
	require.NoError(t, err)
	// Verify message status
	msg, err = queue.GetMessage(ctx, msgID)
	require.NoError(t, err)
	assert.Equal(t, domain.MessageStatusCompleted, msg.Status)
}
func TestRedisQueue_DeadLetterOperations(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	now := time.Now()
	// Create test messages
	for i := 0; i < 5; i++ {
		msg := &domain.Message{
			ID:   fmt.Sprintf("test-msg-%d", i),
			Type: topic,
			Data: map[string]interface{}{
				"payload": fmt.Sprintf("test message %d", i),
			},
			Status:    domain.MessageStatusDeadLetter,
			CreatedAt: now,
			UpdatedAt: now,
		}
		// Add messages to dead letter queue
		err := queue.moveToDeadLetter(ctx, msg)
		require.NoError(t, err)
	}
	// Test listing dead letters
	messages, err := queue.ListDeadLetters(ctx, topic, 0, 10)
	require.NoError(t, err)
	require.Len(t, messages, 5)
}
func TestRedisQueue_Cleanup(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	processingLockDuration := 5 * time.Minute
	now := time.Now().Add(-processingLockDuration * 2)
	// Create a test message that's expired
	msg := &domain.Message{
		ID:   "test-msg",
		Type: topic,
		Data: map[string]interface{}{
			"payload": "test message",
		},
		Status:    domain.MessageStatusProcessing,
		CreatedAt: now,
		UpdatedAt: now,
	}
	// Store the full message in Redis
	messageKey := fmt.Sprintf("message:%s", msg.ID)
	messageBytes, err := json.Marshal(msg)
	require.NoError(t, err)
	err = queue.client.Set(ctx, messageKey, messageBytes, 0).Err()
	require.NoError(t, err)
	// Add message to processing set
	err = queue.client.Set(ctx, fmt.Sprintf("%s%s", processingPrefix, msg.ID), "test", processingLockDuration).Err()
	require.NoError(t, err)
	// Run cleanup
	queue.cleanupExpired()
	// Verify message was cleaned up from processing set
	exists, err := queue.client.Exists(ctx, fmt.Sprintf("%s%s", processingPrefix, msg.ID)).Result()
	require.NoError(t, err)
	assert.Equal(t, int64(0), exists)
	// Verify message data is still intact
	storedBytes, err := queue.client.Get(ctx, messageKey).Bytes()
	require.NoError(t, err)
	var storedMsg domain.Message
	err = json.Unmarshal(storedBytes, &storedMsg)
	require.NoError(t, err)
	assert.Equal(t, msg.ID, storedMsg.ID)
	assert.Equal(t, msg.Type, storedMsg.Type)
	assert.Equal(t, msg.Data, storedMsg.Data)
	assert.Equal(t, msg.Status, storedMsg.Status)
	assert.Equal(t, msg.CreatedAt.Unix(), storedMsg.CreatedAt.Unix())
	assert.Equal(t, msg.UpdatedAt.Unix(), storedMsg.UpdatedAt.Unix())
}
func TestRedisQueue_Concurrency(t *testing.T) {
	queue, cleanup := setupTestQueue(t)
	defer cleanup()
	ctx := context.Background()
	topic := "test-topic"
	messageCount := 100
	// Channel to track processed messages
	processed := make(chan *domain.Message, messageCount)
	// Subscribe to topic
	err := queue.Subscribe(ctx, topic, func(ctx context.Context, msg *domain.Message) error {
		processed <- msg
		return nil
	})
	require.NoError(t, err)
	// Publish messages concurrently
	for i := 0; i < messageCount; i++ {
		go func(i int) {
			data := map[string]interface{}{
				"payload": fmt.Sprintf("message-%d", i),
			}
			messageBytes, err := json.Marshal(data)
			require.NoError(t, err)
			err = queue.Publish(ctx, topic, messageBytes)
			require.NoError(t, err)
		}(i)
	}
	// Wait for all messages to be processed
	receivedMessages := make(map[string]bool)
	timeout := time.After(10 * time.Second)
	for i := 0; i < messageCount; i++ {
		select {
		case msg := <-processed:
			payload, ok := msg.Data["payload"].(string)
			require.True(t, ok)
			receivedMessages[payload] = true
		case <-timeout:
			t.Fatal("timeout waiting for messages")
		}
	}
	assert.Len(t, receivedMessages, messageCount)
}
</file>

<file path="internal/repository/queue/redis_queue.go">
package queue
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"sync"
	"time"
	"github.com/google/uuid"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/redis/go-redis/v9"
)
const (
	keyPrefix        = "queue:"
	pendingPrefix    = "pending:"
	processingPrefix = "processing:"
	deadLetterPrefix = "dead_letter:"
	// Lock duration for message processing
	processingLockDuration = 5 * time.Minute
)
// RedisQueue implements domain.QueueService using Redis
type RedisQueue struct {
	client    *redis.Client
	config    domain.QueueConfig
	handlers  map[string]domain.MessageHandler
	mu        sync.RWMutex
	done      chan struct{}
	closeOnce sync.Once
}
// NewRedisQueue creates a new Redis-based queue service
func NewRedisQueue(client *redis.Client, config domain.QueueConfig) *RedisQueue {
	q := &RedisQueue{
		client:   client,
		config:   config,
		handlers: make(map[string]domain.MessageHandler),
		done:     make(chan struct{}),
	}
	// Start background workers
	go q.processMessages()
	go q.cleanupExpired()
	return q
}
// Publish publishes a message to a topic
func (q *RedisQueue) Publish(ctx context.Context, topic string, data []byte) error {
	timer := prometheus.NewTimer(metrics.MessageProcessingDuration.WithLabelValues(topic))
	defer timer.ObserveDuration()
	// Unmarshal data into map
	var dataMap map[string]interface{}
	if err := json.Unmarshal(data, &dataMap); err != nil {
		metrics.QueueOperations.WithLabelValues("publish", topic, "failure").Inc()
		return fmt.Errorf("failed to unmarshal message data: %w", err)
	}
	msg := &domain.Message{
		ID:        uuid.NewString(),
		Type:      topic,
		Data:      dataMap,
		Status:    domain.MessageStatusPending,
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
	}
	// Marshal message data
	msgBytes, err := json.Marshal(msg)
	if err != nil {
		metrics.QueueOperations.WithLabelValues("publish", topic, "failure").Inc()
		return fmt.Errorf("failed to marshal message: %w", err)
	}
	// Store in Redis
	pipe := q.client.Pipeline()
	pipe.Set(ctx, processingPrefix+msg.ID, msgBytes, 0)
	pipe.LPush(ctx, keyPrefix+msg.Type, msg.ID)
	pipe.IncrBy(ctx, keyPrefix+msg.Type+":size", 1)
	if _, err := pipe.Exec(ctx); err != nil {
		metrics.QueueOperations.WithLabelValues("publish", topic, "failure").Inc()
		return fmt.Errorf("failed to store message: %w", err)
	}
	metrics.QueueOperations.WithLabelValues("publish", topic, "success").Inc()
	metrics.QueueSize.WithLabelValues(topic).Inc()
	return nil
}
// Subscribe subscribes to a topic with a message handler
func (q *RedisQueue) Subscribe(ctx context.Context, topic string, handler domain.MessageHandler) error {
	q.mu.Lock()
	defer q.mu.Unlock()
	if _, exists := q.handlers[topic]; exists {
		return fmt.Errorf("handler already registered for topic: %s", topic)
	}
	q.handlers[topic] = handler
	metrics.QueueOperations.WithLabelValues("subscribe", topic, "success").Inc()
	return nil
}
// Unsubscribe removes a subscription from a topic
func (q *RedisQueue) Unsubscribe(ctx context.Context, topic string) error {
	q.mu.Lock()
	defer q.mu.Unlock()
	if _, exists := q.handlers[topic]; !exists {
		return fmt.Errorf("no handler registered for topic: %s", topic)
	}
	delete(q.handlers, topic)
	metrics.QueueOperations.WithLabelValues("unsubscribe", topic, "success").Inc()
	return nil
}
// GetMessage retrieves a message by ID
func (q *RedisQueue) GetMessage(ctx context.Context, id string) (*domain.Message, error) {
	data, err := q.client.Get(ctx, processingPrefix+id).Bytes()
	if err != nil {
		if err == redis.Nil {
			return nil, nil
		}
		return nil, fmt.Errorf("failed to get message: %w", err)
	}
	var msg domain.Message
	if err := json.Unmarshal(data, &msg); err != nil {
		return nil, fmt.Errorf("failed to unmarshal message: %w", err)
	}
	return &msg, nil
}
// RetryMessage marks a message for retry
func (q *RedisQueue) RetryMessage(ctx context.Context, id string) error {
	msg, err := q.GetMessage(ctx, id)
	if err != nil {
		return err
	}
	if msg == nil {
		return fmt.Errorf("message not found: %s", id)
	}
	if msg.RetryCount >= msg.MaxRetries {
		return q.moveToDeadLetter(ctx, msg)
	}
	msg.RetryCount++
	msg.Status = domain.MessageStatusRetrying
	msg.UpdatedAt = time.Now()
	msg.NextRetryAt = q.calculateNextRetry(msg.RetryCount)
	// Store updated message
	msgBytes, _ := json.Marshal(msg)
	pipe := q.client.Pipeline()
	pipe.Set(ctx, processingPrefix+msg.ID, msgBytes, 0)
	pipe.LPush(ctx, keyPrefix+msg.Type, msg.ID)
	pipe.IncrBy(ctx, keyPrefix+msg.Type+":size", 1)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to retry message: %w", err)
	}
	metrics.MessageRetries.WithLabelValues(msg.Type).Inc()
	return nil
}
// AckMessage acknowledges a message as processed
func (q *RedisQueue) AckMessage(ctx context.Context, id string) error {
	msg, err := q.GetMessage(ctx, id)
	if err != nil {
		return err
	}
	if msg == nil {
		return fmt.Errorf("message not found: %s", id)
	}
	now := time.Now()
	msg.Status = domain.MessageStatusCompleted
	msg.ProcessedAt = &now
	msg.UpdatedAt = now
	// Store updated message and clean up
	msgBytes, _ := json.Marshal(msg)
	pipe := q.client.Pipeline()
	pipe.Set(ctx, processingPrefix+msg.ID, msgBytes, 0)
	pipe.Del(ctx, processingPrefix+msg.ID)
	pipe.DecrBy(ctx, keyPrefix+msg.Type+":size", 1)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to acknowledge message: %w", err)
	}
	metrics.QueueOperations.WithLabelValues("ack", msg.Type, "success").Inc()
	metrics.QueueSize.WithLabelValues(msg.Type).Dec()
	return nil
}
// NackMessage marks a message as failed
func (q *RedisQueue) NackMessage(ctx context.Context, id string, err error) error {
	msg, getErr := q.GetMessage(ctx, id)
	if getErr != nil {
		return getErr
	}
	if msg == nil {
		return fmt.Errorf("message not found: %s", id)
	}
	msg.Status = domain.MessageStatusFailed
	msg.UpdatedAt = time.Now()
	msg.ErrorMessage = err.Error()
	if msg.RetryCount < msg.MaxRetries {
		return q.RetryMessage(ctx, id)
	}
	return q.moveToDeadLetter(ctx, msg)
}
// ListDeadLetters retrieves messages in the dead letter queue
func (q *RedisQueue) ListDeadLetters(ctx context.Context, topic string, offset, limit int) ([]*domain.Message, error) {
	ids, err := q.client.LRange(ctx, deadLetterPrefix+topic, int64(offset), int64(offset+limit-1)).Result()
	if err != nil {
		return nil, fmt.Errorf("failed to list dead letters: %w", err)
	}
	messages := make([]*domain.Message, 0, len(ids))
	for _, id := range ids {
		if msg, err := q.GetMessage(ctx, id); err == nil && msg != nil {
			messages = append(messages, msg)
		}
	}
	return messages, nil
}
// ReplayDeadLetter moves a message from dead letter queue back to main queue
func (q *RedisQueue) ReplayDeadLetter(ctx context.Context, id string) error {
	msg, err := q.GetMessage(ctx, id)
	if err != nil {
		return err
	}
	if msg == nil {
		return fmt.Errorf("message not found: %s", id)
	}
	msg.Status = domain.MessageStatusPending
	msg.RetryCount = 0
	msg.UpdatedAt = time.Now()
	msg.NextRetryAt = nil
	msg.ProcessedAt = nil
	msg.DeadLetterAt = nil
	// Move message back to main queue
	msgBytes, _ := json.Marshal(msg)
	pipe := q.client.Pipeline()
	pipe.Set(ctx, processingPrefix+msg.ID, msgBytes, 0)
	pipe.LRem(ctx, deadLetterPrefix+msg.Type, 0, msg.ID)
	pipe.LPush(ctx, keyPrefix+msg.Type, msg.ID)
	pipe.IncrBy(ctx, keyPrefix+msg.Type+":size", 1)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to replay dead letter: %w", err)
	}
	metrics.DeadLetterMessages.WithLabelValues(msg.Type).Dec()
	metrics.QueueSize.WithLabelValues(msg.Type).Inc()
	return nil
}
// PurgeDeadLetters removes all messages from dead letter queue
func (q *RedisQueue) PurgeDeadLetters(ctx context.Context, topic string) error {
	pipe := q.client.Pipeline()
	pipe.Del(ctx, deadLetterPrefix+topic)
	pipe.Set(ctx, deadLetterPrefix+topic+":size", 0, 0)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to purge dead letters: %w", err)
	}
	metrics.DeadLetterMessages.WithLabelValues(topic).Set(0)
	return nil
}
// Close closes the queue service
func (q *RedisQueue) Close() error {
	q.closeOnce.Do(func() {
		close(q.done)
	})
	return nil
}
// Helper methods
func (q *RedisQueue) processMessages() {
	ticker := time.NewTicker(q.config.PollInterval)
	defer ticker.Stop()
	for {
		select {
		case <-q.done:
			return
		case <-ticker.C:
			q.processBatch()
		}
	}
}
func (q *RedisQueue) processBatch() {
	ctx := context.Background()
	q.mu.RLock()
	topics := make([]string, 0, len(q.handlers))
	for topic := range q.handlers {
		topics = append(topics, topic)
	}
	q.mu.RUnlock()
	for _, topic := range topics {
		for i := 0; i < q.config.BatchSize; i++ {
			id, err := q.client.RPopLPush(ctx, keyPrefix+topic, processingPrefix+topic).Result()
			if err == redis.Nil {
				break
			}
			if err != nil {
				metrics.ProcessingErrors.WithLabelValues(topic, "redis_error").Inc()
				continue
			}
			go q.processMessage(ctx, id, topic)
		}
	}
}
func (q *RedisQueue) processMessage(ctx context.Context, id, topic string) {
	start := time.Now()
	msg, err := q.GetMessage(ctx, id)
	if err != nil {
		metrics.ProcessingErrors.WithLabelValues(topic, "get_message_error").Inc()
		return
	}
	q.mu.RLock()
	handler := q.handlers[topic]
	q.mu.RUnlock()
	if handler == nil {
		metrics.ProcessingErrors.WithLabelValues(topic, "no_handler").Inc()
		return
	}
	// Set processing status
	msg.Status = domain.MessageStatusProcessing
	msg.UpdatedAt = time.Now()
	msgBytes, _ := json.Marshal(msg)
	q.client.Set(ctx, processingPrefix+id, msgBytes, 0)
	// Create processing context with timeout
	ctx, cancel := context.WithTimeout(ctx, q.config.ProcessingTimeout)
	defer cancel()
	// Process message
	err = handler(ctx, msg)
	duration := time.Since(start)
	metrics.MessageProcessingDuration.WithLabelValues(topic).Observe(duration.Seconds())
	if err != nil {
		metrics.ProcessingErrors.WithLabelValues(topic, "handler_error").Inc()
		q.NackMessage(ctx, id, err)
	} else {
		q.AckMessage(ctx, id)
	}
}
func (q *RedisQueue) moveToDeadLetter(ctx context.Context, msg *domain.Message) error {
	now := time.Now()
	msg.Status = domain.MessageStatusDeadLetter
	msg.UpdatedAt = now
	msg.DeadLetterAt = &now
	// Move message to dead letter queue
	msgBytes, _ := json.Marshal(msg)
	pipe := q.client.Pipeline()
	pipe.Set(ctx, processingPrefix+msg.ID, msgBytes, 0)
	pipe.LPush(ctx, deadLetterPrefix+msg.Type, msg.ID)
	pipe.DecrBy(ctx, keyPrefix+msg.Type+":size", 1)
	pipe.IncrBy(ctx, deadLetterPrefix+msg.Type+":size", 1)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to move to dead letter: %w", err)
	}
	metrics.DeadLetterMessages.WithLabelValues(msg.Type).Inc()
	metrics.QueueSize.WithLabelValues(msg.Type).Dec()
	return nil
}
func (q *RedisQueue) calculateNextRetry(retryCount int) *time.Time {
	if retryCount <= 0 || retryCount > len(q.config.RetryDelays) {
		return nil
	}
	delay := time.Duration(q.config.RetryDelays[retryCount-1]) * time.Second
	next := time.Now().Add(delay)
	return &next
}
func (q *RedisQueue) cleanupExpired() {
	ticker := time.NewTicker(time.Hour)
	defer ticker.Stop()
	for {
		select {
		case <-q.done:
			return
		case <-ticker.C:
			ctx := context.Background()
			q.mu.RLock()
			topics := make([]string, 0, len(q.handlers))
			for topic := range q.handlers {
				topics = append(topics, topic)
			}
			q.mu.RUnlock()
			for _, topic := range topics {
				q.cleanupExpiredForTopic(ctx, topic)
			}
		}
	}
}
func (q *RedisQueue) cleanupExpiredForTopic(ctx context.Context, topic string) {
	// Cleanup expired processing messages
	ids, _ := q.client.LRange(ctx, processingPrefix+topic, 0, -1).Result()
	for _, id := range ids {
		msg, err := q.GetMessage(ctx, id)
		if err != nil || msg == nil {
			continue
		}
		if time.Since(msg.UpdatedAt) > processingLockDuration {
			q.RetryMessage(ctx, id)
		}
	}
	// Cleanup expired dead letter messages
	if q.config.DeadLetterTTL > 0 {
		ids, _ = q.client.LRange(ctx, deadLetterPrefix+topic, 0, -1).Result()
		for _, id := range ids {
			msg, err := q.GetMessage(ctx, id)
			if err != nil || msg == nil {
				continue
			}
			if msg.DeadLetterAt != nil && time.Since(*msg.DeadLetterAt) > q.config.DeadLetterTTL {
				pipe := q.client.Pipeline()
				pipe.Del(ctx, processingPrefix+id)
				pipe.LRem(ctx, deadLetterPrefix+topic, 0, id)
				pipe.DecrBy(ctx, deadLetterPrefix+topic+":size", 1)
				pipe.Exec(ctx)
				metrics.DeadLetterMessages.WithLabelValues(topic).Dec()
			}
		}
	}
}
</file>

<file path="internal/repository/redis/session_store_test.go">
package redis
import (
	"context"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"testing"
	"time"
	"github.com/alicebob/miniredis/v2"
	"github.com/google/uuid"
	"github.com/redis/go-redis/v9"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)
func setupTestRedis(t *testing.T) (*redis.Client, func()) {
	mr, err := miniredis.Run()
	require.NoError(t, err)
	client := redis.NewClient(&redis.Options{
		Addr: mr.Addr(),
	})
	return client, func() {
		client.Close()
		mr.Close()
	}
}
func createTestSession() *domain.Session {
	now := time.Now()
	return &domain.Session{
		ID:         uuid.New().String(),
		UserID:     uuid.New().String(),
		Token:      uuid.New().String(),
		UserAgent:  "test-agent",
		IP:         "127.0.0.1",
		ExpiresAt:  now.Add(24 * time.Hour),
		CreatedAt:  now,
		LastSeenAt: now,
	}
}
func TestRedisSessionStore_Create(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 2,
	})
	t.Run("successful creation", func(t *testing.T) {
		session := createTestSession()
		err := store.Create(context.Background(), session)
		require.NoError(t, err)
		// Verify session was stored
		stored, err := store.Get(context.Background(), session.ID)
		require.NoError(t, err)
		assert.Equal(t, session.ID, stored.ID)
		assert.Equal(t, session.UserID, stored.UserID)
	})
	t.Run("max sessions limit", func(t *testing.T) {
		userID := uuid.New().String()
		// Create first session
		session1 := createTestSession()
		session1.UserID = userID
		err := store.Create(context.Background(), session1)
		require.NoError(t, err)
		// Create second session
		session2 := createTestSession()
		session2.UserID = userID
		err = store.Create(context.Background(), session2)
		require.NoError(t, err)
		// Try to create third session
		session3 := createTestSession()
		session3.UserID = userID
		err = store.Create(context.Background(), session3)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "maximum number of sessions reached")
	})
}
func TestRedisSessionStore_Get(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 2,
	})
	t.Run("get existing session", func(t *testing.T) {
		session := createTestSession()
		err := store.Create(context.Background(), session)
		require.NoError(t, err)
		stored, err := store.Get(context.Background(), session.ID)
		require.NoError(t, err)
		assert.Equal(t, session.ID, stored.ID)
		assert.Equal(t, session.UserID, stored.UserID)
		assert.Equal(t, session.Token, stored.Token)
	})
	t.Run("get non-existent session", func(t *testing.T) {
		_, err := store.Get(context.Background(), "non-existent")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "session not found")
	})
}
func TestRedisSessionStore_GetUserSessions(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 5,
	})
	t.Run("get multiple user sessions", func(t *testing.T) {
		userID := uuid.New().String()
		// Create two sessions for the same user
		session1 := createTestSession()
		session1.UserID = userID
		session2 := createTestSession()
		session2.UserID = userID
		require.NoError(t, store.Create(context.Background(), session1))
		require.NoError(t, store.Create(context.Background(), session2))
		// Get user sessions
		sessions, err := store.GetUserSessions(context.Background(), userID)
		require.NoError(t, err)
		assert.Len(t, sessions, 2)
		// Verify session IDs
		sessionIDs := map[string]bool{
			sessions[0].ID: true,
			sessions[1].ID: true,
		}
		assert.True(t, sessionIDs[session1.ID])
		assert.True(t, sessionIDs[session2.ID])
	})
	t.Run("get sessions for user with no sessions", func(t *testing.T) {
		sessions, err := store.GetUserSessions(context.Background(), "non-existent")
		require.NoError(t, err)
		assert.Empty(t, sessions)
	})
}
func TestRedisSessionStore_Update(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 2,
	})
	t.Run("update existing session", func(t *testing.T) {
		session := createTestSession()
		err := store.Create(context.Background(), session)
		require.NoError(t, err)
		// Update session
		newIP := "192.168.1.1"
		session.IP = newIP
		err = store.Update(context.Background(), session)
		require.NoError(t, err)
		// Verify update
		stored, err := store.Get(context.Background(), session.ID)
		require.NoError(t, err)
		assert.Equal(t, newIP, stored.IP)
	})
}
func TestRedisSessionStore_Delete(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 2,
	})
	t.Run("delete existing session", func(t *testing.T) {
		session := createTestSession()
		err := store.Create(context.Background(), session)
		require.NoError(t, err)
		// Delete session
		err = store.Delete(context.Background(), session.ID)
		require.NoError(t, err)
		// Verify deletion
		_, err = store.Get(context.Background(), session.ID)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "session not found")
	})
	t.Run("delete non-existent session", func(t *testing.T) {
		err := store.Delete(context.Background(), "non-existent")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "session not found")
	})
}
func TestRedisSessionStore_DeleteUserSessions(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 5,
	})
	t.Run("delete all user sessions", func(t *testing.T) {
		userID := uuid.New().String()
		// Create multiple sessions
		session1 := createTestSession()
		session1.UserID = userID
		session2 := createTestSession()
		session2.UserID = userID
		require.NoError(t, store.Create(context.Background(), session1))
		require.NoError(t, store.Create(context.Background(), session2))
		// Delete all user sessions
		err := store.DeleteUserSessions(context.Background(), userID)
		require.NoError(t, err)
		// Verify deletion
		sessions, err := store.GetUserSessions(context.Background(), userID)
		require.NoError(t, err)
		assert.Empty(t, sessions)
	})
}
func TestRedisSessionStore_Touch(t *testing.T) {
	client, cleanup := setupTestRedis(t)
	defer cleanup()
	store := NewSessionStore(client, config.SessionConfig{
		CookieName:         "session",
		CookieDomain:       "",
		CookiePath:         "/",
		CookieSecure:       true,
		CookieHTTPOnly:     true,
		CookieSameSite:     "lax",
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 2,
	})
	t.Run("touch existing session", func(t *testing.T) {
		session := createTestSession()
		err := store.Create(context.Background(), session)
		require.NoError(t, err)
		// Record the original last seen time
		originalLastSeen := session.LastSeenAt
		// Wait a moment to ensure time difference
		time.Sleep(time.Millisecond * 100)
		// Touch session
		err = store.Touch(context.Background(), session.ID)
		require.NoError(t, err)
		// Verify last seen time was updated
		stored, err := store.Get(context.Background(), session.ID)
		require.NoError(t, err)
		assert.True(t, stored.LastSeenAt.After(originalLastSeen))
	})
	t.Run("touch non-existent session", func(t *testing.T) {
		err := store.Touch(context.Background(), "non-existent")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "session not found")
	})
}
</file>

<file path="internal/repository/redis/session_store.go">
package redis
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"sync"
	"time"
	pkgconfig "metadatatool/internal/pkg/config"
	"github.com/google/uuid"
	"github.com/redis/go-redis/v9"
)
const (
	// Key prefixes
	sessionKeyPrefix    = "session:"
	userSessionsPrefix  = "user_sessions:"
	defaultCleanupBatch = 1000
)
// RedisSessionStore implements the domain.SessionStore interface using Redis
type RedisSessionStore struct {
	client    *redis.Client
	config    pkgconfig.SessionConfig
	closeOnce sync.Once
	done      chan struct{}
}
// NewSessionStore creates a new Redis session store
func NewSessionStore(client *redis.Client, config pkgconfig.SessionConfig) *RedisSessionStore {
	store := &RedisSessionStore{
		client: client,
		config: config,
		done:   make(chan struct{}),
	}
	// Start cleanup goroutine if cleanup interval is set
	if config.CleanupInterval > 0 {
		go store.cleanupLoop()
	}
	return store
}
// Create stores a new session in Redis
func (s *RedisSessionStore) Create(ctx context.Context, session *domain.Session) error {
	// Check if user has reached max sessions
	if s.config.MaxSessionsPerUser > 0 {
		count, err := s.client.SCard(ctx, userSessionsKey(session.UserID)).Result()
		if err != nil && err != redis.Nil {
			return fmt.Errorf("failed to count user sessions: %w", err)
		}
		if count >= int64(s.config.MaxSessionsPerUser) {
			return fmt.Errorf("maximum number of sessions reached for user")
		}
	}
	// Marshal session to JSON
	data, err := json.Marshal(session)
	if err != nil {
		return fmt.Errorf("failed to marshal session: %w", err)
	}
	// Store session with expiration
	sessionKey := sessionKey(session.ID)
	pipe := s.client.Pipeline()
	pipe.Set(ctx, sessionKey, data, time.Until(session.ExpiresAt))
	pipe.SAdd(ctx, userSessionsKey(session.UserID), session.ID)
	pipe.ExpireAt(ctx, userSessionsKey(session.UserID), session.ExpiresAt)
	_, err = pipe.Exec(ctx)
	if err != nil {
		return fmt.Errorf("failed to store session: %w", err)
	}
	return nil
}
// Get retrieves a session by ID
func (s *RedisSessionStore) Get(ctx context.Context, sessionID string) (*domain.Session, error) {
	data, err := s.client.Get(ctx, sessionKey(sessionID)).Bytes()
	if err == redis.Nil {
		return nil, domain.ErrSessionNotFound
	}
	if err != nil {
		return nil, fmt.Errorf("failed to get session: %w", err)
	}
	var session domain.Session
	if err := json.Unmarshal(data, &session); err != nil {
		return nil, fmt.Errorf("failed to unmarshal session: %w", err)
	}
	return &session, nil
}
// GetUserSessions retrieves all active sessions for a user
func (s *RedisSessionStore) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	sessionIDs, err := s.client.SMembers(ctx, userSessionsKey(userID)).Result()
	if err != nil {
		return nil, fmt.Errorf("failed to get user session IDs: %w", err)
	}
	if len(sessionIDs) == 0 {
		return []*domain.Session{}, nil
	}
	pipe := s.client.Pipeline()
	cmds := make([]*redis.StringCmd, len(sessionIDs))
	for i, id := range sessionIDs {
		cmds[i] = pipe.Get(ctx, sessionKey(id))
	}
	_, err = pipe.Exec(ctx)
	if err != nil && err != redis.Nil {
		return nil, fmt.Errorf("failed to get sessions: %w", err)
	}
	sessions := make([]*domain.Session, 0, len(sessionIDs))
	for _, cmd := range cmds {
		data, err := cmd.Bytes()
		if err == redis.Nil {
			continue
		}
		if err != nil {
			return nil, fmt.Errorf("failed to get session data: %w", err)
		}
		var session domain.Session
		if err := json.Unmarshal(data, &session); err != nil {
			return nil, fmt.Errorf("failed to unmarshal session: %w", err)
		}
		sessions = append(sessions, &session)
	}
	return sessions, nil
}
// Update updates an existing session
func (s *RedisSessionStore) Update(ctx context.Context, session *domain.Session) error {
	data, err := json.Marshal(session)
	if err != nil {
		return fmt.Errorf("failed to marshal session: %w", err)
	}
	err = s.client.Set(ctx, sessionKey(session.ID), data, time.Until(session.ExpiresAt)).Err()
	if err != nil {
		return fmt.Errorf("failed to update session: %w", err)
	}
	return nil
}
// Delete removes a session
func (s *RedisSessionStore) Delete(ctx context.Context, sessionID string) error {
	// Get session first to get user ID
	session, err := s.Get(ctx, sessionID)
	if err != nil {
		return err
	}
	pipe := s.client.Pipeline()
	pipe.Del(ctx, sessionKey(sessionID))
	pipe.SRem(ctx, userSessionsKey(session.UserID), sessionID)
	_, err = pipe.Exec(ctx)
	if err != nil {
		return fmt.Errorf("failed to delete session: %w", err)
	}
	return nil
}
// DeleteUserSessions removes all sessions for a user
func (s *RedisSessionStore) DeleteUserSessions(ctx context.Context, userID string) error {
	sessions, err := s.GetUserSessions(ctx, userID)
	if err != nil {
		return err
	}
	if len(sessions) == 0 {
		return nil
	}
	pipe := s.client.Pipeline()
	for _, session := range sessions {
		pipe.Del(ctx, sessionKey(session.ID))
	}
	pipe.Del(ctx, userSessionsKey(userID))
	_, err = pipe.Exec(ctx)
	if err != nil {
		return fmt.Errorf("failed to delete user sessions: %w", err)
	}
	return nil
}
// DeleteExpired removes all expired sessions
func (s *RedisSessionStore) DeleteExpired(ctx context.Context) error {
	// This is handled automatically by Redis TTL
	return nil
}
// Touch updates the last seen time of a session
func (s *RedisSessionStore) Touch(ctx context.Context, sessionID string) error {
	session, err := s.Get(ctx, sessionID)
	if err != nil {
		return err
	}
	session.LastSeenAt = time.Now()
	return s.Update(ctx, session)
}
// Close stops the cleanup goroutine
func (s *RedisSessionStore) Close() error {
	s.closeOnce.Do(func() {
		close(s.done)
	})
	return nil
}
// cleanupLoop periodically runs cleanup of expired sessions
func (s *RedisSessionStore) cleanupLoop() {
	ticker := time.NewTicker(s.config.CleanupInterval)
	defer ticker.Stop()
	for {
		select {
		case <-ticker.C:
			if err := s.DeleteExpired(context.Background()); err != nil {
				// Log error but continue
				fmt.Printf("Failed to cleanup expired sessions: %v\n", err)
			}
		case <-s.done:
			return
		}
	}
}
// Helper functions for Redis keys
func sessionKey(id string) string {
	return fmt.Sprintf("%s%s", sessionKeyPrefix, id)
}
func userSessionsKey(userID string) string {
	return fmt.Sprintf("%s%s", userSessionsPrefix, userID)
}
func (s *RedisSessionStore) CreateSession(ctx context.Context, userID string) (*domain.Session, error) {
	// Get current sessions for user
	userSessionsKey := fmt.Sprintf("%s%s", userSessionsPrefix, userID)
	currentSessions, err := s.client.SMembers(ctx, userSessionsKey).Result()
	if err != nil && err != redis.Nil {
		return nil, fmt.Errorf("failed to get user sessions: %w", err)
	}
	// Check if max sessions limit is reached
	if len(currentSessions) >= s.config.MaxSessionsPerUser {
		// Delete oldest session
		if err := s.deleteOldestSession(ctx, userID, currentSessions); err != nil {
			return nil, fmt.Errorf("failed to delete oldest session: %w", err)
		}
	}
	// Create new session
	sessionID := uuid.NewString()
	session := &domain.Session{
		ID:        sessionID,
		UserID:    userID,
		CreatedAt: time.Now(),
		ExpiresAt: time.Now().Add(s.config.SessionDuration),
	}
	// Marshal session data
	sessionData, err := json.Marshal(session)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal session: %w", err)
	}
	// Store session in Redis
	sessionKey := fmt.Sprintf("%s%s", sessionKeyPrefix, sessionID)
	pipe := s.client.Pipeline()
	pipe.Set(ctx, sessionKey, sessionData, s.config.SessionDuration)
	pipe.SAdd(ctx, userSessionsKey, sessionID)
	if _, err := pipe.Exec(ctx); err != nil {
		return nil, fmt.Errorf("failed to store session: %w", err)
	}
	return session, nil
}
func (s *RedisSessionStore) GetSession(ctx context.Context, sessionID string) (*domain.Session, error) {
	sessionKey := fmt.Sprintf("%s%s", sessionKeyPrefix, sessionID)
	data, err := s.client.Get(ctx, sessionKey).Result()
	if err == redis.Nil {
		return nil, domain.ErrSessionNotFound
	}
	if err != nil {
		return nil, fmt.Errorf("failed to get session: %w", err)
	}
	var session domain.Session
	if err := json.Unmarshal([]byte(data), &session); err != nil {
		return nil, fmt.Errorf("failed to unmarshal session: %w", err)
	}
	return &session, nil
}
func (s *RedisSessionStore) DeleteSession(ctx context.Context, sessionID string) error {
	// Get session first to get user ID
	session, err := s.GetSession(ctx, sessionID)
	if err != nil {
		return err
	}
	// Delete session and remove from user sessions set
	sessionKey := fmt.Sprintf("%s%s", sessionKeyPrefix, sessionID)
	userSessionsKey := fmt.Sprintf("%s%s", userSessionsPrefix, session.UserID)
	pipe := s.client.Pipeline()
	pipe.Del(ctx, sessionKey)
	pipe.SRem(ctx, userSessionsKey, sessionID)
	if _, err := pipe.Exec(ctx); err != nil {
		return fmt.Errorf("failed to delete session: %w", err)
	}
	return nil
}
func (s *RedisSessionStore) deleteOldestSession(ctx context.Context, userID string, sessions []string) error {
	var oldestSession *domain.Session
	var oldestSessionID string
	// Find oldest session
	for _, sessionID := range sessions {
		session, err := s.GetSession(ctx, sessionID)
		if err != nil {
			if err == domain.ErrSessionNotFound {
				continue
			}
			return fmt.Errorf("failed to get session for user %s: %w", userID, err)
		}
		// Validate that the session belongs to the correct user
		if session.UserID != userID {
			return fmt.Errorf("session %s does not belong to user %s", sessionID, userID)
		}
		if oldestSession == nil || session.CreatedAt.Before(oldestSession.CreatedAt) {
			oldestSession = session
			oldestSessionID = sessionID
		}
	}
	if oldestSessionID != "" {
		if err := s.DeleteSession(ctx, oldestSessionID); err != nil {
			return fmt.Errorf("failed to delete oldest session for user %s: %w", userID, err)
		}
	}
	return nil
}
</file>

<file path="internal/repository/storage/cleanup_worker.go">
package storage
import (
	"context"
	"log"
	"time"
)
// CleanupWorker handles periodic cleanup of temporary files
type CleanupWorker struct {
	storage   *s3Storage
	interval  time.Duration
	stopCh    chan struct{}
	isRunning bool
}
// NewCleanupWorker creates a new cleanup worker
func NewCleanupWorker(storage *s3Storage, interval time.Duration) *CleanupWorker {
	return &CleanupWorker{
		storage:  storage,
		interval: interval,
		stopCh:   make(chan struct{}),
	}
}
// Start begins the periodic cleanup process
func (w *CleanupWorker) Start() {
	if w.isRunning {
		return
	}
	w.isRunning = true
	go func() {
		ticker := time.NewTicker(w.interval)
		defer ticker.Stop()
		// Run initial cleanup
		if err := w.storage.CleanupTempFiles(context.Background()); err != nil {
			log.Printf("Error during initial cleanup: %v", err)
		}
		for {
			select {
			case <-ticker.C:
				if err := w.storage.CleanupTempFiles(context.Background()); err != nil {
					log.Printf("Error during cleanup: %v", err)
				}
			case <-w.stopCh:
				return
			}
		}
	}()
}
// Stop halts the cleanup process
func (w *CleanupWorker) Stop() {
	if !w.isRunning {
		return
	}
	w.isRunning = false
	close(w.stopCh)
}
</file>

<file path="internal/repository/storage/cleanup.go">
package storage
import (
	"context"
	"log"
	"time"
)
// StartCleanupWorker starts a goroutine that periodically cleans up temporary files
func (s *s3Storage) StartCleanupWorker(ctx context.Context) {
	if s.cfg.CleanupInterval <= 0 {
		log.Printf("Cleanup worker disabled: cleanup interval is %v", s.cfg.CleanupInterval)
		return
	}
	go func() {
		ticker := time.NewTicker(s.cfg.CleanupInterval)
		defer ticker.Stop()
		// Run initial cleanup
		if err := s.CleanupTempFiles(ctx); err != nil {
			log.Printf("Error during initial cleanup: %v", err)
		}
		for {
			select {
			case <-ticker.C:
				if err := s.CleanupTempFiles(ctx); err != nil {
					log.Printf("Error during cleanup: %v", err)
				}
			case <-ctx.Done():
				log.Printf("Cleanup worker stopped: %v", ctx.Err())
				return
			}
		}
	}()
	log.Printf("Cleanup worker started with interval %v", s.cfg.CleanupInterval)
}
</file>

<file path="internal/repository/storage/s3.go">
package storage
import (
	"context"
	"fmt"
	"io"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"path/filepath"
	"sync"
	"time"
	"github.com/aws/aws-sdk-go-v2/aws"
	awsconfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/aws/aws-sdk-go-v2/service/s3/types"
)
type s3Storage struct {
	client  *s3.Client
	bucket  string
	cfg     *config.StorageConfig
	quotaMu sync.RWMutex
}
// NewS3Storage creates a new S3 storage service
func NewS3Storage(cfg *config.StorageConfig) (domain.StorageService, error) {
	if cfg.Bucket == "" {
		return nil, &domain.StorageError{
			Code:    "InvalidConfig",
			Message: "S3 bucket name is required",
			Op:      "NewS3Storage",
		}
	}
	// Create AWS credentials
	creds := credentials.NewStaticCredentialsProvider(cfg.AccessKey, cfg.SecretKey, "")
	// Configure AWS client
	awsCfg, err := awsconfig.LoadDefaultConfig(context.Background(),
		awsconfig.WithCredentialsProvider(creds),
		awsconfig.WithRegion(cfg.Region),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}
	// Create S3 client
	client := s3.NewFromConfig(awsCfg)
	return &s3Storage{
		client: client,
		bucket: cfg.Bucket,
		cfg:    cfg,
	}, nil
}
// generateKey generates a storage key
func generateKey(pathType domain.StoragePathType, filename string) string {
	ext := filepath.Ext(filename)
	timestamp := time.Now().UTC().Format("20060102150405")
	return fmt.Sprintf("%s/%s/%s%s", pathType.String(), timestamp[:8], timestamp, ext)
}
// Upload uploads a file to S3
func (s *s3Storage) Upload(ctx context.Context, file *domain.StorageFile) error {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("s3_upload"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("s3_upload", "started").Inc()
	// Generate storage key if not provided
	if file.Key == "" {
		file.Key = generateKey(domain.StoragePathPerm, file.Name)
	}
	// Validate upload
	if err := s.ValidateUpload(ctx, file.Size, file.ContentType); err != nil {
		metrics.AudioOpErrors.WithLabelValues("s3_upload", "validation_error").Inc()
		return err
	}
	// Convert metadata to AWS format
	awsMetadata := make(map[string]string)
	for k, v := range file.Metadata {
		awsMetadata[k] = v
	}
	// Upload file
	_, err := s.client.PutObject(ctx, &s3.PutObjectInput{
		Bucket:      aws.String(s.bucket),
		Key:         aws.String(file.Key),
		Body:        file.Content,
		ContentType: aws.String(file.ContentType),
		Metadata:    awsMetadata,
	})
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("s3_upload", "s3_error").Inc()
		return fmt.Errorf("failed to upload to S3: %w", err)
	}
	metrics.AudioOps.WithLabelValues("s3_upload", "completed").Inc()
	return nil
}
// Download downloads a file from S3
func (s *s3Storage) Download(ctx context.Context, key string) (*domain.StorageFile, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("s3_download"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("s3_download", "started").Inc()
	// Get object
	result, err := s.client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("s3_download", "s3_error").Inc()
		return nil, fmt.Errorf("failed to download from S3: %w", err)
	}
	// Convert metadata from AWS format
	metadata := make(map[string]string)
	for k, v := range result.Metadata {
		metadata[k] = v
	}
	var size int64
	if result.ContentLength != nil {
		size = *result.ContentLength
	}
	file := &domain.StorageFile{
		Key:         key,
		Name:        filepath.Base(key),
		Size:        size,
		ContentType: aws.ToString(result.ContentType),
		Content:     result.Body,
		Metadata:    metadata,
	}
	metrics.AudioOps.WithLabelValues("s3_download", "completed").Inc()
	return file, nil
}
// Delete removes a file from S3
func (s *s3Storage) Delete(ctx context.Context, key string) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("delete"))
	defer timer.ObserveDuration()
	// Delete file
	_, err := s.client.DeleteObject(ctx, &s3.DeleteObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("delete").Inc()
		return fmt.Errorf("failed to delete file: %w", err)
	}
	metrics.StorageOperationSuccess.WithLabelValues("delete").Inc()
	return nil
}
// GetURL generates a pre-signed URL for the file
func (s *s3Storage) GetURL(ctx context.Context, key string) (string, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("s3_get_url"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("s3_get_url", "started").Inc()
	// Create presigner
	presigner := s3.NewPresignClient(s.client)
	// Generate presigned URL
	request, err := presigner.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = time.Duration(15 * time.Minute)
	})
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("s3_get_url", "presign_error").Inc()
		return "", fmt.Errorf("failed to generate presigned URL: %w", err)
	}
	metrics.AudioOps.WithLabelValues("s3_get_url", "completed").Inc()
	return request.URL, nil
}
// GetMetadata retrieves metadata for a file
func (s *s3Storage) GetMetadata(ctx context.Context, key string) (*domain.FileMetadata, error) {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("get_metadata"))
	defer timer.ObserveDuration()
	// Get object metadata
	result, err := s.client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("get_metadata").Inc()
		return nil, fmt.Errorf("failed to get file metadata: %w", err)
	}
	metadata := &domain.FileMetadata{
		Key:          key,
		Size:         aws.ToInt64(result.ContentLength),
		ContentType:  aws.ToString(result.ContentType),
		LastModified: *result.LastModified,
		ETag:         aws.ToString(result.ETag),
	}
	metrics.StorageOperationSuccess.WithLabelValues("get_metadata").Inc()
	return metadata, nil
}
// ListFiles lists files with the given prefix
func (s *s3Storage) ListFiles(ctx context.Context, prefix string) ([]*domain.FileMetadata, error) {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("list_files"))
	defer timer.ObserveDuration()
	var files []*domain.FileMetadata
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
		Prefix: aws.String(prefix),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			metrics.StorageOperationErrors.WithLabelValues("list_files").Inc()
			return nil, fmt.Errorf("failed to list files: %w", err)
		}
		for _, obj := range page.Contents {
			files = append(files, &domain.FileMetadata{
				Key:          aws.ToString(obj.Key),
				Size:         aws.ToInt64(obj.Size),
				LastModified: *obj.LastModified,
				ETag:         aws.ToString(obj.ETag),
				StorageClass: string(obj.StorageClass),
			})
		}
	}
	metrics.StorageOperationSuccess.WithLabelValues("list_files").Inc()
	return files, nil
}
// GetQuotaUsage gets the total storage usage
func (s *s3Storage) GetQuotaUsage(ctx context.Context) (int64, error) {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("get_quota"))
	defer timer.ObserveDuration()
	var totalSize int64
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			metrics.StorageOperationErrors.WithLabelValues("get_quota").Inc()
			return 0, fmt.Errorf("failed to get quota usage: %w", err)
		}
		for _, obj := range page.Contents {
			totalSize += aws.ToInt64(obj.Size)
		}
	}
	metrics.StorageOperationSuccess.WithLabelValues("get_quota").Inc()
	return totalSize, nil
}
// GetUserQuotaUsage gets the total storage usage for a specific user
func (s *s3Storage) GetUserQuotaUsage(ctx context.Context, userID string) (int64, error) {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("get_user_quota"))
	defer timer.ObserveDuration()
	var totalSize int64
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
		Prefix: aws.String(fmt.Sprintf("users/%s/", userID)),
	})
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			metrics.StorageOperationErrors.WithLabelValues("get_user_quota").Inc()
			return 0, fmt.Errorf("failed to get user quota usage: %w", err)
		}
		for _, obj := range page.Contents {
			totalSize += aws.ToInt64(obj.Size)
		}
	}
	metrics.StorageQuotaUsage.WithLabelValues(userID).Set(float64(totalSize))
	metrics.StorageOperationSuccess.WithLabelValues("get_user_quota").Inc()
	return totalSize, nil
}
// ValidateUpload validates a file upload request
func (s *s3Storage) ValidateUpload(ctx context.Context, fileSize int64, mimeType string) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("validate_upload"))
	defer timer.ObserveDuration()
	// Check file size
	if fileSize > s.cfg.MaxFileSize {
		return &domain.StorageError{
			Code:    "FILE_TOO_LARGE",
			Message: fmt.Sprintf("file size %d exceeds maximum allowed size %d", fileSize, s.cfg.MaxFileSize),
		}
	}
	// Check file type
	allowed := false
	for _, allowedType := range s.cfg.AllowedFileTypes {
		if mimeType == allowedType {
			allowed = true
			break
		}
	}
	if !allowed {
		return &domain.StorageError{
			Code:    "INVALID_FILE_TYPE",
			Message: fmt.Sprintf("file type %s is not allowed", mimeType),
		}
	}
	// Check total quota
	usage, err := s.GetQuotaUsage(ctx)
	if err != nil {
		return fmt.Errorf("failed to check quota: %w", err)
	}
	if usage+fileSize > s.cfg.TotalQuota {
		return &domain.StorageError{
			Code:    "QUOTA_EXCEEDED",
			Message: "total storage quota exceeded",
		}
	}
	return nil
}
// CleanupTempFiles removes expired temporary files
func (s *s3Storage) CleanupTempFiles(ctx context.Context) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("cleanup"))
	defer timer.ObserveDuration()
	var deletedCount int64
	var totalSize int64
	// List all temporary files
	paginator := s3.NewListObjectsV2Paginator(s.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(s.bucket),
		Prefix: aws.String(domain.StoragePathTemp.String()),
	})
	now := time.Now()
	var objectsToDelete []types.ObjectIdentifier
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			metrics.StorageOperationErrors.WithLabelValues("cleanup").Inc()
			return fmt.Errorf("failed to list temporary files: %w", err)
		}
		for _, obj := range page.Contents {
			// Check if file is expired based on LastModified
			if now.Sub(*obj.LastModified) > s.cfg.TempFileExpiry {
				objectsToDelete = append(objectsToDelete, types.ObjectIdentifier{
					Key: obj.Key,
				})
				deletedCount++
				totalSize += aws.ToInt64(obj.Size)
				// Delete in batches of 1000 (S3 limit)
				if len(objectsToDelete) >= 1000 {
					if err := s.deleteObjects(ctx, objectsToDelete); err != nil {
						return err
					}
					objectsToDelete = objectsToDelete[:0]
				}
			}
		}
	}
	// Delete any remaining objects
	if len(objectsToDelete) > 0 {
		if err := s.deleteObjects(ctx, objectsToDelete); err != nil {
			return err
		}
	}
	metrics.StorageCleanupFilesDeleted.Add(float64(deletedCount))
	metrics.StorageCleanupBytesReclaimed.Add(float64(totalSize))
	metrics.StorageOperationSuccess.WithLabelValues("cleanup").Inc()
	return nil
}
// deleteObjects deletes a batch of objects from S3
func (s *s3Storage) deleteObjects(ctx context.Context, objects []types.ObjectIdentifier) error {
	_, err := s.client.DeleteObjects(ctx, &s3.DeleteObjectsInput{
		Bucket: aws.String(s.bucket),
		Delete: &types.Delete{
			Objects: objects,
			Quiet:   aws.Bool(true),
		},
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("cleanup_batch").Inc()
		return fmt.Errorf("failed to delete objects batch: %w", err)
	}
	return nil
}
// DeleteAudio deletes an audio file from storage
func (s *s3Storage) DeleteAudio(ctx context.Context, path string) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("delete_audio"))
	defer timer.ObserveDuration()
	// Delete file
	_, err := s.client.DeleteObject(ctx, &s3.DeleteObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(path),
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("delete_audio").Inc()
		return fmt.Errorf("failed to delete audio file: %w", err)
	}
	metrics.StorageOperationSuccess.WithLabelValues("delete_audio").Inc()
	return nil
}
// GetSignedURL generates a pre-signed URL for the file with expiry
func (s *s3Storage) GetSignedURL(ctx context.Context, path string, expiry time.Duration) (string, error) {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("get_signed_url"))
	defer timer.ObserveDuration()
	// Create presigner
	presigner := s3.NewPresignClient(s.client)
	// Generate presigned URL
	request, err := presigner.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(path),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = expiry
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("get_signed_url").Inc()
		return "", fmt.Errorf("failed to generate signed URL: %w", err)
	}
	metrics.StorageOperationSuccess.WithLabelValues("get_signed_url").Inc()
	return request.URL, nil
}
// UploadAudio uploads an audio file to storage
func (s *s3Storage) UploadAudio(ctx context.Context, file io.Reader, path string) error {
	timer := metrics.NewTimer(metrics.StorageOperationDuration.WithLabelValues("upload_audio"))
	defer timer.ObserveDuration()
	// Upload file
	_, err := s.client.PutObject(ctx, &s3.PutObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(path),
		Body:   file,
	})
	if err != nil {
		metrics.StorageOperationErrors.WithLabelValues("upload_audio").Inc()
		return fmt.Errorf("failed to upload audio file: %w", err)
	}
	metrics.StorageOperationSuccess.WithLabelValues("upload_audio").Inc()
	return nil
}
</file>

<file path="internal/repository/cache_service.go">
// Package repository implements the data access layer
package repository
import (
	"github.com/redis/go-redis/v9"
)
type CacheService struct {
	client *redis.Client
}
// NewCacheService creates a new Redis cache service
func NewCacheService(client *redis.Client) *CacheService {
	return &CacheService{
		client: client,
	}
}
// ... existing code ...
</file>

<file path="internal/test/integration/auth_test.go">
package integration
import (
	"encoding/json"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/test/testutil"
	"net/http"
	"net/http/httptest"
	"testing"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)
func TestAuthFlow(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	t.Run("complete auth flow", func(t *testing.T) {
		// 1. Register new user
		user, err := ts.CreateTestUser("test@example.com", "password123", domain.RoleUser)
		require.NoError(t, err)
		require.NotEmpty(t, user.ID)
		// 2. Login with credentials
		token, err := ts.GetAuthToken("test@example.com", "password123")
		require.NoError(t, err)
		require.NotEmpty(t, token)
		// 3. Access protected endpoint
		w, err := ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 4. Logout
		w, err = ts.MakeRequest(http.MethodPost, "/api/v1/auth/logout", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 5. Verify can't access protected endpoint after logout
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusUnauthorized, w.Code)
	})
}
func TestSessionManagement(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Create test user and get auth token
	user, err := ts.CreateTestUser("test@example.com", "password123", domain.RoleUser)
	require.NoError(t, err)
	require.NotEmpty(t, user.ID)
	token, err := ts.GetAuthToken("test@example.com", "password123")
	require.NoError(t, err)
	require.NotEmpty(t, token)
	t.Run("session operations", func(t *testing.T) {
		// 1. Get active sessions
		w, err := ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 2. Create another session (login again)
		token2, err := ts.GetAuthToken("test@example.com", "password123")
		require.NoError(t, err)
		require.NotEmpty(t, token2)
		// 3. Verify both sessions work
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token2,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 4. Logout from first session
		w, err = ts.MakeRequest(http.MethodPost, "/api/v1/auth/logout", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 5. Verify first session is invalid but second still works
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusUnauthorized, w.Code)
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
			"Authorization": "Bearer " + token2,
		})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
	})
}
func TestAPIKeyAuth(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Create test user with API key
	token, err := ts.GetAuthToken("test@example.com", "password123")
	require.NoError(t, err)
	require.NotEmpty(t, token)
	t.Run("api key operations", func(t *testing.T) {
		// 1. Generate API key
		w, err := ts.MakeRequest(http.MethodPost, "/api/v1/auth/apikey", nil,
			map[string]string{"Authorization": "Bearer " + token})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		var keyResult struct {
			Data struct {
				APIKey string `json:"api_key"`
			} `json:"data"`
		}
		require.NoError(t, decodeJSON(w, &keyResult))
		assert.NotEmpty(t, keyResult.Data.APIKey)
		// 2. Access endpoint with API key
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/tracks", nil,
			map[string]string{"X-API-Key": keyResult.Data.APIKey})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 3. Try invalid API key
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/tracks", nil,
			map[string]string{"X-API-Key": "invalid-key"})
		require.NoError(t, err)
		require.Equal(t, http.StatusUnauthorized, w.Code)
	})
}
func TestRoleBasedAccess(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Create admin user
	adminUser, err := ts.CreateTestUser("admin@example.com", "password123", domain.RoleAdmin)
	require.NoError(t, err)
	require.NotEmpty(t, adminUser.ID)
	// Get admin token
	adminToken, err := ts.GetAuthToken("admin@example.com", "password123")
	require.NoError(t, err)
	require.NotEmpty(t, adminToken)
	// Create regular user
	userUser, err := ts.CreateTestUser("user@example.com", "password123", domain.RoleUser)
	require.NoError(t, err)
	require.NotEmpty(t, userUser.ID)
	// Get regular user token
	userToken, err := ts.GetAuthToken("user@example.com", "password123")
	require.NoError(t, err)
	require.NotEmpty(t, userToken)
	t.Run("role-based permissions", func(t *testing.T) {
		// 1. Admin can access admin endpoints
		w, err := ts.MakeRequest(http.MethodPost, "/api/v1/tracks/batch", nil,
			map[string]string{"Authorization": "Bearer " + adminToken})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		// 2. Regular user cannot access admin endpoints
		w, err = ts.MakeRequest(http.MethodPost, "/api/v1/tracks/batch", nil,
			map[string]string{"Authorization": "Bearer " + userToken})
		require.NoError(t, err)
		require.Equal(t, http.StatusForbidden, w.Code)
		// 3. Both can access regular endpoints
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/tracks", nil,
			map[string]string{"Authorization": "Bearer " + adminToken})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
		w, err = ts.MakeRequest(http.MethodGet, "/api/v1/tracks", nil,
			map[string]string{"Authorization": "Bearer " + userToken})
		require.NoError(t, err)
		require.Equal(t, http.StatusOK, w.Code)
	})
}
// Helper function to decode JSON response
func decodeJSON(resp *httptest.ResponseRecorder, v interface{}) error {
	return json.NewDecoder(resp.Body).Decode(v)
}
</file>

<file path="internal/test/integration/helper.go">
package integration
import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/handler"
	"metadatatool/internal/handler/middleware"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/repository/base"
	"metadatatool/internal/usecase"
	"net/http"
	"net/http/httptest"
	"testing"
	"github.com/gin-gonic/gin"
	"github.com/stretchr/testify/require"
)
// TestServer represents a test server instance
type TestServer struct {
	Router       *gin.Engine
	AuthHandler  *handler.AuthHandler
	SessionStore domain.SessionStore
	AuthService  domain.AuthService
	UserRepo     domain.UserRepository
	cleanup      func()
}
// NewTestServer creates a new test server with all necessary dependencies
func NewTestServer(t *testing.T) *TestServer {
	// Setup repositories and services
	userRepo := base.NewInMemoryUserRepository()
	sessionRepo := base.NewInMemorySessionRepository()
	authService := base.NewInMemoryAuthService()
	// Initialize usecases
	authUseCase := usecase.NewAuthUseCase(userRepo, sessionRepo, authService)
	userUseCase := usecase.NewUserUseCase(userRepo)
	// Initialize handlers
	authHandler := handler.NewAuthHandler(authUseCase, userUseCase, sessionRepo)
	// Setup router
	gin.SetMode(gin.TestMode)
	router := gin.New()
	router.Use(gin.Recovery())
	// Auth routes
	api := router.Group("/api/v1")
	{
		auth := api.Group("/auth")
		{
			auth.POST("/register", authHandler.Register)
			auth.POST("/login", authHandler.Login)
			auth.POST("/refresh", authHandler.RefreshToken)
			auth.POST("/logout", middleware.Auth(authService), authHandler.Logout)
			auth.POST("/apikey", middleware.Auth(authService), authHandler.GenerateAPIKey)
			// Session management routes (protected)
			sessions := auth.Group("")
			sessions.Use(middleware.Auth(authService))
			{
				sessions.GET("/sessions", authHandler.GetActiveSessions)
				sessions.DELETE("/sessions/:id", authHandler.RevokeSession)
				sessions.DELETE("/sessions", authHandler.RevokeAllSessions)
			}
		}
		protected := api.Group("/protected")
		protected.Use(middleware.Auth(authService))
		{
			protected.GET("/user", func(c *gin.Context) {
				c.JSON(http.StatusOK, gin.H{"message": "User endpoint"})
			})
		}
		admin := api.Group("/admin")
		admin.Use(middleware.Auth(authService), middleware.RequireRole(domain.RoleAdmin))
		{
			admin.GET("/dashboard", func(c *gin.Context) {
				c.JSON(http.StatusOK, gin.H{"message": "Admin dashboard"})
			})
		}
	}
	// Create test server
	ts := &TestServer{
		Router:       router,
		AuthHandler:  authHandler,
		SessionStore: sessionRepo,
		AuthService:  authService,
		UserRepo:     userRepo,
		cleanup:      func() {},
	}
	return ts
}
// Close cleans up resources
func (ts *TestServer) Close() {
	if ts.cleanup != nil {
		ts.cleanup()
	}
}
// MakeRequest is a helper to make HTTP requests in tests
func (ts *TestServer) MakeRequest(method, path string, body interface{}, headers map[string]string) *httptest.ResponseRecorder {
	var reqBody []byte
	var err error
	if body != nil {
		reqBody, err = json.Marshal(body)
		if err != nil {
			panic(fmt.Sprintf("failed to marshal request body: %v", err))
		}
	}
	w := httptest.NewRecorder()
	req := httptest.NewRequest(method, path, bytes.NewBuffer(reqBody))
	req.Header.Set("Content-Type", "application/json")
	// Add custom headers
	for k, v := range headers {
		req.Header.Set(k, v)
	}
	ts.Router.ServeHTTP(w, req)
	return w
}
// CreateTestUser creates a test user and returns the user object
func (ts *TestServer) CreateTestUser(t *testing.T, email, password string, role domain.Role) *domain.User {
	// Hash password
	hashedPassword, err := ts.AuthService.HashPassword(password)
	require.NoError(t, err)
	// Create user
	user := &domain.User{
		Email:    email,
		Password: hashedPassword,
		Role:     role,
		Name:     "Test User",
	}
	err = ts.UserRepo.Create(context.Background(), user)
	require.NoError(t, err)
	return user
}
// GetAuthToken performs login and returns the auth token
func (ts *TestServer) GetAuthToken(t *testing.T, email, password string) string {
	// Create user if it doesn't exist
	ts.CreateTestUser(t, email, password, domain.RoleUser)
	// Get user
	user, err := ts.UserRepo.GetByEmail(context.Background(), email)
	require.NoError(t, err)
	// Generate tokens
	tokens, err := ts.AuthService.GenerateTokens(user)
	require.NoError(t, err)
	return tokens.AccessToken
}
// GetAuthHeader returns the Authorization header with the token
func (ts *TestServer) GetAuthHeader(token string) map[string]string {
	return map[string]string{
		"Authorization": fmt.Sprintf("Bearer %s", token),
	}
}
</file>

<file path="internal/test/testutil/server.go">
package testutil
import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"metadatatool/internal/handler"
	"metadatatool/internal/handler/middleware"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/repository/base"
	"metadatatool/internal/usecase"
	"net/http"
	"net/http/httptest"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
)
// TestServer represents a test server instance
type TestServer struct {
	Router       *gin.Engine
	AuthHandler  *handler.AuthHandler
	SessionStore domain.SessionStore
	AuthService  domain.AuthService
	UserRepo     domain.UserRepository
	Cleanup      func()
}
// NewTestServer creates a new test server with all dependencies
func NewTestServer() *TestServer {
	gin.SetMode(gin.TestMode)
	router := gin.New()
	router.Use(gin.Recovery())
	// Setup repositories and services
	userRepo := base.NewInMemoryUserRepository()
	sessionStore := base.NewInMemorySessionRepository()
	authService := base.NewInMemoryAuthService()
	// Setup session config
	sessionConfig := domain.SessionConfig{
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 5,
	}
	// Setup use cases
	authUseCase := usecase.NewAuthUseCase(userRepo, sessionStore, authService)
	userUseCase := usecase.NewUserUseCase(userRepo)
	// Setup handlers
	authHandler := handler.NewAuthHandler(authUseCase, userUseCase, sessionStore)
	// Setup routes
	api := router.Group("/api/v1")
	{
		auth := api.Group("/auth")
		{
			auth.POST("/register", authHandler.Register)
			auth.POST("/login", middleware.CreateSession(sessionStore, sessionConfig), authHandler.Login)
			auth.POST("/refresh", authHandler.RefreshToken)
			// Protected auth routes
			protected := auth.Group("")
			protected.Use(middleware.Session(sessionStore, sessionConfig))
			protected.Use(middleware.RequireSession())
			{
				protected.POST("/logout", middleware.ClearSession(sessionStore, sessionConfig), authHandler.Logout)
				protected.POST("/apikey", authHandler.GenerateAPIKey)
				protected.GET("/sessions", authHandler.GetActiveSessions)
				protected.DELETE("/sessions/:id", authHandler.RevokeSession)
				protected.DELETE("/sessions", authHandler.RevokeAllSessions)
			}
		}
		protected := api.Group("/protected")
		protected.Use(middleware.Session(sessionStore, sessionConfig))
		protected.Use(middleware.RequireSession())
		{
			protected.GET("/user", func(c *gin.Context) {
				c.JSON(http.StatusOK, gin.H{"message": "User endpoint"})
			})
		}
		admin := api.Group("/admin")
		admin.Use(middleware.Session(sessionStore, sessionConfig))
		admin.Use(middleware.RequireSession())
		admin.Use(middleware.RequireRole(domain.RoleAdmin))
		{
			admin.GET("/dashboard", func(c *gin.Context) {
				c.JSON(http.StatusOK, gin.H{"message": "Admin dashboard"})
			})
		}
		tracks := api.Group("/tracks")
		{
			tracks.GET("", func(c *gin.Context) { c.Status(http.StatusOK) })
			authenticated := tracks.Group("")
			authenticated.Use(middleware.Session(sessionStore, sessionConfig))
			authenticated.Use(middleware.RequireSession())
			{
				authenticated.POST("", func(c *gin.Context) { c.Status(http.StatusOK) })
				authenticated.PUT("/:id", func(c *gin.Context) { c.Status(http.StatusOK) })
				authenticated.DELETE("/:id", func(c *gin.Context) { c.Status(http.StatusOK) })
				admin := authenticated.Group("")
				admin.Use(middleware.RequireRole(domain.RoleAdmin))
				{
					admin.POST("/batch", func(c *gin.Context) { c.Status(http.StatusOK) })
					admin.POST("/export", func(c *gin.Context) { c.Status(http.StatusOK) })
				}
			}
		}
	}
	return &TestServer{
		Router:       router,
		AuthHandler:  authHandler,
		SessionStore: sessionStore,
		AuthService:  authService,
		UserRepo:     userRepo,
		Cleanup:      func() {},
	}
}
// MakeRequest makes an HTTP request to the test server
func (ts *TestServer) MakeRequest(method, path string, body interface{}, headers map[string]string) (*httptest.ResponseRecorder, error) {
	var reqBody io.Reader
	if body != nil {
		jsonBody, err := json.Marshal(body)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal request body: %w", err)
		}
		reqBody = bytes.NewBuffer(jsonBody)
	}
	req := httptest.NewRequest(method, path, reqBody)
	req.Header.Set("Content-Type", "application/json")
	for key, value := range headers {
		req.Header.Set(key, value)
	}
	w := httptest.NewRecorder()
	ts.Router.ServeHTTP(w, req)
	return w, nil
}
// GetAuthToken gets an authentication token for a user
func (ts *TestServer) GetAuthToken(email, password string) (string, error) {
	// First, get the user
	user, err := ts.UserRepo.GetByEmail(context.Background(), email)
	if err != nil || user == nil {
		// Create user if it doesn't exist
		user, err = ts.CreateTestUser(email, password, domain.RoleUser)
		if err != nil {
			return "", fmt.Errorf("failed to create user: %w", err)
		}
	}
	// Login to get token
	loginReq := map[string]interface{}{
		"email":    email,
		"password": password,
	}
	w, err := ts.MakeRequest(http.MethodPost, "/api/v1/auth/login", loginReq, nil)
	if err != nil {
		return "", fmt.Errorf("failed to login: %w", err)
	}
	if w.Code != http.StatusOK {
		return "", fmt.Errorf("login failed with status %d: %s", w.Code, w.Body.String())
	}
	// Log response body for debugging
	fmt.Printf("Login response body: %s\n", w.Body.String())
	// Parse response body
	var loginResp struct {
		Data struct {
			Session struct {
				Token string `json:"token"`
			} `json:"session"`
		} `json:"data"`
	}
	if err := json.NewDecoder(w.Body).Decode(&loginResp); err != nil {
		return "", fmt.Errorf("failed to decode login response: %w", err)
	}
	if loginResp.Data.Session.Token == "" {
		return "", fmt.Errorf("no token in login response")
	}
	return loginResp.Data.Session.Token, nil
}
// CreateTestUser creates a new user for testing
func (ts *TestServer) CreateTestUser(email, password string, role domain.Role) (*domain.User, error) {
	// Hash password
	hashedPassword, err := ts.AuthService.HashPassword(password)
	if err != nil {
		return nil, fmt.Errorf("failed to hash password: %w", err)
	}
	// Create user
	user := &domain.User{
		ID:       uuid.New().String(),
		Email:    email,
		Password: hashedPassword,
		Role:     role,
		Name:     "Test User",
	}
	// Save user
	if err := ts.UserRepo.Create(context.Background(), user); err != nil {
		return nil, fmt.Errorf("failed to create user: %w", err)
	}
	return user, nil
}
</file>

<file path="internal/test/auth_test.go">
package test
import (
	"bytes"
	"encoding/json"
	"fmt"
	"metadatatool/internal/handler"
	"metadatatool/internal/handler/middleware"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/repository/base"
	"metadatatool/internal/test/testutil"
	"metadatatool/internal/usecase"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"
	"github.com/gin-gonic/gin"
	"github.com/stretchr/testify/require"
)
// TestServer represents a test server instance
type TestServer struct {
	Router       *gin.Engine
	AuthHandler  *handler.AuthHandler
	SessionStore domain.SessionStore
	AuthService  domain.AuthService
	UserRepo     domain.UserRepository
	cleanup      func()
}
// NewTestServer creates a new test server with all necessary dependencies
func NewTestServer(t *testing.T) *TestServer {
	// Setup repositories
	userRepo := base.NewInMemoryUserRepository()
	sessionRepo := base.NewInMemorySessionRepository()
	authService := base.NewInMemoryAuthService()
	// Setup session config
	sessionConfig := domain.SessionConfig{
		SessionDuration:    24 * time.Hour,
		CleanupInterval:    time.Hour,
		MaxSessionsPerUser: 5,
	}
	// Initialize usecases
	authUseCase := usecase.NewAuthUseCase(userRepo, sessionRepo, authService)
	userUseCase := usecase.NewUserUseCase(userRepo)
	// Initialize handlers
	authHandler := handler.NewAuthHandler(authUseCase, userUseCase, sessionRepo)
	// Setup router
	gin.SetMode(gin.TestMode)
	router := gin.New()
	router.Use(gin.Recovery())
	// Auth routes
	auth := router.Group("/api/v1/auth")
	{
		auth.POST("/register", authHandler.Register)
		auth.POST("/login", middleware.CreateSession(sessionRepo, sessionConfig), authHandler.Login)
		auth.POST("/refresh", authHandler.RefreshToken)
		// Protected auth routes
		protected := auth.Group("")
		protected.Use(middleware.Session(sessionRepo, sessionConfig))
		protected.Use(middleware.RequireSession())
		{
			protected.POST("/logout", middleware.ClearSession(sessionRepo, domain.SessionConfig{
				CookieName:     "session_id",
				CookiePath:     "/",
				CookieDomain:   "localhost",
				CookieSecure:   true,
				CookieHTTPOnly: true,
			}), authHandler.Logout)
			protected.POST("/apikey", authHandler.GenerateAPIKey)
			protected.GET("/sessions", authHandler.GetActiveSessions)
			protected.DELETE("/sessions/:id", authHandler.RevokeSession)
			protected.DELETE("/sessions", authHandler.RevokeAllSessions)
		}
	}
	// Protected routes for testing
	api := router.Group("/api/v1")
	{
		tracks := api.Group("/tracks")
		{
			tracks.GET("", func(c *gin.Context) { c.Status(http.StatusOK) })
			authenticated := tracks.Group("")
			authenticated.Use(middleware.Session(sessionRepo, sessionConfig))
			authenticated.Use(middleware.RequireSession())
			{
				authenticated.POST("", func(c *gin.Context) { c.Status(http.StatusOK) })
				authenticated.PUT("/:id", func(c *gin.Context) { c.Status(http.StatusOK) })
				authenticated.DELETE("/:id", func(c *gin.Context) { c.Status(http.StatusOK) })
				admin := authenticated.Group("")
				admin.Use(middleware.RequireRole(domain.RoleAdmin))
				{
					admin.POST("/batch", func(c *gin.Context) { c.Status(http.StatusOK) })
					admin.POST("/export", func(c *gin.Context) { c.Status(http.StatusOK) })
				}
			}
		}
	}
	// Create test server
	ts := &TestServer{
		Router:       router,
		AuthHandler:  authHandler,
		SessionStore: sessionRepo,
		UserRepo:     userRepo,
		cleanup:      func() {},
	}
	return ts
}
// Close cleans up resources
func (ts *TestServer) Close() {
	if ts.cleanup != nil {
		ts.cleanup()
	}
}
// MakeRequest is a helper to make HTTP requests in tests
func (ts *TestServer) MakeRequest(method, path string, body interface{}, headers map[string]string) *httptest.ResponseRecorder {
	var reqBody []byte
	var err error
	if body != nil {
		reqBody, err = json.Marshal(body)
		if err != nil {
			panic(fmt.Sprintf("failed to marshal request body: %v", err))
		}
	}
	w := httptest.NewRecorder()
	req := httptest.NewRequest(method, path, bytes.NewBuffer(reqBody))
	req.Header.Set("Content-Type", "application/json")
	// Add custom headers
	for k, v := range headers {
		req.Header.Set(k, v)
	}
	ts.Router.ServeHTTP(w, req)
	return w
}
// GetAuthHeader returns the Authorization header with the token
func (ts *TestServer) GetAuthHeader(token string) map[string]string {
	return map[string]string{
		"Authorization": fmt.Sprintf("Bearer %s", token),
	}
}
// Helper function to decode JSON responses
func decodeJSON(resp *httptest.ResponseRecorder, v interface{}) error {
	return json.NewDecoder(resp.Body).Decode(v)
}
func TestAuthFlow(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Test user registration
	user, err := ts.CreateTestUser("test@example.com", "password123", domain.RoleUser)
	require.NoError(t, err)
	require.NotEmpty(t, user.ID)
	// Test login and get token
	token, err := ts.GetAuthToken("test@example.com", "password123")
	require.NoError(t, err)
	require.NotEmpty(t, token)
	// Test accessing protected endpoint
	w, err := ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
		"Authorization": "Bearer " + token,
	})
	require.NoError(t, err)
	require.Equal(t, http.StatusOK, w.Code)
	// Test logout
	w, err = ts.MakeRequest(http.MethodPost, "/api/v1/auth/logout", nil, map[string]string{
		"Authorization": "Bearer " + token,
	})
	require.NoError(t, err)
	require.Equal(t, http.StatusOK, w.Code)
	// Test accessing protected endpoint after logout (should fail)
	w, err = ts.MakeRequest(http.MethodGet, "/api/v1/protected/user", nil, map[string]string{
		"Authorization": "Bearer " + token,
	})
	require.NoError(t, err)
	require.Equal(t, http.StatusUnauthorized, w.Code)
}
func TestInvalidAuth(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Test registration with invalid email
	w, err := ts.MakeRequest(http.MethodPost, "/api/v1/auth/register", map[string]string{
		"email":    "invalid-email",
		"password": "password123",
	}, nil)
	require.NoError(t, err)
	require.Equal(t, http.StatusBadRequest, w.Code)
	// Test login with non-existent user
	w, err = ts.MakeRequest(http.MethodPost, "/api/v1/auth/login", map[string]string{
		"email":    "nonexistent@example.com",
		"password": "password123",
	}, nil)
	require.NoError(t, err)
	require.Equal(t, http.StatusUnauthorized, w.Code)
}
func TestRoleBasedAccess(t *testing.T) {
	ts := testutil.NewTestServer()
	defer ts.Cleanup()
	// Create admin user
	adminUser, err := ts.CreateTestUser("admin@example.com", "admin123", domain.RoleAdmin)
	require.NoError(t, err)
	require.NotEmpty(t, adminUser.ID)
	// Get admin token
	adminToken, err := ts.GetAuthToken("admin@example.com", "admin123")
	require.NoError(t, err)
	require.NotEmpty(t, adminToken)
	// Create regular user
	regularUser, err := ts.CreateTestUser("user@example.com", "user123", domain.RoleUser)
	require.NoError(t, err)
	require.NotEmpty(t, regularUser.ID)
	// Get regular user token
	userToken, err := ts.GetAuthToken("user@example.com", "user123")
	require.NoError(t, err)
	require.NotEmpty(t, userToken)
	// Test admin access to admin endpoint
	w, err := ts.MakeRequest(http.MethodGet, "/api/v1/admin/dashboard", nil, map[string]string{
		"Authorization": "Bearer " + adminToken,
	})
	require.NoError(t, err)
	require.Equal(t, http.StatusOK, w.Code)
	// Test regular user access to admin endpoint (should fail)
	w, err = ts.MakeRequest(http.MethodGet, "/api/v1/admin/dashboard", nil, map[string]string{
		"Authorization": "Bearer " + userToken,
	})
	require.NoError(t, err)
	require.Equal(t, http.StatusForbidden, w.Code)
}
</file>

<file path="internal/test/helper.go">
package test
import (
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/test/testutil"
	"net/http/httptest"
	"testing"
	"github.com/stretchr/testify/require"
)
// CreateTestUser creates a new user for testing
func CreateTestUser(t *testing.T, ts *testutil.TestServer, email, password string, role domain.Role) *domain.User {
	user, err := ts.CreateTestUser(email, password, role)
	require.NoError(t, err)
	require.NotEmpty(t, user.ID)
	return user
}
// GetAuthToken gets an authentication token for a user
func GetAuthToken(t *testing.T, ts *testutil.TestServer, email, password string) string {
	token, err := ts.GetAuthToken(email, password)
	require.NoError(t, err)
	require.NotEmpty(t, token)
	return token
}
// MakeRequest makes an HTTP request to the test server
func MakeRequest(t *testing.T, ts *testutil.TestServer, method, path string, body interface{}, headers map[string]string) *httptest.ResponseRecorder {
	w, err := ts.MakeRequest(method, path, body, headers)
	require.NoError(t, err)
	return w
}
// GetAuthHeader returns the Authorization header with the token
func GetAuthHeader(token string) map[string]string {
	return map[string]string{
		"Authorization": fmt.Sprintf("Bearer %s", token),
	}
}
// DecodeJSON decodes a JSON response into a struct
func DecodeJSON(resp *httptest.ResponseRecorder, v interface{}) error {
	return json.NewDecoder(resp.Body).Decode(v)
}
</file>

<file path="internal/usecase/audio_processor_service.go">
package usecase
import (
	"context"
	"encoding/json"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"metadatatool/internal/pkg/metrics"
	"os/exec"
	"strconv"
	"strings"
	"time"
	"github.com/google/uuid"
)
// AudioProcessorService implements audio processing functionality
type AudioProcessorService struct {
	storage    domain.StorageService
	tracks     domain.TrackRepository
	ai         domain.AIService
	processor  domain.AudioProcessor
	ffmpegPath string
}
// NewAudioProcessorService creates a new audio processor service
func NewAudioProcessorService(storage domain.StorageService, tracks domain.TrackRepository, ai domain.AIService, processor domain.AudioProcessor, ffmpegPath string) *AudioProcessorService {
	if ffmpegPath == "" {
		ffmpegPath = "ffmpeg" // Use from PATH if not specified
	}
	return &AudioProcessorService{
		storage:    storage,
		tracks:     tracks,
		ai:         ai,
		processor:  processor,
		ffmpegPath: ffmpegPath,
	}
}
// Process implements domain.AudioProcessor
func (s *AudioProcessorService) Process(ctx context.Context, file *domain.ProcessingAudioFile, options *domain.AudioProcessOptions) (*domain.AudioProcessResult, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("process"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("process", "started").Inc()
	// Extract technical metadata
	technicalData, err := s.extractTechnicalData(ctx, file.Path)
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("process", "technical_data_failed").Inc()
		return nil, fmt.Errorf("failed to extract technical data: %w", err)
	}
	// Create audio analysis object
	analysis := &domain.AudioAnalysis{
		AnalyzedAt:   time.Now(),
		Duration:     technicalData.duration,
		SampleCount:  technicalData.sampleCount,
		SampleRate:   technicalData.sampleRate,
		AnalyzerInfo: "ffmpeg",
	}
	// Perform audio analysis
	if err := s.analyzeAudio(ctx, file.Path, analysis); err != nil {
		metrics.AudioOpErrors.WithLabelValues("process", "analysis_failed").Inc()
		return nil, fmt.Errorf("failed to analyze audio: %w", err)
	}
	// Create complete metadata
	metadata := &domain.CompleteTrackMetadata{
		BasicTrackMetadata: domain.BasicTrackMetadata{
			Duration:  technicalData.duration,
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
		},
		Technical: domain.AudioTechnicalMetadata{
			SampleRate: technicalData.sampleRate,
			Channels:   technicalData.channels,
			Bitrate:    technicalData.bitrate,
			FileSize:   file.Size,
		},
	}
	result := &domain.AudioProcessResult{
		Metadata:     metadata,
		Analysis:     analysis,
		AnalyzerInfo: "ffmpeg",
	}
	metrics.AudioOps.WithLabelValues("process", "completed").Inc()
	return result, nil
}
// ProcessAudio is now a higher-level function that uses the Process method
func (s *AudioProcessorService) ProcessAudio(ctx context.Context, file *domain.ProcessingAudioFile, options *domain.AudioProcessOptions) (*domain.Track, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("complete_process"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("complete_process", "started").Inc()
	// Process audio file
	result, err := s.Process(ctx, file, options)
	if err != nil {
		metrics.AudioOpErrors.WithLabelValues("complete_process", "processing_failed").Inc()
		return nil, fmt.Errorf("failed to process audio: %w", err)
	}
	// Create track record with complete metadata
	track := &domain.Track{
		ID:          uuid.New().String(),
		StoragePath: options.FilePath,
		FileSize:    result.Metadata.Technical.FileSize,
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
		Metadata:    *result.Metadata,
	}
	// Save track to database
	if err := s.tracks.Create(ctx, track); err != nil {
		metrics.AudioOpErrors.WithLabelValues("complete_process", "db_save_failed").Inc()
		// Try to cleanup uploaded file
		_ = s.storage.Delete(ctx, options.FilePath)
		return nil, fmt.Errorf("failed to save track: %w", err)
	}
	// Trigger AI analysis in background
	go func() {
		bgCtx := context.Background()
		if err := s.ai.EnrichMetadata(bgCtx, track); err != nil {
			metrics.AudioOpErrors.WithLabelValues("complete_process", "ai_analysis_failed").Inc()
			// Log error but don't fail the request
			fmt.Printf("AI analysis failed for track %s: %v\n", track.ID, err)
		}
	}()
	metrics.AudioOps.WithLabelValues("complete_process", "completed").Inc()
	return track, nil
}
// BatchProcess processes multiple audio files
func (s *AudioProcessorService) BatchProcess(ctx context.Context, files []*domain.ProcessingAudioFile, options *domain.AudioProcessOptions) ([]*domain.Track, error) {
	timer := metrics.NewTimer(metrics.AudioOpDurations.WithLabelValues("batch_process"))
	defer timer.ObserveDuration()
	metrics.AudioOps.WithLabelValues("batch_process", "started").Inc()
	var tracks []*domain.Track
	for _, file := range files {
		track, err := s.ProcessAudio(ctx, file, options)
		if err != nil {
			metrics.AudioOpErrors.WithLabelValues("batch_process", "processing_failed").Inc()
			return nil, fmt.Errorf("failed to process audio file: %w", err)
		}
		tracks = append(tracks, track)
	}
	metrics.AudioOps.WithLabelValues("batch_process", "completed").Inc()
	return tracks, nil
}
// technicalData holds extracted technical metadata
type technicalData struct {
	duration    float64
	sampleRate  int
	sampleCount int64
	channels    int
	bitDepth    int
	bitrate     int
}
// ffprobeOutput represents the JSON output from ffprobe
type ffprobeOutput struct {
	Format struct {
		Duration string `json:"duration"`
		BitRate  string `json:"bit_rate"`
		Size     string `json:"size"`
		Format   string `json:"format_name"`
		Tags     map[string]string
	} `json:"format"`
	Streams []struct {
		CodecType  string `json:"codec_type"`
		SampleRate string `json:"sample_rate,omitempty"`
		Channels   int    `json:"channels,omitempty"`
		BitDepth   int    `json:"bits_per_sample,omitempty"`
		Duration   string `json:"duration"`
		Tags       map[string]string
	} `json:"streams"`
}
// extractTechnicalData extracts technical metadata using ffprobe
func (s *AudioProcessorService) extractTechnicalData(ctx context.Context, filepath string) (*technicalData, error) {
	cmd := exec.CommandContext(ctx, "ffprobe",
		"-v", "quiet",
		"-print_format", "json",
		"-show_format",
		"-show_streams",
		filepath,
	)
	output, err := cmd.Output()
	if err != nil {
		return nil, fmt.Errorf("ffprobe failed: %w", err)
	}
	var probe ffprobeOutput
	if err := json.Unmarshal(output, &probe); err != nil {
		return nil, fmt.Errorf("failed to parse ffprobe output: %w", err)
	}
	data := &technicalData{}
	// Parse duration
	if duration, err := strconv.ParseFloat(probe.Format.Duration, 64); err == nil {
		data.duration = duration
	}
	// Parse bitrate
	if bitrate, err := strconv.Atoi(probe.Format.BitRate); err == nil {
		data.bitrate = bitrate / 1000 // Convert to kbps
	}
	// Find audio stream
	for _, stream := range probe.Streams {
		if stream.CodecType == "audio" {
			// Parse sample rate
			if sampleRate, err := strconv.Atoi(stream.SampleRate); err == nil {
				data.sampleRate = sampleRate
			}
			data.channels = stream.Channels
			data.bitDepth = stream.BitDepth
			// Calculate sample count
			if data.duration > 0 && data.sampleRate > 0 {
				data.sampleCount = int64(data.duration * float64(data.sampleRate))
			}
			break
		}
	}
	return data, nil
}
// analyzeAudio performs audio analysis using ffmpeg
func (s *AudioProcessorService) analyzeAudio(ctx context.Context, filepath string, analysis *domain.AudioAnalysis) error {
	// Example analysis using ffmpeg's ebur128 filter
	cmd := exec.CommandContext(ctx, s.ffmpegPath,
		"-i", filepath,
		"-filter_complex", "ebur128=peak=true",
		"-f", "null",
		"-",
	)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("ffmpeg analysis failed: %w", err)
	}
	// Parse the output
	metrics := s.parseFFmpegOutput(output)
	// Update analysis with parsed values
	if loudness, ok := metrics["loudness"]; ok {
		analysis.Loudness = loudness
	}
	if energy, ok := metrics["energy"]; ok {
		analysis.Energy = energy
	}
	// Add example values for other metrics
	// In a real implementation, these would be calculated from the audio analysis
	analysis.BPM = 120.0        // Example value
	analysis.Key = "C"          // Example value
	analysis.Mode = "major"     // Example value
	analysis.Tempo = 120.0      // Example value
	analysis.Danceability = 0.8 // Example value
	analysis.Valence = 0.65     // Example value
	analysis.Complexity = 0.45  // Example value
	analysis.Intensity = 0.7    // Example value
	analysis.Mood = "energetic" // Example value
	// Add example segments
	analysis.Segments = []domain.AudioSegment{
		{
			Start:      0.0,
			Duration:   30.0,
			Loudness:   -14.0,
			Timbre:     []float64{1.0, 0.8, 0.6},
			Pitches:    []float64{0.9, 0.1, 0.3},
			Confidence: 0.95,
		},
		{
			Start:      30.0,
			Duration:   30.0,
			Loudness:   -13.5,
			Timbre:     []float64{0.9, 0.7, 0.5},
			Pitches:    []float64{0.8, 0.2, 0.4},
			Confidence: 0.92,
		},
	}
	return nil
}
// parseFFmpegOutput parses ffmpeg output to extract relevant data
func (s *AudioProcessorService) parseFFmpegOutput(output []byte) map[string]float64 {
	results := make(map[string]float64)
	lines := strings.Split(string(output), "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.Contains(line, "Integrated loudness:") {
			parts := strings.Fields(line)
			if len(parts) >= 3 {
				if val, err := strconv.ParseFloat(parts[2], 64); err == nil {
					results["loudness"] = val
				}
			}
		} else if strings.Contains(line, "Loudness range:") {
			parts := strings.Fields(line)
			if len(parts) >= 3 {
				if val, err := strconv.ParseFloat(parts[2], 64); err == nil {
					results["loudness_range"] = val
				}
			}
		} else if strings.Contains(line, "True peak:") {
			parts := strings.Fields(line)
			if len(parts) >= 3 {
				if val, err := strconv.ParseFloat(parts[2], 64); err == nil {
					results["true_peak"] = val
				}
			}
		}
	}
	// Calculate energy from loudness and peak
	if loudness, ok := results["loudness"]; ok {
		if peak, ok := results["true_peak"]; ok {
			// Simple energy calculation (this is just an example)
			energy := (loudness + 23.0) / 23.0 * (peak + 3.0) / 3.0
			results["energy"] = energy
		}
	}
	return results
}
</file>

<file path="internal/usecase/audio_service.go">
package usecase
import (
	"context"
	"fmt"
	"io"
	"metadatatool/internal/pkg/domain"
	"os"
	"path/filepath"
	"time"
)
// AudioService implements audio processing and storage functionality
type AudioService struct {
	processor domain.AudioProcessor
	storage   domain.StorageService
	tempDir   string
}
// NewAudioService creates a new audio service
func NewAudioService(processor domain.AudioProcessor, storage domain.StorageService, tempDir string) domain.AudioService {
	if tempDir == "" {
		tempDir = os.TempDir()
	}
	return &AudioService{
		processor: processor,
		storage:   storage,
		tempDir:   tempDir,
	}
}
// Process processes an audio file with the given options
func (s *AudioService) Process(ctx context.Context, file *domain.ProcessingAudioFile, options *domain.AudioProcessOptions) (*domain.AudioProcessResult, error) {
	// Validate file
	if err := s.validateFile(file); err != nil {
		return nil, fmt.Errorf("file validation failed: %w", err)
	}
	// Check format support
	if !s.isSupportedFormat(string(file.Format)) {
		return nil, fmt.Errorf("unsupported audio format: %s", file.Format)
	}
	// Create temp file for processing if needed
	var tempPath string
	if file.Content != nil {
		var err error
		tempPath, err = s.createTempFile(file.Content, filepath.Ext(file.Name))
		if err != nil {
			return nil, fmt.Errorf("failed to create temp file: %w", err)
		}
		defer s.cleanupTempFile(tempPath)
		file.Path = tempPath
	}
	// Process the file
	result, err := s.processor.Process(ctx, file, options)
	if err != nil {
		return nil, fmt.Errorf("audio processing failed: %w", err)
	}
	return result, nil
}
// Upload uploads an audio file to storage
func (s *AudioService) Upload(ctx context.Context, file *domain.StorageFile) error {
	// Validate file
	if err := s.validateStorageFile(file); err != nil {
		return fmt.Errorf("file validation failed: %w", err)
	}
	// Generate storage key if not provided
	if file.Key == "" {
		file.Key = generateStorageKey(file.Name)
	}
	// Upload to storage
	if err := s.storage.Upload(ctx, file); err != nil {
		return fmt.Errorf("storage upload failed: %w", err)
	}
	return nil
}
// Download downloads an audio file from storage
func (s *AudioService) Download(ctx context.Context, url string) (*domain.StorageFile, error) {
	// Download from storage
	file, err := s.storage.Download(ctx, url)
	if err != nil {
		return nil, fmt.Errorf("storage download failed: %w", err)
	}
	return file, nil
}
// Delete deletes an audio file from storage
func (s *AudioService) Delete(ctx context.Context, url string) error {
	// Delete from storage
	if err := s.storage.Delete(ctx, url); err != nil {
		return fmt.Errorf("storage delete failed: %w", err)
	}
	return nil
}
// GetURL gets the URL for an audio file in storage
func (s *AudioService) GetURL(ctx context.Context, id string) (string, error) {
	// Get URL from storage
	url, err := s.storage.GetURL(ctx, id)
	if err != nil {
		return "", fmt.Errorf("failed to get storage URL: %w", err)
	}
	return url, nil
}
// validateFile validates a processing audio file
func (s *AudioService) validateFile(file *domain.ProcessingAudioFile) error {
	if file == nil {
		return fmt.Errorf("file is nil")
	}
	if file.Name == "" {
		return fmt.Errorf("file name is empty")
	}
	if file.Size <= 0 {
		return fmt.Errorf("invalid file size: %d", file.Size)
	}
	if !file.Format.IsValid() {
		return fmt.Errorf("unsupported format: %s", file.Format)
	}
	return nil
}
// validateStorageFile validates a storage file
func (s *AudioService) validateStorageFile(file *domain.StorageFile) error {
	if file == nil {
		return fmt.Errorf("file is nil")
	}
	if file.Name == "" {
		return fmt.Errorf("file name is empty")
	}
	if file.Size <= 0 {
		return fmt.Errorf("invalid file size: %d", file.Size)
	}
	if file.Content == nil {
		return fmt.Errorf("file content is nil")
	}
	return nil
}
// isSupportedFormat checks if the audio format is supported
func (s *AudioService) isSupportedFormat(format string) bool {
	supportedFormats := map[string]bool{
		"mp3":  true,
		"wav":  true,
		"flac": true,
		"aac":  true,
		"ogg":  true,
		"m4a":  true,
	}
	return supportedFormats[format]
}
// createTempFile creates a temporary file for processing
func (s *AudioService) createTempFile(reader io.Reader, ext string) (string, error) {
	// Create temp file
	tmpFile, err := os.CreateTemp(s.tempDir, fmt.Sprintf("audio-*%s", ext))
	if err != nil {
		return "", fmt.Errorf("failed to create temp file: %w", err)
	}
	defer tmpFile.Close()
	// Copy content to temp file
	if _, err := io.Copy(tmpFile, reader); err != nil {
		os.Remove(tmpFile.Name()) // Clean up on error
		return "", fmt.Errorf("failed to write temp file: %w", err)
	}
	return tmpFile.Name(), nil
}
// cleanupTempFile removes a temporary file
func (s *AudioService) cleanupTempFile(path string) {
	if path != "" {
		os.Remove(path)
	}
}
// generateStorageKey generates a unique storage key for a file
func generateStorageKey(filename string) string {
	ext := filepath.Ext(filename)
	timestamp := time.Now().UTC().Format("20060102150405")
	return fmt.Sprintf("audio/%s/%s%s", timestamp[:8], timestamp, ext)
}
</file>

<file path="internal/usecase/audio_storage_service.go">
package usecase
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
)
// AudioStorageService handles audio file storage operations
type AudioStorageService struct {
	storage   domain.StorageService
	tracks    domain.TrackRepository
	processor domain.AudioProcessor
}
// NewAudioStorageService creates a new audio storage service
func NewAudioStorageService(storage domain.StorageService, tracks domain.TrackRepository, processor domain.AudioProcessor) domain.AudioService {
	return &AudioStorageService{
		storage:   storage,
		tracks:    tracks,
		processor: processor,
	}
}
// Process processes an audio file and returns the results
func (s *AudioStorageService) Process(ctx context.Context, file *domain.ProcessingAudioFile, options *domain.AudioProcessOptions) (*domain.AudioProcessResult, error) {
	result, err := s.processor.Process(ctx, file, options)
	if err != nil {
		return nil, fmt.Errorf("failed to process audio: %w", err)
	}
	return result, nil
}
// Upload stores an audio file and returns its URL
func (s *AudioStorageService) Upload(ctx context.Context, file *domain.StorageFile) error {
	if err := s.storage.Upload(ctx, file); err != nil {
		return fmt.Errorf("failed to upload file: %w", err)
	}
	return nil
}
// GetURL retrieves a pre-signed URL for an audio file
func (s *AudioStorageService) GetURL(ctx context.Context, id string) (string, error) {
	track, err := s.tracks.GetByID(ctx, id)
	if err != nil {
		return "", fmt.Errorf("failed to get track: %w", err)
	}
	if track == nil {
		return "", fmt.Errorf("track not found: %s", id)
	}
	return s.storage.GetURL(ctx, track.StoragePath)
}
// Delete deletes an audio file from storage
func (s *AudioStorageService) Delete(ctx context.Context, url string) error {
	if err := s.storage.Delete(ctx, url); err != nil {
		return fmt.Errorf("failed to delete file: %w", err)
	}
	return nil
}
// Download downloads an audio file from storage
func (s *AudioStorageService) Download(ctx context.Context, url string) (*domain.StorageFile, error) {
	file, err := s.storage.Download(ctx, url)
	if err != nil {
		return nil, fmt.Errorf("failed to download file: %w", err)
	}
	return file, nil
}
</file>

<file path="internal/usecase/auth_service_test.go">
package usecase
import (
	"context"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"testing"
	"time"
	"github.com/golang-jwt/jwt/v5"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
	"github.com/stretchr/testify/require"
)
// MockUserRepository is a mock implementation of domain.UserRepository
type MockUserRepository struct {
	mock.Mock
}
func (m *MockUserRepository) Create(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) GetByID(ctx context.Context, id string) (*domain.User, error) {
	args := m.Called(ctx, id)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByEmail(ctx context.Context, email string) (*domain.User, error) {
	args := m.Called(ctx, email)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) GetByAPIKey(ctx context.Context, apiKey string) (*domain.User, error) {
	args := m.Called(ctx, apiKey)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*domain.User), args.Error(1)
}
func (m *MockUserRepository) Update(ctx context.Context, user *domain.User) error {
	args := m.Called(ctx, user)
	return args.Error(0)
}
func (m *MockUserRepository) Delete(ctx context.Context, id string) error {
	args := m.Called(ctx, id)
	return args.Error(0)
}
func (m *MockUserRepository) List(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	args := m.Called(ctx, offset, limit)
	return args.Get(0).([]*domain.User), args.Error(1)
}
func (m *MockUserRepository) UpdateAPIKey(ctx context.Context, userID string, apiKey string) error {
	args := m.Called(ctx, userID, apiKey)
	return args.Error(0)
}
func setupAuthService() (*authService, *MockUserRepository) {
	userRepo := new(MockUserRepository)
	cfg := &config.AuthConfig{
		JWTSecret:       "test-secret",
		AccessTokenTTL:  time.Hour,
		RefreshTokenTTL: 24 * time.Hour,
	}
	service := NewAuthService(cfg, userRepo).(*authService)
	return service, userRepo
}
func createTestUser() *domain.User {
	return &domain.User{
		ID:       "test-id",
		Email:    "test@example.com",
		Password: "hashed-password",
		Name:     "Test User",
		Role:     domain.RoleUser,
	}
}
func TestAuthService_GenerateTokens(t *testing.T) {
	service, _ := setupAuthService()
	t.Run("successful token generation", func(t *testing.T) {
		user := createTestUser()
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		assert.NotEmpty(t, tokens.AccessToken)
		assert.NotEmpty(t, tokens.RefreshToken)
		// Verify access token
		claims, err := service.ValidateToken(tokens.AccessToken)
		require.NoError(t, err)
		assert.Equal(t, user.ID, claims.UserID)
		assert.Equal(t, user.Email, claims.Email)
		assert.Equal(t, user.Role, claims.Role)
		assert.NotEmpty(t, claims.Permissions)
	})
}
func TestAuthService_ValidateToken(t *testing.T) {
	service, _ := setupAuthService()
	user := createTestUser()
	t.Run("valid token", func(t *testing.T) {
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		claims, err := service.ValidateToken(tokens.AccessToken)
		assert.NoError(t, err)
		assert.Equal(t, user.ID, claims.UserID)
		assert.Equal(t, user.Email, claims.Email)
		assert.Equal(t, user.Role, claims.Role)
	})
	t.Run("expired token", func(t *testing.T) {
		// Create token that's already expired
		token := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{
			"user_id": user.ID,
			"email":   user.Email,
			"role":    user.Role,
			"exp":     time.Now().Add(-time.Hour).Unix(),
		})
		tokenString, err := token.SignedString([]byte(service.cfg.JWTSecret))
		require.NoError(t, err)
		claims, err := service.ValidateToken(tokenString)
		assert.Error(t, err)
		assert.Nil(t, claims)
	})
	t.Run("invalid token", func(t *testing.T) {
		claims, err := service.ValidateToken("invalid-token")
		assert.Error(t, err)
		assert.Nil(t, claims)
	})
}
func TestAuthService_RefreshToken(t *testing.T) {
	service, userRepo := setupAuthService()
	user := createTestUser()
	t.Run("successful refresh", func(t *testing.T) {
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		userRepo.On("GetByID", mock.Anything, user.ID).Return(user, nil)
		newTokens, err := service.RefreshToken(tokens.RefreshToken)
		assert.NoError(t, err)
		assert.NotEmpty(t, newTokens.AccessToken)
		assert.NotEmpty(t, newTokens.RefreshToken)
		assert.NotEqual(t, tokens.AccessToken, newTokens.AccessToken)
		assert.NotEqual(t, tokens.RefreshToken, newTokens.RefreshToken)
		userRepo.AssertExpectations(t)
	})
	t.Run("invalid refresh token", func(t *testing.T) {
		newTokens, err := service.RefreshToken("invalid-token")
		assert.Error(t, err)
		assert.Nil(t, newTokens)
	})
	t.Run("user not found", func(t *testing.T) {
		tokens, err := service.GenerateTokens(user)
		require.NoError(t, err)
		userRepo.On("GetByID", mock.Anything, user.ID).Return(nil, nil)
		newTokens, err := service.RefreshToken(tokens.RefreshToken)
		assert.Error(t, err)
		assert.Nil(t, newTokens)
		userRepo.AssertExpectations(t)
	})
}
func TestAuthService_HashPassword(t *testing.T) {
	service, _ := setupAuthService()
	t.Run("successful password hashing", func(t *testing.T) {
		password := "test-password"
		hash, err := service.HashPassword(password)
		assert.NoError(t, err)
		assert.NotEmpty(t, hash)
		assert.NotEqual(t, password, hash)
		// Verify password matches hash
		err = service.VerifyPassword(hash, password)
		assert.NoError(t, err)
	})
	t.Run("empty password", func(t *testing.T) {
		hash, err := service.HashPassword("")
		assert.Error(t, err)
		assert.Empty(t, hash)
	})
}
func TestAuthService_VerifyPassword(t *testing.T) {
	service, _ := setupAuthService()
	t.Run("correct password", func(t *testing.T) {
		password := "test-password"
		hash, err := service.HashPassword(password)
		require.NoError(t, err)
		err = service.VerifyPassword(hash, password)
		assert.NoError(t, err)
	})
	t.Run("incorrect password", func(t *testing.T) {
		password := "test-password"
		hash, err := service.HashPassword(password)
		require.NoError(t, err)
		err = service.VerifyPassword(hash, "wrong-password")
		assert.Error(t, err)
	})
	t.Run("invalid hash", func(t *testing.T) {
		err := service.VerifyPassword("invalid-hash", "test-password")
		assert.Error(t, err)
	})
}
func TestAuthService_HasPermission(t *testing.T) {
	service, _ := setupAuthService()
	testCases := []struct {
		name       string
		role       domain.Role
		permission domain.Permission
		hasAccess  bool
	}{
		{
			name:       "admin has all permissions",
			role:       domain.RoleAdmin,
			permission: domain.PermissionManageUsers,
			hasAccess:  true,
		},
		{
			name:       "user has basic permissions",
			role:       domain.RoleUser,
			permission: domain.PermissionReadTrack,
			hasAccess:  true,
		},
		{
			name:       "user cannot manage users",
			role:       domain.RoleUser,
			permission: domain.PermissionManageUsers,
			hasAccess:  false,
		},
		{
			name:       "guest can only read",
			role:       domain.RoleGuest,
			permission: domain.PermissionReadTrack,
			hasAccess:  true,
		},
		{
			name:       "guest cannot create",
			role:       domain.RoleGuest,
			permission: domain.PermissionCreateTrack,
			hasAccess:  false,
		},
	}
	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			hasAccess := service.HasPermission(tc.role, tc.permission)
			assert.Equal(t, tc.hasAccess, hasAccess)
		})
	}
}
func TestAuthService_GetPermissions(t *testing.T) {
	service, _ := setupAuthService()
	testCases := []struct {
		name             string
		role             domain.Role
		expectedContains []domain.Permission
		notExpected      []domain.Permission
	}{
		{
			name: "admin permissions",
			role: domain.RoleAdmin,
			expectedContains: []domain.Permission{
				domain.PermissionManageUsers,
				domain.PermissionCreateTrack,
				domain.PermissionDeleteTrack,
			},
			notExpected: nil,
		},
		{
			name: "user permissions",
			role: domain.RoleUser,
			expectedContains: []domain.Permission{
				domain.PermissionCreateTrack,
				domain.PermissionReadTrack,
			},
			notExpected: []domain.Permission{
				domain.PermissionManageUsers,
				domain.PermissionManageRoles,
			},
		},
		{
			name: "guest permissions",
			role: domain.RoleGuest,
			expectedContains: []domain.Permission{
				domain.PermissionReadTrack,
			},
			notExpected: []domain.Permission{
				domain.PermissionCreateTrack,
				domain.PermissionManageUsers,
			},
		},
	}
	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			permissions := service.GetPermissions(tc.role)
			// Check expected permissions are present
			for _, expected := range tc.expectedContains {
				assert.Contains(t, permissions, expected)
			}
			// Check unexpected permissions are not present
			for _, notExpected := range tc.notExpected {
				assert.NotContains(t, permissions, notExpected)
			}
		})
	}
}
func TestAuthService_GenerateAPIKey(t *testing.T) {
	service, _ := setupAuthService()
	t.Run("generate unique keys", func(t *testing.T) {
		key1, err := service.GenerateAPIKey()
		assert.NoError(t, err)
		assert.NotEmpty(t, key1)
		key2, err := service.GenerateAPIKey()
		assert.NoError(t, err)
		assert.NotEmpty(t, key2)
		assert.NotEqual(t, key1, key2)
	})
}
func TestAuthService_Login(t *testing.T) {
	service, userRepo := setupAuthService()
	tests := []struct {
		name          string
		email         string
		password      string
		mockUser      *domain.User
		mockError     error
		expectedError error
	}{
		{
			name:     "successful login",
			email:    "test@example.com",
			password: "password123",
			mockUser: &domain.User{
				ID:       "user123",
				Email:    "test@example.com",
				Password: "$2a$10$abcdefghijklmnopqrstuvwxyz",
			},
			mockError:     nil,
			expectedError: nil,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			userRepo.On("GetByEmail", mock.Anything, tt.email).Return(tt.mockUser, tt.mockError)
			// Call the service
			token, err := service.ValidateToken(tt.mockUser.Password)
			// Assert results
			if tt.expectedError != nil {
				assert.Error(t, err)
				assert.Equal(t, tt.expectedError, err)
			} else {
				assert.NoError(t, err)
				assert.NotNil(t, token)
			}
		})
	}
}
</file>

<file path="internal/usecase/auth_service.go">
package usecase
import (
	"context"
	"fmt"
	pkgconfig "metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/domain"
	"time"
	"github.com/golang-jwt/jwt/v5"
	"github.com/google/uuid"
	"golang.org/x/crypto/bcrypt"
)
type authService struct {
	cfg      *pkgconfig.AuthConfig
	userRepo domain.UserRepository
}
// NewAuthService creates a new authentication service
func NewAuthService(cfg *pkgconfig.AuthConfig, userRepo domain.UserRepository) domain.AuthService {
	return &authService{
		cfg:      cfg,
		userRepo: userRepo,
	}
}
// GenerateTokens creates a new pair of access and refresh tokens
func (s *authService) GenerateTokens(user *domain.User) (*domain.TokenPair, error) {
	// Get permissions for the user's role
	permissions := s.GetPermissions(user.Role)
	// Create access token with permissions
	accessToken := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{
		"user_id":     user.ID,
		"email":       user.Email,
		"role":        user.Role,
		"permissions": permissions,
		"exp":         time.Now().Add(s.cfg.AccessTokenTTL).Unix(),
	})
	accessTokenString, err := accessToken.SignedString([]byte(s.cfg.JWTSecret))
	if err != nil {
		return nil, fmt.Errorf("failed to sign access token: %w", err)
	}
	// Create refresh token
	refreshToken := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{
		"user_id": user.ID,
		"exp":     time.Now().Add(s.cfg.RefreshTokenTTL).Unix(),
	})
	refreshTokenString, err := refreshToken.SignedString([]byte(s.cfg.JWTSecret))
	if err != nil {
		return nil, fmt.Errorf("failed to sign refresh token: %w", err)
	}
	return &domain.TokenPair{
		AccessToken:  accessTokenString,
		RefreshToken: refreshTokenString,
	}, nil
}
// ValidateToken validates and parses a JWT token
func (s *authService) ValidateToken(tokenString string) (*domain.Claims, error) {
	token, err := jwt.Parse(tokenString, func(token *jwt.Token) (interface{}, error) {
		if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
			return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
		}
		return []byte(s.cfg.JWTSecret), nil
	})
	if err != nil {
		return nil, fmt.Errorf("failed to parse token: %w", err)
	}
	if claims, ok := token.Claims.(jwt.MapClaims); ok && token.Valid {
		uid, ok := claims["user_id"].(string)
		if !ok {
			return nil, fmt.Errorf("invalid token: missing user_id")
		}
		email, ok := claims["email"].(string)
		if !ok {
			return nil, fmt.Errorf("invalid token: missing email")
		}
		role, ok := claims["role"].(string)
		if !ok {
			return nil, fmt.Errorf("invalid token: missing role")
		}
		var permissions []domain.Permission
		if perms, ok := claims["permissions"].([]interface{}); ok {
			for _, p := range perms {
				if pStr, ok := p.(string); ok {
					permissions = append(permissions, domain.Permission(pStr))
				}
			}
		}
		return &domain.Claims{
			UserID:      uid,
			Email:       email,
			Role:        domain.Role(role),
			Permissions: permissions,
		}, nil
	}
	return nil, fmt.Errorf("invalid token claims")
}
// RefreshToken validates a refresh token and generates new token pair
func (s *authService) RefreshToken(refreshToken string) (*domain.TokenPair, error) {
	claims, err := s.ValidateToken(refreshToken)
	if err != nil {
		return nil, fmt.Errorf("invalid refresh token: %w", err)
	}
	ctx := context.Background()
	user, err := s.userRepo.GetByID(ctx, claims.UserID)
	if err != nil {
		return nil, fmt.Errorf("failed to get user: %w", err)
	}
	if user == nil {
		return nil, fmt.Errorf("user not found")
	}
	return s.GenerateTokens(user)
}
// HashPassword creates a bcrypt hash of the password
func (s *authService) HashPassword(password string) (string, error) {
	hashedBytes, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)
	if err != nil {
		return "", fmt.Errorf("failed to hash password: %w", err)
	}
	return string(hashedBytes), nil
}
// VerifyPassword checks if the provided password matches the hash
func (s *authService) VerifyPassword(hashedPassword, password string) error {
	return bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(password))
}
// GenerateAPIKey creates a new API key
func (s *authService) GenerateAPIKey() (string, error) {
	return uuid.NewString(), nil
}
// HasPermission checks if a role has a specific permission
func (s *authService) HasPermission(role domain.Role, permission domain.Permission) bool {
	permissions := domain.RolePermissions[role]
	for _, p := range permissions {
		if p == permission {
			return true
		}
	}
	return false
}
// GetPermissions returns all permissions for a role
func (s *authService) GetPermissions(role domain.Role) []domain.Permission {
	return domain.RolePermissions[role]
}
</file>

<file path="internal/usecase/auth_usecase.go">
package usecase
import (
	"context"
	"errors"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
)
// AuthUseCase handles authentication operations
type AuthUseCase struct {
	userRepo    domain.UserRepository
	sessionRepo domain.SessionStore
	authService domain.AuthService
}
// NewAuthUseCase creates a new auth use case
func NewAuthUseCase(userRepo domain.UserRepository, sessionRepo domain.SessionStore, authService domain.AuthService) *AuthUseCase {
	return &AuthUseCase{
		userRepo:    userRepo,
		sessionRepo: sessionRepo,
		authService: authService,
	}
}
// RegisterInput represents registration request data
type RegisterInput struct {
	Email    string      `json:"email"`
	Password string      `json:"password"`
	Role     domain.Role `json:"role"`
	Name     string      `json:"name"`
}
// Register creates a new user account
func (uc *AuthUseCase) Register(ctx context.Context, input RegisterInput) (*domain.User, error) {
	// Check if email already exists
	existingUser, err := uc.userRepo.GetByEmail(ctx, input.Email)
	if err != nil {
		return nil, fmt.Errorf("error checking existing user: %w", err)
	}
	if existingUser != nil {
		return nil, fmt.Errorf("email already registered")
	}
	// Hash password
	hashedPassword, err := uc.authService.HashPassword(input.Password)
	if err != nil {
		return nil, fmt.Errorf("error hashing password: %w", err)
	}
	// Create user
	now := time.Now()
	user := &domain.User{
		Email:     input.Email,
		Password:  hashedPassword,
		Role:      input.Role,
		Name:      input.Name,
		CreatedAt: now,
		UpdatedAt: now,
	}
	if err := uc.userRepo.Create(ctx, user); err != nil {
		return nil, fmt.Errorf("error creating user: %w", err)
	}
	return user, nil
}
// LoginInput represents login request data
type LoginInput struct {
	Email    string `json:"email"`
	Password string `json:"password"`
}
// LoginOutput contains the result of a successful login
type LoginOutput struct {
	AccessToken  string
	RefreshToken string
	User         *domain.User
	Session      *domain.Session
}
// Login authenticates a user and creates a session
func (uc *AuthUseCase) Login(ctx context.Context, input LoginInput) (*LoginOutput, error) {
	// Get user by email
	user, err := uc.userRepo.GetByEmail(ctx, input.Email)
	if err != nil {
		if errors.Is(err, domain.ErrUserNotFound) {
			return nil, domain.ErrInvalidCredentials
		}
		return nil, fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return nil, domain.ErrInvalidCredentials
	}
	// Check password
	if err := uc.authService.VerifyPassword(user.Password, input.Password); err != nil {
		return nil, domain.ErrInvalidCredentials
	}
	// Generate tokens
	tokens, err := uc.authService.GenerateTokens(user)
	if err != nil {
		return nil, fmt.Errorf("error generating tokens: %w", err)
	}
	// Create session
	session := &domain.Session{
		UserID:    user.ID,
		Token:     tokens.AccessToken,
		ExpiresAt: time.Now().Add(24 * time.Hour),
		CreatedAt: time.Now(),
	}
	if err := uc.sessionRepo.Create(ctx, session); err != nil {
		return nil, fmt.Errorf("error creating session: %w", err)
	}
	return &LoginOutput{
		AccessToken:  tokens.AccessToken,
		RefreshToken: tokens.RefreshToken,
		User:         user,
		Session:      session,
	}, nil
}
// CreateSession creates a new session
func (uc *AuthUseCase) CreateSession(ctx context.Context, session *domain.Session) error {
	return uc.sessionRepo.Create(ctx, session)
}
// Logout ends a user session
func (uc *AuthUseCase) Logout(ctx context.Context, sessionID string) error {
	return uc.sessionRepo.Delete(ctx, sessionID)
}
// ValidateToken validates a JWT token and returns the associated user
func (uc *AuthUseCase) ValidateToken(ctx context.Context, tokenString string) (*domain.User, error) {
	claims, err := uc.authService.ValidateToken(tokenString)
	if err != nil {
		return nil, fmt.Errorf("error validating token: %w", err)
	}
	// Get user
	user, err := uc.userRepo.GetByID(ctx, claims.UserID)
	if err != nil {
		return nil, fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return nil, fmt.Errorf("user not found")
	}
	return user, nil
}
// GetUserSessions retrieves all active sessions for a user
func (uc *AuthUseCase) GetUserSessions(ctx context.Context, userID string) ([]*domain.Session, error) {
	return uc.sessionRepo.GetUserSessions(ctx, userID)
}
// RevokeSession revokes a specific session
func (uc *AuthUseCase) RevokeSession(ctx context.Context, sessionID string) error {
	return uc.sessionRepo.Delete(ctx, sessionID)
}
// RevokeAllSessions revokes all sessions for a user
func (uc *AuthUseCase) RevokeAllSessions(ctx context.Context, userID string) error {
	return uc.sessionRepo.DeleteUserSessions(ctx, userID)
}
// RefreshToken refreshes an access token using a refresh token
func (uc *AuthUseCase) RefreshToken(ctx context.Context, refreshToken string) (string, string, *domain.User, error) {
	// Validate refresh token
	claims, err := uc.authService.ValidateToken(refreshToken)
	if err != nil {
		return "", "", nil, fmt.Errorf("invalid refresh token: %w", err)
	}
	// Get user
	user, err := uc.userRepo.GetByID(ctx, claims.UserID)
	if err != nil {
		return "", "", nil, fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return "", "", nil, domain.ErrUserNotFound
	}
	// Generate new tokens
	tokens, err := uc.authService.GenerateTokens(user)
	if err != nil {
		return "", "", nil, fmt.Errorf("error generating tokens: %w", err)
	}
	return tokens.AccessToken, tokens.RefreshToken, user, nil
}
// GetSession retrieves a session by ID
func (uc *AuthUseCase) GetSession(ctx context.Context, sessionID string) (*domain.Session, error) {
	session, err := uc.sessionRepo.Get(ctx, sessionID)
	if err != nil {
		return nil, fmt.Errorf("error getting session: %w", err)
	}
	if session == nil {
		return nil, fmt.Errorf("session not found")
	}
	return session, nil
}
// GenerateAPIKey generates a new API key for a user
func (uc *AuthUseCase) GenerateAPIKey(ctx context.Context, userID string) (string, error) {
	// Generate a new API key
	apiKey, err := uc.authService.GenerateAPIKey()
	if err != nil {
		return "", fmt.Errorf("failed to generate API key: %w", err)
	}
	// Update user with new API key
	if err := uc.userRepo.UpdateAPIKey(ctx, userID, apiKey); err != nil {
		return "", fmt.Errorf("failed to update API key: %w", err)
	}
	return apiKey, nil
}
</file>

<file path="internal/usecase/cache_service.go">
package usecase
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/config"
	"metadatatool/internal/pkg/metrics"
	"time"
	"github.com/redis/go-redis/v9"
)
// CacheService handles caching operations
type CacheService struct {
	client *redis.Client
	cfg    *config.RedisConfig
}
// NewCacheService creates a new cache service
func NewCacheService(client *redis.Client, cfg *config.RedisConfig) *CacheService {
	return &CacheService{
		client: client,
		cfg:    cfg,
	}
}
// Get retrieves a value from cache
func (s *CacheService) Get(ctx context.Context, key string) ([]byte, error) {
	val, err := s.client.Get(ctx, key).Bytes()
	if err == redis.Nil {
		metrics.CacheMisses.WithLabelValues("redis").Inc()
		return nil, nil
	} else if err != nil {
		return nil, fmt.Errorf("failed to get from cache: %w", err)
	}
	metrics.CacheHits.WithLabelValues("redis").Inc()
	return val, nil
}
// Set stores a value in cache
func (s *CacheService) Set(ctx context.Context, key string, value []byte, expiration time.Duration) error {
	err := s.client.Set(ctx, key, value, expiration).Err()
	if err != nil {
		return fmt.Errorf("failed to set in cache: %w", err)
	}
	return nil
}
// Delete removes a value from cache
func (s *CacheService) Delete(ctx context.Context, key string) error {
	err := s.client.Del(ctx, key).Err()
	if err != nil {
		return fmt.Errorf("failed to delete from cache: %w", err)
	}
	return nil
}
// PreWarm pre-warms the cache with frequently accessed data
func (s *CacheService) PreWarm(ctx context.Context, keys []string) error {
	pipe := s.client.Pipeline()
	for _, key := range keys {
		pipe.Get(ctx, key)
	}
	_, err := pipe.Exec(ctx)
	if err != nil && err != redis.Nil {
		return fmt.Errorf("failed to pre-warm cache: %w", err)
	}
	return nil
}
</file>

<file path="internal/usecase/ddex_service.go">
package usecase
import (
	"context"
	"encoding/xml"
	"fmt"
	"math"
	"metadatatool/internal/pkg/domain"
	"regexp"
	"strings"
	"time"
	"github.com/google/uuid"
)
type ddexService struct {
	schemaValidator SchemaValidator
	config          *DDEXConfig
}
// DDEXConfig holds configuration for the DDEX service
type DDEXConfig struct {
	MessageSender    string
	MessageRecipient string
	SchemaPath       string
	ValidateSchema   bool
}
// SchemaValidator defines the interface for XML schema validation
type SchemaValidator interface {
	ValidateAgainstSchema(xmlData []byte, schemaPath string) error
}
// NewDDEXService creates a new DDEX service instance
func NewDDEXService(validator SchemaValidator, config *DDEXConfig) domain.DDEXService {
	if config == nil {
		config = &DDEXConfig{
			MessageSender:    "MetadataTool",
			MessageRecipient: "DSP",
			ValidateSchema:   false,
		}
	}
	return &ddexService{
		schemaValidator: validator,
		config:          config,
	}
}
// ValidateTrack validates track metadata against DDEX schema
func (s *ddexService) ValidateTrack(ctx context.Context, track *domain.Track) (bool, []string) {
	var validationErrors []string
	// Required fields validation
	if err := s.validateRequiredFields(track); err != nil {
		validationErrors = append(validationErrors, err...)
	}
	// Format validation
	if err := s.validateFormats(track); err != nil {
		validationErrors = append(validationErrors, err...)
	}
	// Business rules validation
	if err := s.validateBusinessRules(track); err != nil {
		validationErrors = append(validationErrors, err...)
	}
	// Schema validation if enabled
	if s.config.ValidateSchema {
		xmlData, err := s.ExportTrack(ctx, track)
		if err != nil {
			validationErrors = append(validationErrors, fmt.Sprintf("Failed to generate XML: %v", err))
		} else if err := s.schemaValidator.ValidateAgainstSchema([]byte(xmlData), s.config.SchemaPath); err != nil {
			validationErrors = append(validationErrors, fmt.Sprintf("Schema validation failed: %v", err))
		}
	}
	return len(validationErrors) == 0, validationErrors
}
func (s *ddexService) validateRequiredFields(track *domain.Track) []string {
	var errors []string
	// Required field checks
	if track.ISRC() == "" {
		errors = append(errors, "ISRC is required")
	}
	if track.Title() == "" {
		errors = append(errors, "Title is required")
	}
	if track.Artist() == "" {
		errors = append(errors, "Artist is required")
	}
	if track.Label() == "" {
		errors = append(errors, "Label is required")
	}
	if track.Territory() == "" {
		errors = append(errors, "Territory is required")
	}
	return errors
}
func (s *ddexService) validateFormats(track *domain.Track) []string {
	var errors []string
	// ISRC format (CC-XXX-YY-NNNNN)
	if isrc := track.ISRC(); isrc != "" {
		isrcPattern := regexp.MustCompile(`^[A-Z]{2}[A-Z0-9]{3}\d{7}$`)
		if !isrcPattern.MatchString(isrc) {
			errors = append(errors, "Invalid ISRC format (should be CC-XXX-YY-NNNNN)")
		}
	}
	// Territory format (ISO 3166-1 alpha-2)
	if territory := track.Territory(); territory != "" {
		territoryPattern := regexp.MustCompile(`^[A-Z]{2}$`)
		if !territoryPattern.MatchString(territory) {
			errors = append(errors, "Territory must be a valid ISO 3166-1 alpha-2 code")
		}
	}
	// Audio format validation
	if format := track.AudioFormat(); format != "" {
		validFormats := map[string]bool{
			"AAC":  true,
			"MP3":  true,
			"FLAC": true,
			"WAV":  true,
		}
		if !validFormats[strings.ToUpper(format)] {
			errors = append(errors, "Unsupported audio format")
		}
	}
	return errors
}
func (s *ddexService) validateBusinessRules(track *domain.Track) []string {
	var errors []string
	now := time.Now()
	// Year validation
	if year := track.Year(); year != 0 {
		if year < 1900 || year > now.Year()+1 {
			errors = append(errors, "Year must be between 1900 and next year")
		}
	}
	// Duration validation
	if duration := track.Duration(); duration > 0 {
		if duration < 1 || duration > 7200 { // Max 2 hours
			errors = append(errors, "Duration must be between 1 second and 2 hours")
		}
	}
	// Technical metadata validation
	if bitrate := track.Bitrate(); bitrate > 0 {
		if bitrate < 32 || bitrate > 1411 {
			errors = append(errors, "Bitrate must be between 32 and 1411 kbps")
		}
	}
	if sampleRate := track.SampleRate(); sampleRate > 0 {
		validRates := map[int]bool{
			44100: true,
			48000: true,
			88200: true,
			96000: true,
		}
		if !validRates[sampleRate] {
			errors = append(errors, "Invalid sample rate")
		}
	}
	return errors
}
// ExportTrack exports a single track to DDEX format
func (s *ddexService) ExportTrack(ctx context.Context, track *domain.Track) (string, error) {
	// Create DDEX ERN 4.3 message
	message := s.createERNMessage([]*domain.Track{track})
	// Marshal to XML
	output, err := xml.MarshalIndent(message, "", "  ")
	if err != nil {
		return "", fmt.Errorf("failed to marshal ERN: %w", err)
	}
	// Add XML header and schema references
	xmlHeader := []byte(xml.Header)
	schemaRef := []byte(`<?xml-model href="http://ddex.net/xml/ern/43/release-notification.xsd" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>`)
	result := append(xmlHeader, append(schemaRef, output...)...)
	return string(result), nil
}
// ExportTracks exports multiple tracks to DDEX format
func (s *ddexService) ExportTracks(ctx context.Context, tracks []*domain.Track) (string, error) {
	// Validate all tracks first
	for _, track := range tracks {
		if valid, errors := s.ValidateTrack(ctx, track); !valid {
			return "", fmt.Errorf("track %s validation failed: %v", track.ID, errors)
		}
	}
	// Create DDEX ERN 4.3 message
	message := s.createERNMessage(tracks)
	// Marshal to XML
	output, err := xml.MarshalIndent(message, "", "  ")
	if err != nil {
		return "", fmt.Errorf("failed to marshal ERN: %w", err)
	}
	// Add XML header and schema references
	xmlHeader := []byte(xml.Header)
	schemaRef := []byte(`<?xml-model href="http://ddex.net/xml/ern/43/release-notification.xsd" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>`)
	result := append(xmlHeader, append(schemaRef, output...)...)
	return string(result), nil
}
// createERNMessage creates a DDEX ERN 4.3 message from tracks
func (s *ddexService) createERNMessage(tracks []*domain.Track) *domain.ERNMessage {
	messageId := uuid.New().String()
	now := time.Now().UTC().Format(time.RFC3339)
	// Create message
	message := &domain.ERNMessage{
		MessageHeader: domain.MessageHeader{
			MessageID:              messageId,
			MessageSender:          s.config.MessageSender,
			MessageRecipient:       s.config.MessageRecipient,
			MessageCreatedDateTime: now,
		},
	}
	// Add resources
	var soundRecordings []domain.SoundRecording
	for _, track := range tracks {
		// Convert duration to integer seconds
		durationSecs := int(math.Round(track.Duration()))
		soundRecording := domain.SoundRecording{
			ISRC: track.ISRC(),
			Title: domain.Title{
				TitleText: track.Title(),
			},
			Duration: fmt.Sprintf("PT%dS", durationSecs),
			TechnicalDetails: domain.TechnicalDetails{
				TechnicalResourceDetailsReference: track.ID,
				Audio: domain.Audio{
					Format:     strings.ToUpper(track.AudioFormat()),
					BitRate:    track.Bitrate(),
					SampleRate: track.SampleRate(),
				},
			},
			SoundRecordingType: "MusicalWorkSoundRecording",
			ResourceReference:  track.ID,
		}
		soundRecordings = append(soundRecordings, soundRecording)
	}
	message.ResourceList.SoundRecordings = soundRecordings
	// Add releases
	var releases []domain.Release
	for _, track := range tracks {
		release := domain.Release{
			ReleaseID: domain.ReleaseID{
				ICPN: track.ID,
			},
			ReferenceTitle: domain.Title{
				TitleText: track.Title(),
			},
			ReleaseType: "Single",
		}
		releases = append(releases, release)
	}
	message.ReleaseList.Releases = releases
	// Add deals
	var deals []domain.ReleaseDeal
	for _, track := range tracks {
		deal := domain.ReleaseDeal{
			DealReleaseReference: track.ID,
			Deal: domain.Deal{
				Territory: domain.Territory{
					TerritoryCode: track.Territory(),
				},
				DealTerms: domain.DealTerms{
					CommercialModelType: "PayAsYouGoModel",
					Usage: domain.Usage{
						UseType: "OnDemandStream",
					},
				},
			},
		}
		deals = append(deals, deal)
	}
	message.DealList.ReleaseDeals = deals
	return message
}
</file>

<file path="internal/usecase/track_usecase.go">
package usecase
import (
	"context"
	"fmt"
	"io"
	"time"
	"metadatatool/internal/pkg/domain"
)
// TrackFilter represents filter criteria for listing tracks
type TrackFilter struct {
	Title  string
	Artist string
	Album  string
	Genre  string
	ISRC   string
	Offset int
	Limit  int
}
type TrackUseCase struct {
	trackRepo      domain.TrackRepository
	storageService domain.StorageService
	aiService      domain.AIService
	queueService   domain.QueueService
}
func NewTrackUseCase(
	trackRepo domain.TrackRepository,
	storageService domain.StorageService,
	aiService domain.AIService,
	queueService domain.QueueService,
) *TrackUseCase {
	return &TrackUseCase{
		trackRepo:      trackRepo,
		storageService: storageService,
		aiService:      aiService,
		queueService:   queueService,
	}
}
func (u *TrackUseCase) CreateTrack(ctx context.Context, track *domain.Track, audioData io.Reader) error {
	// Upload audio file
	if err := u.storageService.UploadAudio(ctx, audioData, track.ID); err != nil {
		return fmt.Errorf("failed to upload audio: %w", err)
	}
	// Save track to database
	if err := u.trackRepo.Create(ctx, track); err != nil {
		return fmt.Errorf("failed to create track: %w", err)
	}
	// Queue track for processing
	msg := &domain.Message{
		ID:        track.ID,
		Type:      "track_processing",
		Data:      map[string]interface{}{"track_id": track.ID},
		Status:    domain.MessageStatusPending,
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
	}
	if err := u.queueService.Publish(ctx, "track_processing", msg, domain.PriorityMedium); err != nil {
		return fmt.Errorf("failed to queue track for processing: %w", err)
	}
	return nil
}
func (u *TrackUseCase) GetTrack(ctx context.Context, id string) (*domain.Track, error) {
	track, err := u.trackRepo.GetByID(ctx, id)
	if err != nil {
		return nil, fmt.Errorf("failed to get track: %w", err)
	}
	return track, nil
}
func (u *TrackUseCase) UpdateTrack(ctx context.Context, track *domain.Track) error {
	if err := u.trackRepo.Update(ctx, track); err != nil {
		return fmt.Errorf("failed to update track: %w", err)
	}
	return nil
}
func (u *TrackUseCase) DeleteTrack(ctx context.Context, id string) error {
	track, err := u.trackRepo.GetByID(ctx, id)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	// Delete audio file
	if err := u.storageService.DeleteAudio(ctx, track.StoragePath); err != nil {
		return fmt.Errorf("failed to delete audio: %w", err)
	}
	// Delete track from database
	if err := u.trackRepo.Delete(ctx, id); err != nil {
		return fmt.Errorf("failed to delete track: %w", err)
	}
	return nil
}
func (u *TrackUseCase) ListTracks(ctx context.Context, filter TrackFilter) ([]*domain.Track, error) {
	// Convert our filter to map[string]interface{}
	filterMap := map[string]interface{}{
		"title":  filter.Title,
		"artist": filter.Artist,
		"album":  filter.Album,
		"genre":  filter.Genre,
		"isrc":   filter.ISRC,
	}
	// Get tracks from repository with pagination
	tracks, err := u.trackRepo.List(ctx, filterMap, filter.Offset, filter.Limit)
	if err != nil {
		return nil, fmt.Errorf("failed to list tracks: %w", err)
	}
	return tracks, nil
}
func (u *TrackUseCase) ValidateERN(ctx context.Context, data []byte) error {
	// TODO: Implement ERN validation
	return nil
}
func (u *TrackUseCase) ImportERN(ctx context.Context, data []byte) error {
	// TODO: Implement ERN import
	return nil
}
func (u *TrackUseCase) ExportERN(ctx context.Context, trackIDs []string) ([]byte, error) {
	// TODO: Implement ERN export
	return nil, nil
}
func (u *TrackUseCase) GetAudioURL(ctx context.Context, id string) (string, error) {
	track, err := u.trackRepo.GetByID(ctx, id)
	if err != nil {
		return "", fmt.Errorf("failed to get track: %w", err)
	}
	// Generate signed URL for audio file
	signedURL, err := u.storageService.GetSignedURL(ctx, track.StoragePath, 1*time.Hour)
	if err != nil {
		return "", fmt.Errorf("failed to generate signed URL: %w", err)
	}
	return signedURL, nil
}
func (u *TrackUseCase) UploadAudio(ctx context.Context, id string, audioData io.Reader) error {
	track, err := u.trackRepo.GetByID(ctx, id)
	if err != nil {
		return fmt.Errorf("failed to get track: %w", err)
	}
	// Delete old audio file if it exists
	if track.StoragePath != "" {
		if err := u.storageService.DeleteAudio(ctx, track.StoragePath); err != nil {
			return fmt.Errorf("failed to delete old audio: %w", err)
		}
	}
	// Upload new audio file
	if err := u.storageService.UploadAudio(ctx, audioData, id); err != nil {
		return fmt.Errorf("failed to upload audio: %w", err)
	}
	// Update track with new storage path
	track.StoragePath = id
	if err := u.trackRepo.Update(ctx, track); err != nil {
		return fmt.Errorf("failed to update track: %w", err)
	}
	return nil
}
</file>

<file path="internal/usecase/user_usecase.go">
package usecase
import (
	"context"
	"fmt"
	"metadatatool/internal/pkg/domain"
	"time"
	"github.com/google/uuid"
)
// UserUseCase handles user operations
type UserUseCase struct {
	userRepo domain.UserRepository
}
// NewUserUseCase creates a new user use case
func NewUserUseCase(userRepo domain.UserRepository) *UserUseCase {
	return &UserUseCase{
		userRepo: userRepo,
	}
}
// GenerateAPIKey generates a new API key for a user
func (uc *UserUseCase) GenerateAPIKey(ctx context.Context, userID string) (string, error) {
	user, err := uc.userRepo.GetByID(ctx, userID)
	if err != nil {
		return "", fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return "", fmt.Errorf("user not found")
	}
	// Generate new API key
	apiKey := uuid.New().String()
	user.APIKey = apiKey
	user.UpdatedAt = time.Now()
	if err := uc.userRepo.Update(ctx, user); err != nil {
		return "", fmt.Errorf("error updating user: %w", err)
	}
	return apiKey, nil
}
// RevokeAPIKey revokes a user's API key
func (uc *UserUseCase) RevokeAPIKey(ctx context.Context, userID string) error {
	user, err := uc.userRepo.GetByID(ctx, userID)
	if err != nil {
		return fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return fmt.Errorf("user not found")
	}
	user.APIKey = ""
	user.UpdatedAt = time.Now()
	if err := uc.userRepo.Update(ctx, user); err != nil {
		return fmt.Errorf("error updating user: %w", err)
	}
	return nil
}
// GetUser retrieves a user by ID
func (uc *UserUseCase) GetUser(ctx context.Context, userID string) (*domain.User, error) {
	user, err := uc.userRepo.GetByID(ctx, userID)
	if err != nil {
		return nil, fmt.Errorf("error getting user: %w", err)
	}
	if user == nil {
		return nil, fmt.Errorf("user not found")
	}
	return user, nil
}
// ListUsers retrieves a list of users with pagination
func (uc *UserUseCase) ListUsers(ctx context.Context, offset, limit int) ([]*domain.User, error) {
	users, err := uc.userRepo.List(ctx, offset, limit)
	if err != nil {
		return nil, fmt.Errorf("error listing users: %w", err)
	}
	return users, nil
}
// UpdateUser updates a user's information
func (uc *UserUseCase) UpdateUser(ctx context.Context, user *domain.User) error {
	existingUser, err := uc.userRepo.GetByID(ctx, user.ID)
	if err != nil {
		return fmt.Errorf("error getting user: %w", err)
	}
	if existingUser == nil {
		return fmt.Errorf("user not found")
	}
	user.UpdatedAt = time.Now()
	if err := uc.userRepo.Update(ctx, user); err != nil {
		return fmt.Errorf("error updating user: %w", err)
	}
	return nil
}
// DeleteUser deletes a user
func (uc *UserUseCase) DeleteUser(ctx context.Context, userID string) error {
	if err := uc.userRepo.Delete(ctx, userID); err != nil {
		return fmt.Errorf("error deleting user: %w", err)
	}
	return nil
}
</file>

<file path="metadata/metadata/domain/ddex.go">
package domain
import "encoding/xml"
// DDEXService defines the interface for DDEX operations
type DDEXService interface {
	ValidateMetadata(track *Track) (bool, []string, error)
	ExportERN(track *Track) ([]byte, error)
	BatchExportERN(tracks []*Track) ([]byte, error)
}
// DDEXERN43 represents the DDEX ERN 4.3 format structure
type DDEXERN43 struct {
	XMLName                xml.Name `xml:"ern:NewReleaseMessage"`
	MessageHeader          MessageHeader
	ResourceList           ResourceList
	ReleaseList            ReleaseList
	DealList               DealList
	XMLNs                  string `xml:"xmlns,attr"`
	XMLNsErn               string `xml:"xmlns:ern,attr"`
	MessageSchemaVersionId string `xml:"MessageSchemaVersionId,attr"`
}
// MessageHeader contains DDEX message metadata
type MessageHeader struct {
	MessageId              string `xml:"MessageId"`
	MessageSender          string `xml:"MessageSender"`
	MessageRecipient       string `xml:"MessageRecipient"`
	MessageCreatedDateTime string `xml:"MessageCreatedDateTime"`
	MessageControlType     string `xml:"MessageControlType"`
}
// ResourceList contains sound recording details
type ResourceList struct {
	SoundRecording []SoundRecording `xml:"SoundRecording"`
}
// SoundRecording represents a single audio track
type SoundRecording struct {
	SoundRecordingType  string           `xml:"SoundRecordingType"`
	SoundRecordingId    ResourceId       `xml:"SoundRecordingId"`
	ResourceReference   string           `xml:"ResourceReference"`
	ReferenceTitle      Title            `xml:"ReferenceTitle"`
	Duration            string           `xml:"Duration"`
	ParentalWarningType string           `xml:"ParentalWarningType"`
	TechnicalDetails    TechnicalDetails `xml:"TechnicalDetails"`
}
// ResourceId contains identifiers like ISRC
type ResourceId struct {
	ISRC string `xml:"ISRC"`
}
// Title contains track title information
type Title struct {
	TitleText string `xml:"TitleText"`
}
// TechnicalDetails contains audio specifications
type TechnicalDetails struct {
	TechnicalResourceDetailsReference string `xml:"TechnicalResourceDetailsReference"`
	Audio                             Audio  `xml:"Audio"`
}
// Audio contains audio-specific metadata
type Audio struct {
	AudioCodec string `xml:"AudioCodec"`
	Bitrate    int    `xml:"Bitrate"`
}
// ReleaseList contains release information
type ReleaseList struct {
	Release []Release `xml:"Release"`
}
// Release represents a music release
type Release struct {
	ReleaseId      ReleaseId `xml:"ReleaseId"`
	ReferenceTitle Title     `xml:"ReferenceTitle"`
	ReleaseType    string    `xml:"ReleaseType"`
}
// ReleaseId contains release identifiers
type ReleaseId struct {
	ICPN string `xml:"ICPN"`
}
// DealList contains licensing and territory information
type DealList struct {
	ReleaseDeal []ReleaseDeal `xml:"ReleaseDeal"`
}
// ReleaseDeal represents licensing terms
type ReleaseDeal struct {
	DealReleaseReference string `xml:"DealReleaseReference"`
	Deal                 Deal   `xml:"Deal"`
}
// Deal contains territory and usage terms
type Deal struct {
	Territory Territory `xml:"Territory"`
	DealTerms DealTerms `xml:"DealTerms"`
}
// Territory specifies geographical regions
type Territory struct {
	TerritoryCode string `xml:"TerritoryCode"`
}
// DealTerms contains usage rights and restrictions
type DealTerms struct {
	CommercialModelType string `xml:"CommercialModelType"`
	Usage               Usage  `xml:"Usage"`
}
// Usage defines how content can be used
type Usage struct {
	UseType string `xml:"UseType"`
}
</file>

<file path="metadata/metadata/domain/track.go">
package domain
import (
	"context"
	"time"
)
// Track represents a music track entity
type Track struct {
	ID        string    `json:"id"`
	Title     string    `json:"title"`
	Artist    string    `json:"artist"`
	Album     string    `json:"album,omitempty"`
	Genre     string    `json:"genre,omitempty"`
	Duration  float64   `json:"duration"`
	Metadata  Metadata  `json:"metadata"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
}
// Metadata represents additional track metadata
type Metadata struct {
	ISRC         string            `json:"isrc,omitempty"`
	ISWC         string            `json:"iswc,omitempty"`
	BPM          float64           `json:"bpm,omitempty"`
	Key          string            `json:"key,omitempty"`
	Mood         string            `json:"mood,omitempty"`
	Labels       []string          `json:"labels,omitempty"`
	AITags       []string          `json:"ai_tags,omitempty"`
	Confidence   float64           `json:"confidence,omitempty"`
	ModelVersion string            `json:"model_version,omitempty"`
	CustomFields map[string]string `json:"custom_fields,omitempty"`
}
// TrackRepository defines the interface for track data persistence
type TrackRepository interface {
	Create(ctx context.Context, track *Track) error
	GetByID(ctx context.Context, id string) (*Track, error)
	Update(ctx context.Context, track *Track) error
	Delete(ctx context.Context, id string) error
	List(ctx context.Context, offset, limit int) ([]*Track, error)
}
// Constants for storage paths
const (
	StoragePathAudio = "audio/"
	CacheKeyTrack    = "track:%s"
)
</file>

<file path="metadata/metadata/domain/user.go">
package domain
import (
	"time"
)
// Role constants
const (
	RoleAdmin  = "admin"
	RoleUser   = "user"
	RoleGuest  = "guest"
	RoleSystem = "system"
)
// User represents a system user with role-based permissions
type User struct {
	ID       string `json:"id"`
	Email    string `json:"email"`
	Password string `json:"-"` // Never expose password in JSON
	Name     string `json:"name"`
	Role     string `json:"role"`
	Company  string `json:"company"`
	APIKey   string `json:"api_key,omitempty"`
	// Subscription and usage tracking
	Plan           string    `json:"plan"` // free, pro, enterprise
	TrackQuota     int       `json:"track_quota"`
	TracksUsed     int       `json:"tracks_used"`
	QuotaResetDate time.Time `json:"quota_reset_date"`
	// Compliance and auditing
	CreatedAt   time.Time  `json:"created_at"`
	UpdatedAt   time.Time  `json:"updated_at"`
	DeletedAt   *time.Time `json:"deleted_at,omitempty"`
	LastLoginAt time.Time  `json:"last_login_at"`
}
// UserRepository defines the interface for user data persistence
type UserRepository interface {
	Create(user *User) error
	GetByID(id string) (*User, error)
	GetByEmail(email string) (*User, error)
	GetByAPIKey(apiKey string) (*User, error)
	Update(user *User) error
	Delete(id string) error
	List(offset, limit int) ([]*User, error)
}
// AuthService defines the interface for authentication operations
type AuthService interface {
	GenerateTokens(user *User) (*TokenPair, error)
	ValidateToken(token string) (*Claims, error)
	RefreshToken(refreshToken string) (*TokenPair, error)
	HashPassword(password string) (string, error)
	VerifyPassword(hashedPassword, password string) error
	GenerateAPIKey() (string, error)
}
// TokenPair represents an authentication token pair
type TokenPair struct {
	AccessToken  string `json:"access_token"`
	RefreshToken string `json:"refresh_token"`
}
// Claims represents the JWT claims structure
type Claims struct {
	UserID string `json:"user_id"`
	Email  string `json:"email"`
	Role   string `json:"role"`
}
</file>

<file path="metadata/migrations/000001_create_tracks_table.down.sql">
DROP TRIGGER IF EXISTS update_tracks_updated_at ON tracks;
DROP FUNCTION IF EXISTS update_updated_at_column();
DROP TABLE IF EXISTS tracks;
</file>

<file path="metadata/migrations/000001_create_tracks_table.up.sql">
CREATE TABLE IF NOT EXISTS tracks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(255) NOT NULL,
    artist VARCHAR(255) NOT NULL,
    album VARCHAR(255),
    isrc VARCHAR(12),
    iswc VARCHAR(11),
    year INTEGER,
    label VARCHAR(255),
    publisher VARCHAR(255),
    -- AI-generated metadata
    bpm DECIMAL(5,2),
    key VARCHAR(10),
    mood VARCHAR(50),
    genre VARCHAR(50),
    -- AI processing metadata
    ai_confidence DECIMAL(4,3),
    model_version VARCHAR(50),
    needs_review BOOLEAN DEFAULT false,
    -- Compliance and auditing
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    deleted_at TIMESTAMP WITH TIME ZONE,
    territory VARCHAR(2)
);
-- Indexes for common queries
CREATE INDEX idx_tracks_isrc ON tracks(isrc) WHERE deleted_at IS NULL;
CREATE INDEX idx_tracks_artist ON tracks(artist) WHERE deleted_at IS NULL;
CREATE INDEX idx_tracks_label ON tracks(label) WHERE deleted_at IS NULL;
CREATE INDEX idx_tracks_created_at ON tracks(created_at) WHERE deleted_at IS NULL;
-- Trigger to automatically update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';
CREATE TRIGGER update_tracks_updated_at
    BEFORE UPDATE ON tracks
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
</file>

<file path="metadata/migrations/000002_create_users_table.down.sql">
DROP TRIGGER IF EXISTS update_users_updated_at ON users;
DROP TABLE IF EXISTS users;
DROP TYPE IF EXISTS user_role;
DROP TYPE IF EXISTS subscription_plan;
</file>

<file path="metadata/migrations/000002_create_users_table.up.sql">
CREATE TYPE user_role AS ENUM ('admin', 'label_user', 'api_user');
CREATE TYPE subscription_plan AS ENUM ('free', 'pro', 'enterprise');
CREATE TABLE IF NOT EXISTS users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    password VARCHAR(255) NOT NULL,
    name VARCHAR(255) NOT NULL,
    role user_role NOT NULL,
    company VARCHAR(255),
    api_key VARCHAR(64) UNIQUE,
    -- Subscription and usage tracking
    plan subscription_plan NOT NULL DEFAULT 'free',
    track_quota INTEGER NOT NULL DEFAULT 10,
    tracks_used INTEGER NOT NULL DEFAULT 0,
    quota_reset_date TIMESTAMP WITH TIME ZONE NOT NULL,
    -- Compliance and auditing
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    deleted_at TIMESTAMP WITH TIME ZONE,
    last_login_at TIMESTAMP WITH TIME ZONE
);
-- Indexes for common queries
CREATE INDEX idx_users_email ON users(email) WHERE deleted_at IS NULL;
CREATE INDEX idx_users_api_key ON users(api_key) WHERE deleted_at IS NULL;
CREATE INDEX idx_users_role ON users(role) WHERE deleted_at IS NULL;
-- Trigger to automatically update updated_at timestamp
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
</file>

<file path="metadata/monitoring/alertmanager/templates/slack.tmpl">
{{ define "slack.default.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
{{ end }}

{{ define "slack.default.text" }}
{{ range .Alerts }}
*Alert:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}
*Severity:* {{ .Labels.severity }}
*Started:* {{ .StartsAt | since }}
{{ if .Labels.instance }}*Instance:* {{ .Labels.instance }}{{ end }}
{{ if .Labels.job }}*Job:* {{ .Labels.job }}{{ end }}

{{ end }}
{{ end }}
</file>

<file path="metadata/monitoring/alertmanager/alertmanager.yml">
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'  # Replace with your Slack webhook URL
templates:
  - '/etc/alertmanager/templates/*.tmpl'
route:
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'slack-notifications'
  routes:
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 10s
      repeat_interval: 1h
    - match:
        severity: warning
      receiver: 'slack-warnings'
      group_wait: 30s
      repeat_interval: 2h
    - match:
        severity: info
      receiver: 'slack-notifications'
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname']
receivers:
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        send_resolved: true
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        color: 'danger'
        title_link: 'https://your-grafana-url/alerts'  # Replace with your Grafana URL
  - name: 'slack-warnings'
    slack_configs:
      - channel: '#alerts-warnings'
        send_resolved: true
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        color: 'warning'
        title_link: 'https://your-grafana-url/alerts'  # Replace with your Grafana URL
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        color: 'good'
        title_link: 'https://your-grafana-url/alerts'  # Replace with your Grafana URL
</file>

<file path="metadata/monitoring/grafana/dashboards/metadata_service.json">
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "panels": [],
      "title": "Application Overview",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1
      },
      "id": 2,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "10.1.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(active_users)",
          "refId": "A"
        }
      ],
      "title": "Active Users",
      "type": "gauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(http_requests_total[5m])) by (status)",
          "legendFormat": "{{status}}",
          "refId": "A"
        }
      ],
      "title": "HTTP Request Rate by Status",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 9
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, path))",
          "legendFormat": "{{path}}",
          "refId": "A"
        }
      ],
      "title": "HTTP Request Duration (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 9
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(tracks_processed_total[5m])) by (operation, status)",
          "legendFormat": "{{operation}} - {{status}}",
          "refId": "A"
        }
      ],
      "title": "Track Processing Rate",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 17
      },
      "id": 6,
      "panels": [],
      "title": "Database & Cache",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 18
      },
      "id": 7,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, operation))",
          "legendFormat": "{{operation}}",
          "refId": "A"
        }
      ],
      "title": "Database Query Duration (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 18
      },
      "id": 8,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(cache_hits_total[5m])) by (cache)",
          "legendFormat": "Hits - {{cache}}",
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(cache_misses_total[5m])) by (cache)",
          "legendFormat": "Misses - {{cache}}",
          "refId": "B"
        }
      ],
      "title": "Cache Performance",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 26
      },
      "id": 9,
      "panels": [],
      "title": "AI Service",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 27
      },
      "id": 10,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "histogram_quantile(0.95, sum(rate(ai_request_duration_seconds_bucket[5m])) by (le, operation))",
          "legendFormat": "{{operation}}",
          "refId": "A"
        }
      ],
      "title": "AI Request Duration (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 27
      },
      "id": 11,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(ai_requests_total[5m])) by (operation, model)",
          "legendFormat": "{{operation}} - {{model}}",
          "refId": "A"
        }
      ],
      "title": "AI Request Rate",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["metadata", "application"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Metadata Service Dashboard",
  "uid": "metadata-service",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="metadata/monitoring/grafana/provisioning/dashboards/dashboards.yml">
apiVersion: 1
providers:
  - name: 'Metadata Service'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/dashboards
</file>

<file path="metadata/monitoring/grafana/provisioning/datasources/prometheus.yml">
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      timeInterval: 15s
      queryTimeout: 60s
      httpMethod: POST
</file>

<file path="metadata/monitoring/prometheus/prometheus.yml">
global:
  scrape_interval: 15s
  evaluation_interval: 15s
rule_files:
  - "rules.yml"
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - "alertmanager:9093"
scrape_configs:
  - job_name: 'metadata_service'
    metrics_path: '/metrics'
    static_configs:
      - targets: ['metadata-api:8080']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'metadata_service'
  - job_name: 'node_exporter'
    static_configs:
      - targets: ['node-exporter:9100']
  - job_name: 'redis_exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
  - job_name: 'postgres_exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']
</file>

<file path="metadata/monitoring/prometheus/rules.yml">
groups:
  - name: metadata_service_alerts
    rules:
      # High Error Rate Alert
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) 
          / 
          sum(rate(http_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High HTTP error rate
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
      # Slow Response Time Alert
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Slow response time
          description: "95th percentile of response time is {{ $value | humanizeDuration }} over the last 5 minutes"
      # Database Connection Alert
      - alert: HighDatabaseConnections
        expr: db_connections_open > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High number of database connections
          description: "{{ $value }} database connections currently open"
      # Cache Performance Alert
      - alert: LowCacheHitRate
        expr: |
          sum(rate(cache_hits_total{cache="redis"}[5m])) 
          / 
          (sum(rate(cache_hits_total{cache="redis"}[5m])) + sum(rate(cache_misses_total{cache="redis"}[5m]))) < 0.7
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Low cache hit rate
          description: "Cache hit rate is {{ $value | humanizePercentage }} over the last 10 minutes"
      # AI Service Performance Alert
      - alert: AIServiceLatency
        expr: |
          histogram_quantile(0.95, sum(rate(ai_request_duration_seconds_bucket[5m])) by (le, operation)) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High AI service latency
          description: "95th percentile of AI service latency is {{ $value | humanizeDuration }} for {{ $labels.operation }}"
      # Track Processing Alert
      - alert: HighTrackProcessingFailureRate
        expr: |
          sum(rate(tracks_processed_total{status="error"}[5m])) 
          / 
          sum(rate(tracks_processed_total[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High track processing failure rate
          description: "Track processing failure rate is {{ $value | humanizePercentage }} over the last 5 minutes"
      # Active Users Drop Alert
      - alert: ActiveUsersDrop
        expr: |
          active_users < 10 and hour() > 9 and hour() < 18 and day_of_week() < 6
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Low number of active users
          description: "Only {{ $value }} active users in the last 5 minutes during business hours"
      # Subscription Alert
      - alert: SubscriptionImbalance
        expr: |
          abs(
            (subscriptions_by_plan{plan="enterprise"} - subscriptions_by_plan{plan="pro"}) 
            / 
            subscriptions_by_plan{plan="pro"}
          ) > 0.5
        for: 24h
        labels:
          severity: info
        annotations:
          summary: Unusual subscription distribution
          description: "Unusual ratio between enterprise and pro subscriptions"
      # System Resource Alerts
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / process_virtual_memory_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High memory usage
          description: "Memory usage is at {{ $value | humanizePercentage }}"
      # API Quota Alert
      - alert: APIQuotaNearLimit
        expr: |
          sum(rate(http_requests_total{path=~"/api/v1/external/.*"}[1h])) > 8000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: API quota near limit
          description: "External API requests approaching rate limit ({{ $value | humanize }} req/hour)"
</file>

<file path="metadata/tools/llm_api.py">
#!/usr/bin/env /workspace/tmp_windsurf/venv/bin/python3
import google.generativeai as genai
from openai import OpenAI, AzureOpenAI
from anthropic import Anthropic
import argparse
import os
from dotenv import load_dotenv
from pathlib import Path
import sys
import base64
from typing import Optional, Union, List
import mimetypes
def load_environment():
    """Load environment variables from .env files in order of precedence"""
    # Order of precedence:
    # 1. System environment variables (already loaded)
    # 2. .env.local (user-specific overrides)
    # 3. .env (project defaults)
    # 4. .env.example (example configuration)
    env_files = ['.env.local', '.env', '.env.example']
    env_loaded = False
    print("Current working directory:", Path('.').absolute(), file=sys.stderr)
    print("Looking for environment files:", env_files, file=sys.stderr)
    for env_file in env_files:
        env_path = Path('.') / env_file
        print(f"Checking {env_path.absolute()}", file=sys.stderr)
        if env_path.exists():
            print(f"Found {env_file}, loading variables...", file=sys.stderr)
            load_dotenv(dotenv_path=env_path)
            env_loaded = True
            print(f"Loaded environment variables from {env_file}", file=sys.stderr)
            # Print loaded keys (but not values for security)
            with open(env_path) as f:
                keys = [line.split('=')[0].strip() for line in f if '=' in line and not line.startswith('#')]
                print(f"Keys loaded from {env_file}: {keys}", file=sys.stderr)
    if not env_loaded:
        print("Warning: No .env files found. Using system environment variables only.", file=sys.stderr)
        print("Available system environment variables:", list(os.environ.keys()), file=sys.stderr)
# Load environment variables at module import
load_environment()
def encode_image_file(image_path: str) -> tuple[str, str]:
    """
    Encode an image file to base64 and determine its MIME type.
    Args:
        image_path (str): Path to the image file
    Returns:
        tuple: (base64_encoded_string, mime_type)
    """
    mime_type, _ = mimetypes.guess_type(image_path)
    if not mime_type:
        mime_type = 'image/png'  # Default to PNG if type cannot be determined
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
    return encoded_string, mime_type
def create_llm_client(provider="openai"):
    if provider == "openai":
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        return OpenAI(
            api_key=api_key
        )
    elif provider == "azure":
        api_key = os.getenv('AZURE_OPENAI_API_KEY')
        if not api_key:
            raise ValueError("AZURE_OPENAI_API_KEY not found in environment variables")
        return AzureOpenAI(
            api_key=api_key,
            api_version="2024-08-01-preview",
            azure_endpoint="https://msopenai.openai.azure.com"
        )
    elif provider == "deepseek":
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY not found in environment variables")
        return OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1",
        )
    elif provider == "anthropic":
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        return Anthropic(
            api_key=api_key
        )
    elif provider == "gemini":
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")
        genai.configure(api_key=api_key)
        return genai
    elif provider == "local":
        return OpenAI(
            base_url="http://192.168.180.137:8006/v1",
            api_key="not-needed"
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")
def query_llm(prompt: str, client=None, model=None, provider="openai", image_path: Optional[str] = None) -> Optional[str]:
    """
    Query an LLM with a prompt and optional image attachment.
    Args:
        prompt (str): The text prompt to send
        client: The LLM client instance
        model (str, optional): The model to use
        provider (str): The API provider to use
        image_path (str, optional): Path to an image file to attach
    Returns:
        Optional[str]: The LLM's response or None if there was an error
    """
    if client is None:
        client = create_llm_client(provider)
    try:
        # Set default model
        if model is None:
            if provider == "openai":
                model = "gpt-4o"
            elif provider == "azure":
                model = os.getenv('AZURE_OPENAI_MODEL_DEPLOYMENT', 'gpt-4o-ms')  # Get from env with fallback
            elif provider == "deepseek":
                model = "deepseek-chat"
            elif provider == "anthropic":
                model = "claude-3-sonnet-20240229"
            elif provider == "gemini":
                model = "gemini-pro"
            elif provider == "local":
                model = "Qwen/Qwen2.5-32B-Instruct-AWQ"
        if provider in ["openai", "local", "deepseek", "azure"]:
            messages = [{"role": "user", "content": []}]
            # Add text content
            messages[0]["content"].append({
                "type": "text",
                "text": prompt
            })
            # Add image content if provided
            if image_path:
                if provider == "openai":
                    encoded_image, mime_type = encode_image_file(image_path)
                    messages[0]["content"] = [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{encoded_image}"}}
                    ]
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": 0.7,
            }
            # Add o1-specific parameters
            if model == "o1":
                kwargs["response_format"] = {"type": "text"}
                kwargs["reasoning_effort"] = "low"
                del kwargs["temperature"]
            response = client.chat.completions.create(**kwargs)
            return response.choices[0].message.content
        elif provider == "anthropic":
            messages = [{"role": "user", "content": []}]
            # Add text content
            messages[0]["content"].append({
                "type": "text",
                "text": prompt
            })
            # Add image content if provided
            if image_path:
                encoded_image, mime_type = encode_image_file(image_path)
                messages[0]["content"].append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": mime_type,
                        "data": encoded_image
                    }
                })
            response = client.messages.create(
                model=model,
                max_tokens=1000,
                messages=messages
            )
            return response.content[0].text
        elif provider == "gemini":
            model = client.GenerativeModel(model)
            response = model.generate_content(prompt)
            return response.text
    except Exception as e:
        print(f"Error querying LLM: {e}", file=sys.stderr)
        return None
def main():
    parser = argparse.ArgumentParser(description='Query an LLM with a prompt')
    parser.add_argument('--prompt', type=str, help='The prompt to send to the LLM', required=True)
    parser.add_argument('--provider', choices=['openai','anthropic','gemini','local','deepseek','azure'], default='openai', help='The API provider to use')
    parser.add_argument('--model', type=str, help='The model to use (default depends on provider)')
    parser.add_argument('--image', type=str, help='Path to an image file to attach to the prompt')
    args = parser.parse_args()
    if not args.model:
        if args.provider == 'openai':
            args.model = "gpt-4o" 
        elif args.provider == "deepseek":
            args.model = "deepseek-chat"
        elif args.provider == 'anthropic':
            args.model = "claude-3-5-sonnet-20241022"
        elif args.provider == 'gemini':
            args.model = "gemini-2.0-flash-exp"
        elif args.provider == 'azure':
            args.model = os.getenv('AZURE_OPENAI_MODEL_DEPLOYMENT', 'gpt-4o-ms')  # Get from env with fallback
    client = create_llm_client(args.provider)
    response = query_llm(args.prompt, client, model=args.model, provider=args.provider, image_path=args.image)
    if response:
        print(response)
    else:
        print("Failed to get response from LLM")
if __name__ == "__main__":
    main()
</file>

<file path="metadata/tools/screenshot_utils.py">
#!/usr/bin/env python3
import asyncio
from playwright.async_api import async_playwright
import os
import tempfile
from pathlib import Path
async def take_screenshot(url: str, output_path: str = None, width: int = 1280, height: int = 720) -> str:
    """
    Take a screenshot of a webpage using Playwright.
    Args:
        url (str): The URL to take a screenshot of
        output_path (str, optional): Path to save the screenshot. If None, saves to a temporary file.
        width (int, optional): Viewport width. Defaults to 1280.
        height (int, optional): Viewport height. Defaults to 720.
    Returns:
        str: Path to the saved screenshot
    """
    if output_path is None:
        # Create a temporary file with .png extension
        temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
        output_path = temp_file.name
        temp_file.close()
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page(viewport={'width': width, 'height': height})
        try:
            await page.goto(url, wait_until='networkidle')
            await page.screenshot(path=output_path, full_page=True)
        finally:
            await browser.close()
    return output_path
def take_screenshot_sync(url: str, output_path: str = None, width: int = 1280, height: int = 720) -> str:
    """
    Synchronous wrapper for take_screenshot.
    """
    return asyncio.run(take_screenshot(url, output_path, width, height))
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='Take a screenshot of a webpage')
    parser.add_argument('url', help='URL to take screenshot of')
    parser.add_argument('--output', '-o', help='Output path for screenshot')
    parser.add_argument('--width', '-w', type=int, default=1280, help='Viewport width')
    parser.add_argument('--height', '-H', type=int, default=720, help='Viewport height')
    args = parser.parse_args()
    output_path = take_screenshot_sync(args.url, args.output, args.width, args.height)
    print(f"Screenshot saved to: {output_path}")
</file>

<file path="metadata/tools/search_engine.py">
#!/usr/bin/env python3
import argparse
import sys
import time
from duckduckgo_search import DDGS
def search_with_retry(query, max_results=10, max_retries=3):
    """
    Search using DuckDuckGo and return results with URLs and text snippets.
    Args:
        query (str): Search query
        max_results (int): Maximum number of results to return
        max_retries (int): Maximum number of retry attempts
    """
    for attempt in range(max_retries):
        try:
            print(f"DEBUG: Searching for query: {query} (attempt {attempt + 1}/{max_retries})", 
                  file=sys.stderr)
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=max_results))
            if not results:
                print("DEBUG: No results found", file=sys.stderr)
                return []
            print(f"DEBUG: Found {len(results)} results", file=sys.stderr)
            return results
        except Exception as e:
            print(f"ERROR: Attempt {attempt + 1}/{max_retries} failed: {str(e)}", file=sys.stderr)
            if attempt < max_retries - 1:  # If not the last attempt
                print(f"DEBUG: Waiting 1 second before retry...", file=sys.stderr)
                time.sleep(1)  # Wait 1 second before retry
            else:
                print(f"ERROR: All {max_retries} attempts failed", file=sys.stderr)
                raise
def format_results(results):
    """Format and print search results."""
    for i, r in enumerate(results, 1):
        print(f"\n=== Result {i} ===")
        print(f"URL: {r.get('href', 'N/A')}")
        print(f"Title: {r.get('title', 'N/A')}")
        print(f"Snippet: {r.get('body', 'N/A')}")
def search(query, max_results=10, max_retries=3):
    """
    Main search function that handles search with retry mechanism.
    Args:
        query (str): Search query
        max_results (int): Maximum number of results to return
        max_retries (int): Maximum number of retry attempts
    """
    try:
        results = search_with_retry(query, max_results, max_retries)
        if results:
            format_results(results)
    except Exception as e:
        print(f"ERROR: Search failed: {str(e)}", file=sys.stderr)
        sys.exit(1)
def main():
    parser = argparse.ArgumentParser(description="Search using DuckDuckGo API")
    parser.add_argument("query", help="Search query")
    parser.add_argument("--max-results", type=int, default=10,
                      help="Maximum number of results (default: 10)")
    parser.add_argument("--max-retries", type=int, default=3,
                      help="Maximum number of retry attempts (default: 3)")
    args = parser.parse_args()
    search(args.query, args.max_results, args.max_retries)
if __name__ == "__main__":
    main()
</file>

<file path="metadata/tools/web_scraper.py">
#!/usr/bin/env /workspace/tmp_windsurf/venv/bin/python3
import asyncio
import argparse
import sys
import os
from typing import List, Optional
from playwright.async_api import async_playwright
import html5lib
from multiprocessing import Pool
import time
from urllib.parse import urlparse
import logging
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger(__name__)
async def fetch_page(url: str, context) -> Optional[str]:
    """Asynchronously fetch a webpage's content."""
    page = await context.new_page()
    try:
        logger.info(f"Fetching {url}")
        await page.goto(url)
        await page.wait_for_load_state('networkidle')
        content = await page.content()
        logger.info(f"Successfully fetched {url}")
        return content
    except Exception as e:
        logger.error(f"Error fetching {url}: {str(e)}")
        return None
    finally:
        await page.close()
def parse_html(html_content: Optional[str]) -> str:
    """Parse HTML content and extract text with hyperlinks in markdown format."""
    if not html_content:
        return ""
    try:
        document = html5lib.parse(html_content)
        result = []
        seen_texts = set()  # To avoid duplicates
        def should_skip_element(elem) -> bool:
            """Check if the element should be skipped."""
            return (
                elem.tag in ['{http://www.w3.org/1999/xhtml}script', 
                            '{http://www.w3.org/1999/xhtml}style'] or
                not any(text.strip() for text in elem.itertext())
            )
        def process_element(elem, depth=0):
            """Process an element and its children recursively."""
            if should_skip_element(elem):
                return
            # Handle text content
            if hasattr(elem, 'text') and elem.text:
                text = elem.text.strip()
                if text and text not in seen_texts:
                    # Check if this is an anchor tag
                    if elem.tag == '{http://www.w3.org/1999/xhtml}a':
                        href = None
                        for attr, value in elem.items():
                            if attr.endswith('href'):
                                href = value
                                break
                        if href and not href.startswith(('#', 'javascript:')):
                            # Format as markdown link
                            link_text = f"[{text}]({href})"
                            result.append("  " * depth + link_text)
                            seen_texts.add(text)
                    else:
                        result.append("  " * depth + text)
                        seen_texts.add(text)
            # Process children
            for child in elem:
                process_element(child, depth + 1)
            # Handle tail text
            if hasattr(elem, 'tail') and elem.tail:
                tail = elem.tail.strip()
                if tail and tail not in seen_texts:
                    result.append("  " * depth + tail)
                    seen_texts.add(tail)
        # Start processing from the body tag
        body = document.find('.//{http://www.w3.org/1999/xhtml}body')
        if body is not None:
            process_element(body)
        else:
            # Fallback to processing the entire document
            process_element(document)
        # Define noise patterns as a constant at module level
        NOISE_PATTERNS = frozenset([
            'var ', 
            'function()', 
            '.js',
            '.css',
            'google-analytics',
            'disqus',
            '{',
            '}'
        ])
        def is_valid_line(line: str) -> bool:
            """Check if a line doesn't contain any noise patterns."""
            return not any(pattern in line.lower() for pattern in NOISE_PATTERNS)
        # Filter out common unwanted patterns
        filtered_result = [line for line in result if is_valid_line(line)]
        return '\n'.join(filtered_result)
    except Exception as e:
        logger.error(f"Error parsing HTML: {str(e)}")
        return ""
async def process_urls(urls: List[str], max_concurrent: int = 5) -> List[str]:
    """Process multiple URLs concurrently."""
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        try:
            # Create browser contexts
            n_contexts = min(len(urls), max_concurrent)
            contexts = [await browser.new_context() for _ in range(n_contexts)]
            # Create tasks for each URL
            tasks = []
            for i, url in enumerate(urls):
                context = contexts[i % len(contexts)]
                task = fetch_page(url, context)
                tasks.append(task)
            # Gather results
            html_contents = await asyncio.gather(*tasks)
            # Parse HTML contents in parallel
            with Pool() as pool:
                results = pool.map(parse_html, html_contents)
            return results
        finally:
            # Cleanup
            for context in contexts:
                await context.close()
            await browser.close()
def validate_url(url: str) -> bool:
    """Validate if the given string is a valid URL."""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError as e:
        logger.debug(f"URL validation failed for {url}: {str(e)}")
        return False
def main():
    parser = argparse.ArgumentParser(description='Fetch and extract text content from webpages.')
    parser.add_argument('urls', nargs='+', help='URLs to process')
    parser.add_argument('--max-concurrent', type=int, default=5,
                       help='Maximum number of concurrent browser instances (default: 5)')
    parser.add_argument('--debug', action='store_true',
                       help='Enable debug logging')
    args = parser.parse_args()
    if args.debug:
        logger.setLevel(logging.DEBUG)
    # Validate URLs
    valid_urls = []
    for url in args.urls:
        if validate_url(url):
            valid_urls.append(url)
        else:
            logger.error(f"Invalid URL: {url}")
    if not valid_urls:
        logger.error("No valid URLs provided")
        sys.exit(1)
    start_time = time.time()
    try:
        results = asyncio.run(process_urls(valid_urls, args.max_concurrent))
        # Print results to stdout
        for url, text in zip(valid_urls, results):
            print(f"\n=== Content from {url} ===")
            print(text)
            print("=" * 80)
        logger.info(f"Total processing time: {time.time() - start_time:.2f}s")
    except Exception as e:
        logger.error(f"Error during execution: {str(e)}")
        sys.exit(1)
if __name__ == '__main__':
    main()
</file>

<file path="metadata/.cursorrules">
# Instructions

During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities

# Scratchpad

## Current Task: Convert Project to Cookiecutter Template

 cookiecutter  cursor  windsurf 


[ ] 1.  cookiecutter.json 
    - project_type cursor/windsurf
    - project_name
    - llm_api_key
[ ] 2. 
    -  project_type 
    -  symbolic link 
    -  tests 
    -  README.md
[ ] 3.  hooks 
    - pre_gen_project
    - post_gen_project
        - 
        -  symbolic links
[ ] 4. 
    -  README.md
    - API key 


1. Cursor vs Windsurf 
   - Cursor:  .cursorrules
   - Windsurf:  .windsurfrules  scratchpad.md

2. API Key 
   - 
   -  API key
   -  API key 

3. 
   - tools/  symbolic link 
   - requirements.txt  symbolic link
   - 
   -  post_gen_project 
</file>

<file path="metadata/docker-compose.monitoring.yml">
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - monitoring
    restart: unless-stopped
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - monitoring
    restart: unless-stopped
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    networks:
      - monitoring
    restart: unless-stopped
  redis-exporter:
    image: oliver006/redis_exporter:v1.54.0
    container_name: redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
    ports:
      - "9121:9121"
    networks:
      - monitoring
      - app
    restart: unless-stopped
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.13.2
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:postgres@postgres:5432/metadata?sslmode=disable
    ports:
      - "9187:9187"
    networks:
      - monitoring
      - app
    restart: unless-stopped
  grafana:
    image: grafana/grafana:10.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/dashboards:/etc/grafana/dashboards
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin  # Change this in production
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    networks:
      - monitoring
    restart: unless-stopped
    depends_on:
      - prometheus
networks:
  monitoring:
    driver: bridge
  app:
    external: true  # This should match your application network
volumes:
  prometheus_data:
  grafana_data:
</file>

<file path="metadata/go.mod">
module metadatatool

go 1.23.5

require (
	github.com/aws/aws-sdk-go-v2 v1.36.1
	github.com/aws/aws-sdk-go-v2/config v1.29.6
	github.com/aws/aws-sdk-go-v2/credentials v1.17.59
	github.com/aws/aws-sdk-go-v2/service/s3 v1.75.4
	github.com/caarlos0/env/v6 v6.10.1
	github.com/dhowden/tag v0.0.0-20240417053706-3d75831295e8
	github.com/getsentry/sentry-go v0.31.1
	github.com/go-redis/redis/v8 v8.11.5
	github.com/gofiber/fiber/v2 v2.52.6
	github.com/golang-jwt/jwt/v5 v5.2.1
	github.com/google/uuid v1.6.0
	github.com/mitchellh/mapstructure v1.5.0
	github.com/prometheus/client_golang v1.20.5
	github.com/redis/go-redis/v9 v9.7.0
	github.com/rs/zerolog v1.33.0
	github.com/sashabaranov/go-openai v1.37.0
	github.com/valyala/fasthttp v1.58.0
	golang.org/x/crypto v0.32.0
	gorm.io/driver/postgres v1.5.11
	gorm.io/gorm v1.25.12
)

require (
	github.com/andybalholm/brotli v1.1.1 // indirect
	github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream v1.6.8 // indirect
	github.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.16.28 // indirect
	github.com/aws/aws-sdk-go-v2/internal/configsources v1.3.32 // indirect
	github.com/aws/aws-sdk-go-v2/internal/endpoints/v2 v2.6.32 // indirect
	github.com/aws/aws-sdk-go-v2/internal/ini v1.8.2 // indirect
	github.com/aws/aws-sdk-go-v2/internal/v4a v1.3.32 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/accept-encoding v1.12.2 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/checksum v1.5.6 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.12.13 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/s3shared v1.18.13 // indirect
	github.com/aws/aws-sdk-go-v2/service/sso v1.24.15 // indirect
	github.com/aws/aws-sdk-go-v2/service/ssooidc v1.28.14 // indirect
	github.com/aws/aws-sdk-go-v2/service/sts v1.33.14 // indirect
	github.com/aws/smithy-go v1.22.2 // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect
	github.com/jackc/pgx/v5 v5.5.5 // indirect
	github.com/jackc/puddle/v2 v2.2.1 // indirect
	github.com/jinzhu/inflection v1.0.0 // indirect
	github.com/jinzhu/now v1.1.5 // indirect
	github.com/klauspost/compress v1.17.11 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.55.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/rivo/uniseg v0.2.0 // indirect
	github.com/valyala/bytebufferpool v1.0.0 // indirect
	github.com/valyala/tcplisten v1.0.0 // indirect
	golang.org/x/sync v0.10.0 // indirect
	golang.org/x/sys v0.29.0 // indirect
	golang.org/x/text v0.21.0 // indirect
	google.golang.org/protobuf v1.34.2 // indirect
)
</file>

<file path="metadata/README.md">
# {{ cookiecutter.project_name }}

{% if cookiecutter.project_type == 'cursor' %}
A Cursor-powered AI development environment with advanced agentic capabilities.
{% else %}
A Windsurf-powered AI development environment with streamlined development capabilities.
{% endif %}

## Quick Start

1. Activate the virtual environment:
   ```bash
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

2. Configure your environment:
   - Copy `.env.example` to `.env` if you haven't already
   - Add your API keys in `.env` (optional)

## Available Tools

Your project includes several powerful tools in the `tools/` directory:

### LLM Integration
```python
from tools.llm_api import query_llm

# Use LLM for assistance
response = query_llm(
    "Your question here",
    provider="anthropic"  # Options: openai, anthropic, azure_openai, deepseek, gemini
)
print(response)
```

### Web Scraping
```python
from tools.web_scraper import scrape_urls

# Scrape web content
results = scrape_urls(["https://example.com"], max_concurrent=3)
```

### Search Engine
```python
from tools.search_engine import search

# Search the web
results = search("your search keywords")
```

{% if cookiecutter.project_type == 'cursor' %}
### Screenshot Verification
```python
from tools.screenshot_utils import take_screenshot_sync
from tools.llm_api import query_llm

# Take and analyze screenshots
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')
analysis = query_llm(
    "Describe this webpage",
    provider="openai",
    image_path=screenshot_path
)
```

Note: When you first use the screenshot verification feature, Playwright browsers will be installed automatically.
{% endif %}

## AI Assistant Configuration

{% if cookiecutter.project_type == 'cursor' %}
This project uses `.cursorrules` to configure the AI assistant. The assistant can:
- Help with coding tasks
- Verify screenshots
- Perform web searches
- Analyze images and code
{% else %}
This project uses `.windsurfrules` and `scratchpad.md` to configure the AI assistant. The assistant can:
- Help with coding tasks
- Perform web searches
- Analyze code
{% endif %}

## Environment Variables

Configure these in your `.env` file:

- `LLM_API_KEY`: Your LLM API key (optional)
- `AZURE_OPENAI_API_KEY`: Azure OpenAI API key (optional)
- `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint (optional)
- `AZURE_OPENAI_MODEL_DEPLOYMENT`: Azure OpenAI model deployment name (optional)

Note: Basic functionality works without API keys. Advanced features (like multimodal analysis) require appropriate API keys.

## Development Tools

- `.devcontainer/`: VS Code development container configuration
- `.vscode.example/`: Recommended VS Code settings
- `.github/`: CI/CD workflows

## License

MIT License
</file>

<file path="metadata/requirements.txt">
# Core dependencies
requests>=2.31.0
python-dotenv>=1.0.0

# LLM API clients
openai>=1.12.0
anthropic>=0.18.1

# Web scraping
playwright>=1.42.0
beautifulsoup4>=4.12.0

# Utility packages
pillow>=10.2.0  # For image processing
python-dateutil>=2.8.2
</file>

<file path="monitoring/alertmanager/alertmanager.yml">
global:
  resolve_timeout: 5m
route:
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'default'
receivers:
  - name: 'default'
    # You can configure email, Slack, or other notification channels here
    # For now, we'll use a basic webhook configuration
    webhook_configs:
      - url: 'http://localhost:8080/webhook'  # Replace with your actual webhook URL
</file>

<file path="monitoring/grafana/dashboards/metadatatool.json">
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": null,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(http_requests_total[5m])",
          "refId": "A"
        }
      ],
      "title": "HTTP Request Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "10.1.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "active_users",
          "refId": "A"
        }
      ],
      "title": "Active Users",
      "type": "gauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])",
          "refId": "A"
        }
      ],
      "title": "Average Response Time",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(cache_hits_total[5m])) / (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))",
          "refId": "A"
        }
      ],
      "title": "Cache Hit Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 16
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "database_connections{state='active'}",
          "legendFormat": "Active Connections",
          "refId": "A"
        }
      ],
      "title": "Database Connections",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(database_query_duration_seconds_sum[5m]) / rate(database_query_duration_seconds_count[5m])",
          "legendFormat": "Query Latency",
          "refId": "A"
        }
      ],
      "title": "Database Query Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 24
      },
      "id": 7,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(ai_request_duration_seconds_sum[5m]) / rate(ai_request_duration_seconds_count[5m])",
          "legendFormat": "AI Request Latency",
          "refId": "A"
        }
      ],
      "title": "AI Service Latency",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 24
      },
      "id": 8,
      "options": {
        "legend": {
          "calcs": ["mean", "max", "min"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "avg(ai_confidence)",
          "legendFormat": "AI Confidence",
          "refId": "A"
        }
      ],
      "title": "AI Confidence Score",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 32
      },
      "id": 9,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "100 * (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes",
          "legendFormat": "Memory Usage",
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "100 * (1 - avg by (instance)(irate(node_cpu_seconds_total{mode='idle'}[5m])))",
          "legendFormat": "CPU Usage",
          "refId": "B"
        }
      ],
      "title": "System Resource Usage",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 32
      },
      "id": 10,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "100 * sum(rate(http_requests_total{status=~'5..'}[5m])) / sum(rate(http_requests_total[5m]))",
          "legendFormat": "Error Rate",
          "refId": "A"
        }
      ],
      "title": "HTTP Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 40
      },
      "id": 11,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "rate(tracks_processed_total[5m])",
          "legendFormat": "Tracks Processed",
          "refId": "A"
        }
      ],
      "title": "Track Processing Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 40
      },
      "id": 12,
      "options": {
        "legend": {
          "calcs": ["mean", "max"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(tracks_processed_total{status='success'}[5m])) / sum(rate(tracks_processed_total[5m]))",
          "legendFormat": "Success Rate",
          "refId": "A"
        }
      ],
      "title": "Track Processing Success Rate",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["metadatatool"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Metadata Tool Overview",
  "uid": "metadatatool",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="monitoring/grafana/provisioning/dashboards/provider.yml">
apiVersion: 1
providers:
  - name: 'Metadata Tool'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/dashboards
      foldersFromFilesStructure: true
</file>

<file path="monitoring/grafana/provisioning/datasources/prometheus.yml">
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      timeInterval: 15s
      queryTimeout: 60s
      httpMethod: POST
</file>

<file path="monitoring/prometheus/rules/alerts.yml">
groups:
  - name: metadatatool_alerts
    rules:
      # High Error Rate Alert
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) 
          / 
          sum(rate(http_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High HTTP error rate
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
      # Slow Response Time Alert
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Slow response time
          description: "95th percentile of response time is {{ $value | humanizeDuration }}"
      # Database Connection Alert
      - alert: HighDatabaseConnections
        expr: database_connections{state="active"} > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High number of database connections
          description: "{{ $value }} active database connections"
      # Cache Performance Alert
      - alert: LowCacheHitRate
        expr: |
          sum(rate(cache_hits_total[5m])) 
          / 
          (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m]))) < 0.7
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Low cache hit rate
          description: "Cache hit rate is {{ $value | humanizePercentage }}"
      # AI Service Performance Alert
      - alert: HighAILatency
        expr: |
          histogram_quantile(0.95, sum(rate(ai_request_duration_seconds_bucket[5m])) by (le)) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High AI service latency
          description: "95th percentile of AI request latency is {{ $value | humanizeDuration }}"
      # Track Processing Alert
      - alert: HighTrackProcessingFailureRate
        expr: |
          sum(rate(tracks_processed_total{status="error"}[5m])) 
          / 
          sum(rate(tracks_processed_total[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High track processing failure rate
          description: "Track processing failure rate is {{ $value | humanizePercentage }}"
      # Active Users Drop Alert
      - alert: LowActiveUsers
        expr: active_users < 10 and hour() > 9 and hour() < 18 and day_of_week() < 6
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Low number of active users
          description: "Only {{ $value }} active users during business hours"
</file>

<file path="monitoring/prometheus/prometheus.yml">
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  scrape_timeout: 10s
rule_files:
  - "rules/*.yml"
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - "alertmanager:9093"
scrape_configs:
  # Main application metrics
  - job_name: 'metadatatool'
    metrics_path: '/metrics'
    static_configs:
      - targets: ['localhost:8080']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'metadatatool'
  # Node metrics
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
  # PostgreSQL metrics
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
</file>

<file path="scripts/setup_dev.sh">
#!/bin/bash
# Colors for output
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m'
echo -e "${GREEN}Setting up development environment...${NC}"
# Check if Redis is running
redis-cli ping > /dev/null 2>&1
if [ $? -ne 0 ]; then
    echo -e "${RED}Redis is not running. Starting Redis...${NC}"
    brew services start redis
fi
# Start monitoring stack
echo -e "${GREEN}Starting monitoring stack...${NC}"
docker-compose -f docker-compose.monitoring.yml up -d
# Install frontend dependencies
echo -e "${GREEN}Setting up frontend...${NC}"
mkdir -p frontend
cd frontend
if [ ! -f package.json ]; then
    echo -e "${GREEN}Initializing new React project...${NC}"
    npx create-react-app . --template typescript
    npm install @apollo/client graphql @tanstack/react-query tailwindcss @headlessui/react
    npm install -D @storybook/react @testing-library/react @testing-library/jest-dom
fi
# Run tests
echo -e "${GREEN}Running backend tests...${NC}"
cd ..
# Run tests and capture output
TEST_OUTPUT=$(go test ./... -v 2>&1)
TEST_EXIT_CODE=$?
# Check for test failures
if [ $TEST_EXIT_CODE -ne 0 ]; then
    echo -e "${RED}Some tests failed. Here are the failures:${NC}"
    echo "$TEST_OUTPUT" | grep -A 1 "FAIL:"
    echo -e "\n${YELLOW}Please check TODO_DETAILED.md for instructions on fixing these tests.${NC}"
else
    echo -e "${GREEN}All tests passed!${NC}"
fi
echo -e "${GREEN}Setup complete! You can start implementing the TODO list.${NC}"
echo -e "${GREEN}Don't forget to:${NC}"
echo "1. Check TODO_DETAILED.md for today's tasks"
echo "2. Keep DEBUGGING.md updated"
echo "3. Run tests frequently"
echo "4. Commit small, focused changes"
# If tests failed, exit with error code
if [ $TEST_EXIT_CODE -ne 0 ]; then
    echo -e "${YELLOW}Warning: Some tests failed. Please fix them before proceeding with new features.${NC}"
fi
</file>

<file path="scripts/start_pubsub_emulator.sh">
#!/bin/bash
# Check if gcloud is installed
if ! command -v gcloud &> /dev/null; then
    echo "gcloud is not installed. Please install the Google Cloud SDK."
    exit 1
fi
# Install the pubsub emulator if not already installed
gcloud components install pubsub-emulator
# Start the pubsub emulator in the background
gcloud beta emulators pubsub start --host-port=localhost:8085 &
# Wait for emulator to start
sleep 5
# Export the environment variable
export PUBSUB_EMULATOR_HOST=localhost:8085
echo "Pub/Sub emulator is running at localhost:8085"
</file>

<file path=".gitignore">
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib

# Test binary, built with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Dependency directories (remove the comment below to include it)
vendor/

# Go workspace file
go.work

# Environment files
.env
.env.local
.env.*.local

# IDE specific files
.idea/
.vscode/
*.swp
*.swo
*~

# OS specific files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Application specific
tmp/
logs/
*.log

# Python virtual environment
venv/
</file>

<file path=".golangci.yml">
run:
  timeout: 5m
  tests: true
  skip-dirs:
    - vendor
linters:
  enable:
    - gofmt
    - govet
    - errcheck
    - staticcheck
    - gosimple
    - ineffassign
    - typecheck
    - unused
    - misspell
    - gocyclo
    - gosec
    - bodyclose
    - goconst
    - goimports
    - prealloc
    - unconvert
    - unparam
linters-settings:
  gocyclo:
    min-complexity: 15
  goconst:
    min-len: 3
    min-occurrences: 3
  gosec:
    excludes:
      - G404  # Use of weak random number generator
  govet:
    check-shadowing: true
  misspell:
    locale: US
issues:
  exclude-rules:
    - path: _test\.go
      linters:
        - gosec
        - errcheck
    - path: internal/mock/.*\.go
      linters:
        - unused
        - deadcode
  max-same-issues: 50
</file>

<file path="cursorrules">
### **Product Requirements Document (PRD)**  
**Project Name**: Metadata Tool for Music Catalogs  

---

## **1. Introduction**  

### **1.1 Purpose**  
The Metadata Tool for Music Catalogs is designed to provide **labels, distributors, and rights holders** with a **high-performance, AI-driven** solution for managing and enriching music metadata.  

The tool will enable users to:  
- **Drag and drop** audio files for **automated metadata extraction**.  
- **Edit metadata manually or in bulk** through an intuitive UI.  
- **Access metadata via API** for large-scale processing and enrichment.  
- **Enhance metadata with AI** for **genre classification, mood tagging, BPM detection, and key analysis**.  
- **Ensure compliance with industry standards** (DDEX, CWR, ISRC, ISWC).  
- **Batch process metadata** efficiently, including validation and exports.  

### **1.2 Goals and Objectives**  
- **Backend**: High-performance, scalable metadata extraction using **Golang**.  
- **Frontend**: **React-based UI** with an intuitive user experience.  
- **API-First**: GraphQL & RESTful API for **metadata retrieval, validation, and bulk processing**.  
- **AI Enhancement**: AI-powered **metadata enrichment** with model versioning & cost optimization.  
- **Compliance**: Full **DDEX ERN 4.3 support**, ensuring metadata standardization.  
- **Batch Processing**: Bulk metadata imports, edits, and exports at scale.  

---

## **2. Features & Requirements**  

### **2.1 Core Features**  

#### **Metadata Extraction & Enrichment**  
- Extract metadata fields from audio files:  
  - **Title, Artist, Album, ISRC, ISWC, Release Year, Label, Publisher**  
  - **BPM, Key, Mood, Genre (AI-generated where necessary)**  
- AI-powered **metadata enrichment** for genre classification, mood tagging, and DSP optimization.  

#### **API Integration**  
- **GraphQL & REST API** for metadata retrieval, validation, and enrichment.  
- OAuth2 authentication with JWT-based access control.  

#### **Batch Processing & Editing**  
- **Bulk metadata imports** with validation and enrichment.  
- **CSV, JSON, XML exports** for DSP compatibility.  
- **Pre-validation of metadata** against DDEX standards before submission.  

#### **User Authentication & Permissions**  
- **Role-based access control (RBAC)**:  
  - **Admin**: Full system control, user management, system settings.  
  - **Label User**: Upload, edit, and export metadata.  
  - **API User**: Programmatic access for automation, subject to API limits.  

---

## **3. Technical Architecture**  

### **3.1 Backend (Golang)**  
- **Framework**: Go Fiber for high-performance HTTP handling.  
- **Database**: PostgreSQL (with **partitioning for large-scale metadata storage**).  
- **Storage**: Cloud Storage (S3-compatible) for audio and metadata logs.  
- **Authentication**: OAuth2 with JWT-based session management.  
- **API Documentation**: OpenAPI (Swagger) for REST and GraphQL Playground for flexible queries.  

#### **Metadata Processing Pipeline**  
```
[Upload]  [Cloud Storage]  [Golang API]  [PostgreSQL]
                                  
                        [AI Processing Queue]
                                  
                        [Metadata Enhancement]
                                  
                        [DDEX Validation & Storage]
```

---

### **3.2 Frontend (React & TypeScript)**  
- **Framework**: React with Next.js for SSR support.  
- **State Management**: TanStack Query for API state handling.  
- **Validation**: Zod for form validation.  
- **Styling**: Tailwind CSS for responsive UI.  
- **Testing**: Jest & React Testing Library for unit and integration testing.  

---

## **4. AI Model Integration & Scaling**  

### **4.1 AI-Powered Metadata Processing**  
- **Real-time AI analysis** for premium users; batch processing for standard users.  
- **Custom AI models** replace OpenAI API when cost exceeds **$10K/month**.  
- **Confidence Scoring**: Metadata flagged for human review if confidence < 85%.  
- **Model Versioning**: `model_v1`, `model_v2` with A/B testing for continuous improvement.  

---

## **5. Service Splitting & Scalability**  

### **5.1 Service Architecture**  
- **Split microservices when exceeding 500M records or 10,000 QPS**.  
- **DDEX ingestion, AI processing, and Metadata API run as separate services**.  

### **5.2 Database & Sharding Strategy**  
- **Partner-based sharding** (Sony, Warner, Universal).  
- **PostgreSQL partitioned tables** for large-scale queries.  
- **Migrate to Cloud Spanner if writes exceed 5,000 TPS**.  

### **5.3 Caching Strategy**  
- **Redis clustering** for metadata caching.  
- **Pre-warming**: Pre-load top 10K metadata records in cache before peak traffic.  
- **Probabilistic cache refresh** to prevent TTL stampedes.  

---

## **6. Compliance & Security**  

### **6.1 DDEX Schema Versioning**  
- **Dual-write migration strategy** ensures backward compatibility.  
- **Pre-validation against latest ERN schema before submission**.  

### **6.2 GDPR & Data Protection**  
- **Soft-delete metadata upon user request** for compliance.  
- **Territorial metadata access controls** based on licensing rules.  
- **Audit logging** for metadata changes, retained for:  
  - **7 years (DDEX compliance)**  
  - **3 years (AI metadata logs)**  
  - **6 months (low-risk operational logs)**  

---

## **7. Monetization Strategy**  

### **7.1 Subscription Plans**  
- **Free Tier**: 10 tracks/month, manual editing only.  
- **Pro Plan ($19/month)**: 100 tracks/month, AI metadata enhancement, batch processing.  
- **Enterprise Plan ($99+/month)**: Unlimited tracks, API access, priority support.  

### **7.2 API Pricing**  
- **Pay-as-you-go**: $0.01 per API request.  
- **Volume discounts & enterprise licensing** for large users.  

---

## **8. Monitoring & Cost Optimization**  

### **8.1 Observability & Performance Metrics**  
- **SLOs**:  
  - API response time: **p99 < 200ms**  
  - AI processing latency: **real-time < 1s**, **batch < 10 min**  
  - DDEX ingestion success rate: **99.9% within 5 min**  
- **Cloud Billing API tracks per-feature costs**.  

---

## **9. Deployment & CI/CD**  

### **9.1 Cloud Infrastructure (Google Cloud Platform)**  
- **Compute**: Cloud Run (API), Cloud Functions (AI processing), GKE (scalable workloads).  
- **Storage**: Cloud SQL (PostgreSQL), Firestore (real-time metadata sync).  
- **Security**: Cloud IAM, Secret Manager, Cloud KMS.  

### **9.2 CI/CD Pipeline**  
- **Cloud Build** for automated testing & deployment.  
- **Artifact Registry** for managing Docker containers.  
- **Staging & Production environments** with feature flags for safe rollouts.  

---

## **10. Future Enhancements**  

- **DSP direct integration** with Spotify, Apple Music, Amazon Music.  
- **Blockchain metadata verification for rights management**.  
- **Machine Learning for metadata prediction & trend analysis**.  
- **Multi-region deployment for lower latency and failover handling**.  

---

 **This PRD is now fully aligned with our technical discussions, .cursorrules, and best practices for AI-driven metadata processing.** Let me know if you'd like any final refinements! 
</file>

<file path="DEBUGGING.md">
# Debugging Documentation

## Recent Fixes (2024)

### 1. Queue Middleware Tracing Fix
**File**: `internal/pkg/monitoring/tracing/queue_middleware.go`
**Issue**: Incorrect type conversion from QueuePriority (int) to string
**Fix**: 
- Replaced direct type conversion `string(priority)` with `strconv.Itoa(int(priority))`
- Added proper import for "strconv" package
- Ensures proper string representation of priority values in traces

**Impact**:
- Prevents potential data corruption in tracing output
- Improves trace readability by showing actual numeric values
- Maintains compatibility with OpenTelemetry attribute standards

### 2. Redis Queue Test Improvements
**File**: `internal/repository/queue/redis_queue_test.go`
**Issue**: Multiple unused write warnings in test setup
**Fix**:
- Enhanced test coverage by properly utilizing all message fields
- Added comprehensive message verification
- Implemented proper Redis storage and retrieval testing
- Added timestamp consistency using a shared `now` variable

**Changes**:
```go
// Before
msg := &domain.Message{
    ID: "test-msg",
    Type: topic,  // Unused write
    Data: map[string]interface{}{...},  // Unused write
    Status: domain.MessageStatusProcessing,  // Unused write
    CreatedAt: time.Now(),  // Unused write
    UpdatedAt: time.Now(),  // Unused write
}

// After
now := time.Now().Add(-processingLockDuration * 2)
msg := &domain.Message{...}
// Added full message storage and verification
messageKey := fmt.Sprintf("message:%s", msg.ID)
// Added comprehensive assertions for all fields
```

**Impact**:
- Improved test coverage
- Better verification of message persistence
- More reliable cleanup testing
- Eliminated all unused write warnings

### 3. Session Store Security Enhancement
**File**: `internal/repository/redis/session_store.go`
**Issue**: Unused userID parameter in deleteOldestSession
**Fix**:
- Added user validation in session deletion
- Improved error handling with context
- Added security check to verify session ownership

**Changes**:
```go
// Added validation
if session.UserID != userID {
    return fmt.Errorf("session %s does not belong to user %s", sessionID, userID)
}

// Improved error messages
return fmt.Errorf("failed to get session for user %s: %w", userID, err)
```

**Impact**:
- Enhanced security by preventing unauthorized session access
- Improved error tracing with contextual information
- Better debugging capabilities with detailed error messages

## Testing Instructions

### Queue Middleware Testing
```bash
go test ./internal/pkg/monitoring/tracing/... -v
```
Verify that:
- Tracing attributes are properly formatted
- Priority values are correctly stringified

### Redis Queue Testing
```bash
go test ./internal/repository/queue/... -v
```
Verify that:
- Message cleanup works correctly
- All message fields are properly stored and retrieved
- Concurrency tests pass without race conditions

### Session Store Testing
```bash
go test ./internal/repository/redis/... -v
```
Verify that:
- Session ownership is properly validated
- Error messages contain user context
- Session cleanup works as expected

## Known Limitations

1. Redis Queue:
   - Processing timeout is fixed and not configurable per message
   - Cleanup is synchronous and might impact performance with large datasets

2. Session Store:
   - Session enumeration might be possible (mitigated by UUID usage)
   - No built-in rate limiting for session creation

## Future Improvements

1. Queue System:
   - Add configurable processing timeouts
   - Implement async cleanup with batching
   - Add metrics for queue performance

2. Session Management:
   - Add rate limiting for session creation
   - Implement session activity tracking
   - Add support for session attributes/metadata

## Monitoring Recommendations

1. Queue Metrics to Monitor:
   - Processing time per message
   - Dead letter queue size
   - Message retry counts
   - Queue length per topic

2. Session Metrics to Monitor:
   - Active sessions per user
   - Session creation rate
   - Session cleanup success rate
   - Average session lifetime
</file>

<file path="docker-compose.monitoring.yml">
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - monitoring
    restart: unless-stopped
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - monitoring
    restart: unless-stopped
  grafana:
    image: grafana/grafana:10.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin  # Change this in production
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - SENTRY_DSN=your-sentry-project-dsn
      - SENTRY_ENVIRONMENT=development
      - SENTRY_DEBUG=false
      - SENTRY_SAMPLE_RATE=1.0
      - SENTRY_TRACES_SAMPLE_RATE=0.2
    networks:
      - monitoring
    restart: unless-stopped
    depends_on:
      - prometheus
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    networks:
      - monitoring
    restart: unless-stopped
  redis-exporter:
    image: oliver006/redis_exporter:v1.54.0
    container_name: redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
    ports:
      - "9121:9121"
    networks:
      - monitoring
      - app
    restart: unless-stopped
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.13.2
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:postgres@postgres:5432/metadata?sslmode=disable
    ports:
      - "9187:9187"
    networks:
      - monitoring
      - app
    restart: unless-stopped
networks:
  monitoring:
    driver: bridge
  app:
    external: true  # This should match your application network
volumes:
  prometheus_data:
  grafana_data:
</file>

<file path="Dockerfile">
# Build stage
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Install build dependencies
RUN apk add --no-cache git

# Copy go mod files
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Build the application
RUN CGO_ENABLED=0 GOOS=linux go build -o /app/api ./cmd/api

# Final stage
FROM alpine:3.19

WORKDIR /app

# Install runtime dependencies
RUN apk add --no-cache ca-certificates tzdata

# Copy binary from builder
COPY --from=builder /app/api .

# Create non-root user
RUN adduser -D -g '' appuser
USER appuser

# Expose port
EXPOSE 8080

# Run the application
CMD ["./api"]
</file>

<file path="go.mod">
module metadatatool

go 1.22.0

toolchain go1.23.5

require (
	cloud.google.com/go/bigquery v1.59.1
	cloud.google.com/go/pubsub v1.37.0
	github.com/alicebob/miniredis/v2 v2.34.0
	github.com/aws/aws-sdk-go-v2 v1.36.1
	github.com/aws/aws-sdk-go-v2/config v1.29.6
	github.com/aws/aws-sdk-go-v2/credentials v1.17.59
	github.com/aws/aws-sdk-go-v2/service/s3 v1.75.4
	github.com/beevik/etree v1.5.0
	github.com/dhowden/tag v0.0.0-20240417053706-3d75831295e8
	github.com/getsentry/sentry-go v0.31.1
	github.com/gin-gonic/gin v1.10.0
	github.com/go-playground/validator/v10 v10.20.0
	github.com/go-redis/redis/v8 v8.11.5
	github.com/gofiber/fiber/v2 v2.52.6
	github.com/golang-jwt/jwt/v5 v5.2.1
	github.com/google/uuid v1.6.0
	github.com/lib/pq v1.10.9
	github.com/prometheus/client_golang v1.20.5
	github.com/redis/go-redis/v9 v9.0.5
	github.com/sashabaranov/go-openai v1.37.0
	github.com/sirupsen/logrus v1.9.3
	github.com/sony/gobreaker v1.0.0
	github.com/stretchr/testify v1.10.0
	go.opentelemetry.io/otel v1.34.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.34.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.34.0
	go.opentelemetry.io/otel/sdk v1.34.0
	go.opentelemetry.io/otel/trace v1.34.0
	golang.org/x/crypto v0.33.0
	golang.org/x/time v0.5.0
	google.golang.org/api v0.171.0
	gorm.io/driver/postgres v1.5.11
	gorm.io/gorm v1.25.12
)

require (
	cloud.google.com/go v0.112.1 // indirect
	cloud.google.com/go/compute/metadata v0.5.2 // indirect
	cloud.google.com/go/iam v1.1.6 // indirect
	github.com/alicebob/gopher-json v0.0.0-20230218143504-906a9b012302 // indirect
	github.com/andybalholm/brotli v1.1.1 // indirect
	github.com/apache/arrow/go/v14 v14.0.2 // indirect
	github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream v1.6.8 // indirect
	github.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.16.28 // indirect
	github.com/aws/aws-sdk-go-v2/internal/configsources v1.3.32 // indirect
	github.com/aws/aws-sdk-go-v2/internal/endpoints/v2 v2.6.32 // indirect
	github.com/aws/aws-sdk-go-v2/internal/ini v1.8.2 // indirect
	github.com/aws/aws-sdk-go-v2/internal/v4a v1.3.32 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/accept-encoding v1.12.2 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/checksum v1.5.6 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.12.13 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/s3shared v1.18.13 // indirect
	github.com/aws/aws-sdk-go-v2/service/sso v1.24.15 // indirect
	github.com/aws/aws-sdk-go-v2/service/ssooidc v1.28.14 // indirect
	github.com/aws/aws-sdk-go-v2/service/sts v1.33.14 // indirect
	github.com/aws/smithy-go v1.22.2 // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/bytedance/sonic v1.11.6 // indirect
	github.com/bytedance/sonic/loader v0.1.1 // indirect
	github.com/cenkalti/backoff/v4 v4.3.0 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/cloudwego/base64x v0.1.4 // indirect
	github.com/cloudwego/iasm v0.2.0 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
	github.com/felixge/httpsnoop v1.0.4 // indirect
	github.com/gabriel-vasile/mimetype v1.4.3 // indirect
	github.com/gin-contrib/sse v0.1.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-playground/locales v0.14.1 // indirect
	github.com/go-playground/universal-translator v0.18.1 // indirect
	github.com/goccy/go-json v0.10.2 // indirect
	github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/flatbuffers v23.5.26+incompatible // indirect
	github.com/google/s2a-go v0.1.7 // indirect
	github.com/googleapis/enterprise-certificate-proxy v0.3.2 // indirect
	github.com/googleapis/gax-go/v2 v2.12.3 // indirect
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.25.1 // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect
	github.com/jackc/pgx/v5 v5.5.5 // indirect
	github.com/jackc/puddle/v2 v2.2.1 // indirect
	github.com/jinzhu/inflection v1.0.0 // indirect
	github.com/jinzhu/now v1.1.5 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/klauspost/compress v1.17.11 // indirect
	github.com/klauspost/cpuid/v2 v2.2.7 // indirect
	github.com/leodido/go-urn v1.4.0 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pelletier/go-toml/v2 v2.2.2 // indirect
	github.com/pierrec/lz4/v4 v4.1.18 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.55.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/rivo/uniseg v0.2.0 // indirect
	github.com/stretchr/objx v0.5.2 // indirect
	github.com/twitchyliquid64/golang-asm v0.15.1 // indirect
	github.com/ugorji/go/codec v1.2.12 // indirect
	github.com/valyala/bytebufferpool v1.0.0 // indirect
	github.com/valyala/fasthttp v1.58.0 // indirect
	github.com/valyala/tcplisten v1.0.0 // indirect
	github.com/yuin/gopher-lua v1.1.1 // indirect
	github.com/zeebo/xxh3 v1.0.2 // indirect
	go.opencensus.io v0.24.0 // indirect
	go.opentelemetry.io/auto/sdk v1.1.0 // indirect
	go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.49.0 // indirect
	go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.49.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.34.0 // indirect
	go.opentelemetry.io/otel/metric v1.34.0 // indirect
	go.opentelemetry.io/proto/otlp v1.5.0 // indirect
	golang.org/x/arch v0.8.0 // indirect
	golang.org/x/mod v0.17.0 // indirect
	golang.org/x/net v0.34.0 // indirect
	golang.org/x/oauth2 v0.24.0 // indirect
	golang.org/x/sync v0.11.0 // indirect
	golang.org/x/sys v0.30.0 // indirect
	golang.org/x/text v0.22.0 // indirect
	golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d // indirect
	golang.org/x/xerrors v0.0.0-20231012003039-104605ab7028 // indirect
	google.golang.org/genproto v0.0.0-20240213162025-012b6fc9bca9 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20250115164207-1a7da9e5054f // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20250115164207-1a7da9e5054f // indirect
	google.golang.org/grpc v1.69.4 // indirect
	google.golang.org/protobuf v1.36.3 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Metadata Tool

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="PRD.md">
### **Product Requirements Document (PRD)**  
**Project Name**: Metadata Tool for Music Catalogs  

## **1. Introduction**  

### **1.1 Purpose**  
The Metadata Tool for Music Catalogs is designed to provide **labels, distributors, and rights holders** with a **high-performance, AI-driven** solution for managing and enriching music metadata.  

The tool will enable users to:  
- **Drag and drop** audio files for **automated metadata extraction**.  
- **Edit metadata manually or in bulk** through an intuitive UI.  
- **Access metadata via API** for large-scale processing and enrichment.  
- **Enhance metadata with AI** for **genre classification, mood tagging, BPM detection, and key analysis**.  
- **Ensure compliance with industry standards** (DDEX, CWR, ISRC, ISWC).  
- **Batch process metadata** efficiently, including validation and exports.  

### **1.2 Goals and Objectives**  
- **Backend**: High-performance, scalable metadata extraction using **Golang**.  
- **Frontend**: **React-based UI** with an intuitive user experience.  
- **API-First**: GraphQL & RESTful API for **metadata retrieval, validation, and bulk processing**.  
- **AI Enhancement**: AI-powered **metadata enrichment** with model versioning & cost optimization.  
- **Compliance**: Full **DDEX ERN 4.3 support**, ensuring metadata standardization.  
- **Batch Processing**: Bulk metadata imports, edits, and exports at scale.  

---

## **2. Features & Requirements**  

### **2.1 Core Features**  

#### **Metadata Extraction & AI Processing**  
- **Primary AI Model: Qwen2-Audio**
  - Self-hosted solution for cost-effectiveness
  - Real-time processing for Enterprise tier
  - Confidence threshold: 80%
  - Automatic fallback to OpenAI when needed
  
- **Fallback AI Model: OpenAI**
  - Used when Qwen2 confidence < 80%
  - Used during system degradation
  - Cost-optimized usage patterns
  
- **A/B Testing Infrastructure**
  - 90/10 traffic split for model comparison
  - Real-time performance monitoring
  - BigQuery analytics integration

#### **Queue System (Google Pub/Sub)**
- **High Priority Queue**
  - Real-time processing (<1s latency)
  - Enterprise tier requests
  - Automatic scaling
  
- **Low Priority Queue**
  - Batch processing (<10min processing)
  - Free/Pro tier requests
  - Cost-optimized processing

#### **File Management & Retention**
- **Tier-based Retention**
  - Free Tier: 30 days
  - Pro Tier: 6 months
  - Enterprise: Custom retention policies
  
- **Cleanup Process**
  - 7-day deletion notifications
  - Extension options for Enterprise
  - Temporary file management (24h)

#### **API Integration**  
- **GraphQL & REST API** for metadata retrieval, validation, and enrichment.
- OAuth2 authentication with JWT-based access control.
- Rate limiting based on subscription tier.

#### **Batch Processing & Editing**  
- **Bulk metadata imports** with validation and enrichment.
- **CSV, JSON, XML exports** for DSP compatibility.
- **Pre-validation of metadata** against DDEX standards before submission.

#### **User Authentication & Permissions**  
- **Role-based access control (RBAC)**:  
  - **Admin**: Full system control, user management, system settings.  
  - **Label User**: Upload, edit, and export metadata.  
  - **API User**: Programmatic access for automation, subject to API limits.  

---

## **3. Technical Architecture**  

### **3.1 Backend (Golang)**  
- **Framework**: Go Fiber for high-performance HTTP handling.  
- **Database**: PostgreSQL (with **partitioning for large-scale metadata storage**).  
- **Queue System**: Google Pub/Sub for reliable message processing.
- **Analytics**: BigQuery for A/B testing and performance analysis.
- **Storage**: Cloud Storage (S3-compatible) for audio and metadata logs.  
- **Authentication**: OAuth2 with JWT-based session management.  
- **API Documentation**: OpenAPI (Swagger) for REST and GraphQL Playground for flexible queries.  

#### **Metadata Processing Pipeline**  
```mermaid
graph TD
    A[Upload] --> B[Priority Router]
    B -->|High Priority| C[Real-time Queue]
    B -->|Low Priority| D[Batch Queue]
    C --> E[Qwen2-Audio]
    D --> E
    E -->|Confidence < 80%| F[OpenAI Fallback]
    E -->|Success| G[Metadata Storage]
    F --> G
    E -->|Failure x3| H[Dead Letter Queue]
```

---

### **3.2 Frontend (React & TypeScript)**  
- **Framework**: React with Next.js for SSR support.  
- **State Management**: TanStack Query for API state handling.  
- **Validation**: Zod for form validation.  
- **Styling**: Tailwind CSS for responsive UI.  
- **Testing**: Jest & React Testing Library for unit and integration testing.  

---

## **4. AI Model Integration & Scaling**  

### **4.1 AI-Powered Metadata Processing**  
- **Primary Model: Qwen2-Audio**
  - Self-hosted for cost optimization
  - Real-time processing capability
  - Confidence threshold: 80%
  
- **Fallback Model: OpenAI**
  - Used when Qwen2 confidence < 80%
  - Cost-optimized usage patterns
  - Automatic failover support

### **4.2 A/B Testing & Optimization**
- 90/10 traffic split between models
- Real-time performance monitoring
- BigQuery analytics integration
- Automatic model selection based on performance

---

## **5. Service Splitting & Scalability**  

### **5.1 Service Architecture**  
- **Split microservices when exceeding 500M records or 10,000 QPS**.  
- **DDEX ingestion, AI processing, and Metadata API run as separate services**.  

### **5.2 Database & Sharding Strategy**  
- **Partner-based sharding** (Sony, Warner, Universal).  
- **PostgreSQL partitioned tables** for large-scale queries.  
- **Migrate to Cloud Spanner if writes exceed 5,000 TPS**.  

### **5.3 Caching Strategy**  
- **Redis clustering** for metadata caching.  
- **Pre-warming**: Pre-load top 10K metadata records in cache before peak traffic.  
- **Probabilistic cache refresh** to prevent TTL stampedes.  

---

## **6. Compliance & Security**  

### **6.1 DDEX Schema Versioning**  
- **Dual-write migration strategy** ensures backward compatibility.  
- **Pre-validation against latest ERN schema before submission**.  

### **6.2 GDPR & Data Protection**  
- **Soft-delete metadata upon user request** for compliance.  
- **Territorial metadata access controls** based on licensing rules.  
- **Audit logging** for metadata changes, retained for:  
  - **7 years (DDEX compliance)**  
  - **3 years (AI metadata logs)**  
  - **6 months (low-risk operational logs)**  

---

## **7. Monitoring & Observability**

### **7.1 Metrics & Dashboards**
- **AI Processing Metrics**
  - Response time by model
  - Confidence score distribution
  - Error rates and types
  - Cost per request
  
- **Queue Metrics**
  - Queue size by priority
  - Processing time
  - Error rates
  - DLQ statistics

### **7.2 Alerting**
- **AI Processing Alerts**
  - Confidence score < 70% for >20% of requests
  - Processing time > SLA
  - Error rate > 5%
  
- **Queue Alerts**
  - Queue size > 500 pending jobs
  - DLQ size > 100 messages
  - Processing delay > 10 minutes

---

## **8. Deployment & CI/CD**  

### **8.1 Cloud Infrastructure (Google Cloud Platform)**  
- **Compute**: Cloud Run for queue processors, Cloud Functions for cleanup
- **Queue**: Pub/Sub for reliable message processing
- **Storage**: Cloud Storage, Cloud SQL, Redis
- **Analytics**: BigQuery for A/B testing
- **Security**: Cloud IAM, Secret Manager

### **8.2 CI/CD Pipeline**  
- **Cloud Build** for automated testing & deployment.  
- **Artifact Registry** for managing Docker containers.  
- **Staging & Production environments** with feature flags for safe rollouts.  

---

## **9. Future Enhancements**  

- **Multi-region AI Processing**
- **Enhanced A/B Testing Framework**
- **Custom Model Training Pipeline**
- **Advanced Batch Processing**
- **Global Load Balancing**
- **Cross-region Replication**
</file>

<file path="README.md">
# Metadata Tool

A robust metadata management tool for audio tracks, featuring AI-powered metadata enrichment, DDEX ERN 4.3 compliance, and comprehensive track management capabilities.

## Features

-  Audio track metadata management
-  AI-powered metadata enrichment
-  DDEX ERN 4.3 export and validation
-  Role-based access control
-  High-performance caching
-  Prometheus metrics and monitoring
-  Error tracking with Sentry

## Tech Stack

- **Backend**: Go 1.21+
- **Database**: PostgreSQL
- **Cache**: Redis
- **Storage**: S3-compatible storage
- **AI**: OpenAI GPT integration
- **Monitoring**: Prometheus + Grafana
- **Error Tracking**: Sentry

## Getting Started

### Prerequisites

- Go 1.21 or higher
- PostgreSQL 13+
- Redis 6+
- S3-compatible storage (AWS S3, MinIO, etc.)
- OpenAI API key (for AI features)

### Environment Variables

Copy `.env.example` to `.env` and configure:

```bash
# Server
SERVER_PORT=8080
ENVIRONMENT=development
LOG_LEVEL=info

# Database
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=your_password
DB_NAME=metadatatool
DB_SSLMODE=disable

# Redis
REDIS_ENABLED=true
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Auth
JWT_SECRET=your_jwt_secret
ACCESS_TOKEN_EXPIRY=15m
REFRESH_TOKEN_EXPIRY=7d

# AI
AI_API_KEY=your_openai_api_key
AI_MODEL_NAME=gpt-4
AI_MODEL_VERSION=latest

# Storage
STORAGE_PROVIDER=s3
STORAGE_REGION=us-east-1
STORAGE_BUCKET=metadatatool
STORAGE_ACCESS_KEY=your_access_key
STORAGE_SECRET_KEY=your_secret_key
```

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/metadatatool.git
cd metadatatool
```

2. Install dependencies:
```bash
go mod download
```

3. Run database migrations:
```bash
go run cmd/migrate/main.go up
```

4. Start the server:
```bash
go run cmd/api/main.go
```

### Docker Deployment

Build and run with Docker Compose:

```bash
docker-compose up -d
```

For monitoring stack:

```bash
docker-compose -f docker-compose.monitoring.yml up -d
```

## API Documentation

API documentation is available at `/swagger/index.html` when running in development mode.

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</file>

<file path="requirements.txt">
# Core dependencies
requests>=2.31.0
python-dotenv>=1.0.0

# LLM API clients
openai>=1.12.0
anthropic>=0.18.1

# Web scraping
playwright>=1.42.0
beautifulsoup4>=4.12.0

# Utility packages
pillow>=10.2.0  # For image processing
python-dateutil>=2.8.2
</file>

<file path="TODO_DETAILED.md">
# Detailed Implementation TODO List

## 0. Fix Failing Tests (Early Morning Priority)

### 0.1 Session Middleware Fixes
- [ ] Fix TestSession_Middleware/session_store_error
  ```go
  // internal/handler/middleware/session_test.go
  - Fix status code mismatch (expected 500, got 200)
  - Ensure proper error handling in session store error case
  ```

- [ ] Fix TestCreateSession_Middleware failures
  ```go
  // internal/handler/middleware/session_test.go
  - Fix authentication status code (expected 200, got 401)
  - Fix session creation for authenticated users
  - Fix mock expectations for Create() call
  - Verify session cookie setting
  ```

- [ ] Fix TestRequireSession_Middleware/session_exists
  ```go
  // internal/handler/middleware/session_test.go
  - Fix authentication check (expected 200, got 401)
  - Verify session existence validation
  ```

### 0.2 JWT Service Fixes
- [ ] Fix TestJWTService_ValidateToken/expired_token
  ```go
  // internal/repository/auth/jwt_service_test.go
  - Update error message assertion
  - Expected: "token is expired"
  - Actual: "invalid token: token expired"
  - Standardize error messages across JWT service
  ```

## 1. Complete Core Backend (Morning Session)

### 1.1 Use Case Layer Completion
- [ ] Track Use Cases
  ```go
  // internal/usecase/track_usecase.go
  - Implement CreateTrack(ctx context.Context, input *domain.CreateTrackInput) (*domain.Track, error)
  - Implement UpdateTrack(ctx context.Context, id string, input *domain.UpdateTrackInput) (*domain.Track, error)
  - Implement GetTrack(ctx context.Context, id string) (*domain.Track, error)
  - Implement ListTracks(ctx context.Context, filter *domain.TrackFilter) ([]*domain.Track, error)
  - Add validation for ISRC codes
  - Add validation for required metadata fields
  ```

- [ ] Metadata Use Cases
  ```go
  // internal/usecase/metadata_usecase.go
  - Implement ExtractMetadata(ctx context.Context, audioFile *domain.AudioFile) (*domain.Metadata, error)
  - Implement ValidateMetadata(ctx context.Context, metadata *domain.Metadata) error
  - Add DDEX ERN 4.3 validation rules
  - Add territory-specific validation
  ```

- [ ] Audio Processing Use Cases
  ```go
  // internal/usecase/audio_usecase.go
  - Implement ProcessAudio(ctx context.Context, input *domain.AudioProcessingInput) (*domain.AudioProcessingResult, error)
  - Add format validation
  - Add quality checks
  - Implement waveform generation
  ```

### 1.2 GraphQL Implementation
- [ ] Track Resolvers
  ```go
  // internal/graphql/resolvers/track.go
  - Implement track query resolver
  - Implement tracks query resolver with filtering
  - Implement createTrack mutation
  - Implement updateTrack mutation
  - Add proper error handling
  ```

- [ ] Metadata Resolvers
  ```go
  // internal/graphql/resolvers/metadata.go
  - Implement metadata extraction resolver
  - Implement metadata validation resolver
  - Add proper error responses
  ```

## 2. AI Integration (Afternoon Session 1)

### 2.1 Basic AI Service
- [ ] AI Service Interface
  ```go
  // internal/pkg/domain/ai.go
  - Define AIService interface
  - Add confidence score types
  - Add model version tracking
  ```

- [ ] OpenAI Integration
  ```go
  // internal/repository/ai/openai_service.go
  - Implement OpenAI client
  - Add retry mechanism
  - Add rate limiting
  - Add error handling
  ```

### 2.2 Batch Processing
- [ ] Batch Processor
  ```go
  // internal/usecase/batch_usecase.go
  - Implement BatchProcessor interface
  - Add job queuing
  - Add progress tracking
  - Implement error handling
  ```

- [ ] Queue Integration
  ```go
  // internal/repository/queue/batch_queue.go
  - Add batch job types
  - Implement batch job handler
  - Add job status tracking
  ```

## 3. Frontend Setup (Afternoon Session 2)

### 3.1 Project Structure
```bash
frontend/
 src/
    components/
       atoms/
          Button/
          Input/
          Loading/
       molecules/
          TrackCard/
          MetadataForm/
          UploadForm/
       organisms/
           TrackList/
           MetadataEditor/
           BatchUploader/
    hooks/
       useTrack.ts
       useMetadata.ts
       useUpload.ts
    pages/
        tracks/
        upload/
        metadata/
```

### 3.2 Core Components
- [ ] Base Components
  ```typescript
  // frontend/src/components/atoms/Button/Button.tsx
  - Implement primary button
  - Add loading state
  - Add disabled state
  ```

  ```typescript
  // frontend/src/components/atoms/Input/Input.tsx
  - Implement text input
  - Add validation states
  - Add error messages
  ```

- [ ] Form Components
  ```typescript
  // frontend/src/components/molecules/MetadataForm/MetadataForm.tsx
  - Implement form layout
  - Add field validation
  - Add error handling
  - Add loading states
  ```

## 4. Documentation (End of Day)

### 4.1 API Documentation
- [ ] GraphQL Schema Documentation
  ```graphql
  # docs/schema/
  - Document Track type
  - Document Metadata type
  - Document mutations
  - Add usage examples
  ```

- [ ] REST API Documentation
  ```markdown
  # docs/api/
  - Document endpoints
  - Add request/response examples
  - Document error codes
  ```

### 4.2 Integration Tests
- [ ] Track Integration Tests
  ```go
  // internal/test/integration/track_test.go
  - Test track creation flow
  - Test metadata extraction
  - Test AI enrichment
  ```

- [ ] Batch Processing Tests
  ```go
  // internal/test/integration/batch_test.go
  - Test batch upload
  - Test progress tracking
  - Test error handling
  ```

## Priority Order for Tomorrow:
1. Start with Core Backend (Morning)
   - Focus on Track and Metadata use cases first
   - Then move to GraphQL implementation

2. AI Integration (Early Afternoon)
   - Implement basic AI service
   - Add batch processing support

3. Frontend Setup (Late Afternoon)
   - Set up project structure
   - Implement core components

4. Documentation (End of Day)
   - Document as you go
   - Write integration tests

## Notes:
- Each task should include unit tests
- Follow the error handling patterns in DEBUGGING.md
- Use the monitoring setup from docker-compose.monitoring.yml
- Keep commits small and focused
- Update DEBUGGING.md with any new fixes

## Definition of Done:
- Code passes all tests
- Linter shows no errors
- Documentation is updated
- GraphQL schema is updated
- Integration tests pass
- Frontend components have stories
- Error handling is comprehensive
</file>

<file path="TODO.md">
# Metadata Tool TODO List

##  Completed

### Infrastructure & Setup
- [x] Basic project structure with clean architecture
  - Domain, usecase, repository, handler layers
  - Configuration management
  - GraphQL foundation
- [x] CI/CD Pipeline (GitHub Actions)
  - Test running with Redis and Postgres
  - Linting with golangci-lint
  - Coverage reporting to Codecov
  - Docker build and push
- [x] Basic monitoring infrastructure
  - Prometheus setup
  - Alertmanager configuration
  - Grafana installation
  - OpenTelemetry Collector integration

### Authentication & Session Management
- [x] Complete user model implementation
  - User entity with roles and permissions
  - Password hashing and verification
  - API key support
- [x] Finish JWT authentication flow
  - Token generation and validation
  - Access and refresh token handling
  - Claims management
- [x] Implement role-based access control
  - Role hierarchy
  - Permission system
  - Access control middleware
- [x] Complete Redis session management
  - Session creation and validation
  - Session cleanup
  - Session refresh mechanism
- [x] Add comprehensive auth test coverage
  - JWT service tests
  - Session management tests
  - Integration tests

### Queue System (Pub/Sub)
- [x] Complete message processing implementation
  - Message publishing and subscription
  - Message lifecycle management
  - Batch processing support
- [x] Add error handling and retries
  - Configurable retry policies
  - Exponential backoff
  - Error tracking and reporting
- [x] Implement dead letter queue
  - Failed message handling
  - Message replay capability
  - Automatic cleanup
- [x] Add queue monitoring metrics
  - Operation metrics
  - Processing duration
  - Queue size monitoring
  - Error tracking
- [x] Complete queue system tests
  - Unit tests
  - Integration tests
  - Concurrency tests

##  In Progress / Partially Complete

### Monitoring & Observability
- [x] Configure Grafana dashboards for:
  - Auth metrics
  - Session metrics
  - Queue metrics
  - API performance
  - Error rates
- [ ] Configure dashboards for:
  - System resources
- [x] Set up alerting rules for auth system
- [x] Set up alerting rules for queue system
- [ ] Set up alerting rules for other components
- [x] Implement structured logging for auth
- [x] Implement structured logging for queue
- [ ] Implement structured logging for other components
- [x] Add request tracing for auth flows
- [x] Add request tracing for queue operations
- [ ] Add request tracing for other components
- [x] Create health check endpoints

##  Next Priority Tasks (Phase 1)

### Core Features
1. Track Metadata Model
   - [ ] Define track entity and relationships
   - [ ] Create database schema
   - [ ] Implement CRUD operations
   - [ ] Add validation rules
   - [ ] Write unit tests

2. File Upload System
   - [ ] Set up S3/Cloud Storage integration
   - [ ] Implement file upload handlers
   - [ ] Add file validation
   - [ ] Create progress tracking
   - [ ] Implement error handling

3. Metadata Processing
   - [ ] Implement metadata extraction
   - [ ] Add format validation
   - [ ] Create processing queue
   - [ ] Add retry mechanism
   - [ ] Write processing tests

4. Search Implementation
   - [ ] Set up search infrastructure
   - [ ] Implement basic search endpoints
   - [ ] Add filtering and sorting
   - [ ] Create search index
   - [ ] Add search result caching

##  Phase 2 Tasks

### AI Integration
- [ ] Select and integrate AI service
- [ ] Implement metadata enrichment
- [ ] Add confidence scoring
- [ ] Create validation workflow
- [ ] Set up batch processing

### DDEX Integration
- [ ] Implement DDEX schema validation
- [ ] Create DDEX export functionality
- [ ] Add version compatibility
- [ ] Implement error handling
- [ ] Add DDEX-specific tests

### Frontend Development
- [ ] Set up React with TypeScript
- [ ] Create component library
- [ ] Implement auth flows
- [ ] Build metadata editor
- [ ] Add search interface
- [ ] Create admin dashboard

##  Phase 3 Tasks

### Performance & Scaling
- [ ] Implement caching strategy
- [ ] Add database optimizations
- [ ] Set up load balancing
- [ ] Implement rate limiting
- [ ] Add performance monitoring

### Security & Compliance
- [ ] Conduct security audit
- [ ] Implement encryption
- [ ] Add audit logging
- [ ] Set up vulnerability scanning
- [ ] Create security documentation

### Documentation
- [ ] API documentation
- [ ] System architecture docs
- [ ] Deployment guide
- [ ] User manual
- [ ] Contributing guidelines

##  Immediate Action Items (Next 2 Weeks)

1. Complete Authentication & Session Management
   - Finish user model and JWT implementation
   - Complete Redis session store
   - Add remaining auth tests

2. Finalize Queue System
   - Complete Pub/Sub implementation
   - Add error handling
   - Implement monitoring

3. Set up Monitoring Dashboards
   - Configure Grafana
   - Set up basic alerts
   - Implement health checks

4. Start Core Features
   - Begin with track metadata model
   - Set up basic file upload system

##  Notes

- Focus on completing partially implemented features before starting new ones
- Prioritize monitoring setup to ensure visibility into system health
- Consider breaking down larger tasks into smaller, manageable chunks
- Regular testing and documentation should be part of each task
- Review and update this TODO list weekly
</file>

</files>
